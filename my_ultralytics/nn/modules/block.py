# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""Block modules."""

import torch
import torch.nn as nn
import torch.nn.functional as F

from .conv import Conv, DWConv, GhostConv, LightConv, RepConv, autopad
from .transformer import TransformerBlock
from my_ultralytics.utils.torch_utils import fuse_conv_and_bn
from timm.models.layers import trunc_normal_, DropPath, to_2tuple
import cv2
from torch import Tensor
import math
from my_ultralytics.utils import LOGGER

def debug_tensor(tensor_name: str, tensor: torch.Tensor):
    """
    A robust debugging function that uses the Ultralytics LOGGER.
    This will ensure the output is displayed correctly in the training environment.
    """
    if tensor is None:
        LOGGER.info(f"--- [DEBUG] {tensor_name}: Tensor is None ---")
        return
    # å‡†å¤‡æ—¥å¿—ä¿¡æ¯
    log_message = [f"--- [DEBUG] Inspecting: {tensor_name} ---"]
    log_message.append(f"    - Shape: {tensor.shape}")
    log_message.append(f"    - Dtype: {tensor.dtype}")
    
    # æ£€æŸ¥ NaN/Inf
    has_nan = torch.isnan(tensor).any()
    has_inf = torch.isinf(tensor).any()
    
    if has_nan or has_inf:
        log_message.append(f"    - !!! CRITICAL: Anomaly DETECTED !!!")
        log_message.append(f"    - Has NaN: {has_nan.item()}")
        log_message.append(f"    - Has Inf: {has_inf.item()}")
    else:
        # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°ä¸€äº›ç»Ÿè®¡æ•°æ®
        # ä¸ºäº†é¿å…åœ¨GPUä¸Šè®¡ç®—ç»Ÿè®¡æ•°æ®å¼•å…¥é¢å¤–é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆç§»åŠ¨åˆ°CPU
        stats_tensor = tensor.detach().to(torch.float32).cpu()
        log_message.append(f"    - Stats (on CPU, as float32):")
        log_message.append(f"        - Max:  {stats_tensor.max().item():.6f}")
        log_message.append(f"        - Min:  {stats_tensor.min().item():.6f}")
        log_message.append(f"        - Mean: {stats_tensor.mean().item():.6f}")
        log_message.append(f"        - Std:  {stats_tensor.std().item():.6f}")
        
    log_message.append("--------------------")
    # ä½¿ç”¨LOGGER.info()æ¥æ‰“å°ï¼Œç¡®ä¿å®ƒèƒ½è¢«æ¡†æ¶æ•è·
    LOGGER.info("\n" + "\n".join(log_message))

def inspect_forward_output(module, input_tensor, output_tensor):
    """
    ä¸€ä¸ªå‰å‘ä¼ æ’­Hookå‡½æ•°ï¼Œç”¨äºæ‰“å°å’Œæ£€æŸ¥æ¨¡å—è¾“å‡ºçš„è¯¦ç»†ä¿¡æ¯ã€‚
    """
    try:
        print(f"\n{'='*40}")
        print(f"--- [HOOK] Inspecting Forward Output of: {module.__class__.__name__} ---")
        # 1. æ ¸å¿ƒæ£€æŸ¥ï¼šæ˜¯å¦å­˜åœ¨NaNæˆ–Inf
        has_nan = torch.isnan(output_tensor).any()
        has_inf = torch.isinf(output_tensor).any()
        if has_nan or has_inf:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print("!!! CRITICAL: NaN or Inf DETECTED in the forward pass output!")
            print(f"!!! Has NaN: {has_nan.item()}")
            print(f"!!! Has Inf: {has_inf.item()}")
            print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        else:
            print("âœ… Output tensor is clean (no NaN/Inf).")
        # 2. æ‰“å°è¾“å‡ºå¼ é‡çš„åŸºæœ¬ä¿¡æ¯
        print(f"    - Output Tensor Shape: {output_tensor.shape}")
        print(f"    - Output Tensor Dtype: {output_tensor.dtype}")
        # 3. æ‰“å°å…³é”®çš„ç»Ÿè®¡æ•°æ®
        if output_tensor.numel() > 0:
            # ä½¿ç”¨ .detach() æ¥é¿å…Hookå½±å“æ¢¯åº¦è®¡ç®—
            max_val = torch.max(output_tensor.detach())
            min_val = torch.min(output_tensor.detach())
            mean_val = torch.mean(output_tensor.detach())
            std_val = torch.std(output_tensor.detach())
            
            print(f"    - Statistics:")
            print(f"        - Max Value:  {max_val.item():.6f}")
            print(f"        - Min Value:  {min_val.item():.6f}")
            print(f"        - Mean Value: {mean_val.item():.6f}")
            print(f"        - Std Dev:    {std_val.item():.6f}")
            if std_val.item() < 1e-8 and output_tensor.numel() > 1:
                print("\n    âš ï¸ WARNING: Standard deviation is near-zero!")
                print("        This can cause issues with downstream BatchNorm layers.\n")
        else:
            print("    - Output tensor is empty.")
        print(f"--- [HOOK] Inspection Finished ---")
        print(f"{'='*40}\n")
    except Exception as e:
        print(f"!!! Error during Hook execution: {e}")
        
__all__ = (
    "DFL",
    "HGBlock",
    "HGStem",
    "SPP",
    "SPPF",
    "C1",
    "C2",
    "C3",
    "C2f",
    "C2fAttn",
    "ImagePoolingAttn",
    "ContrastiveHead",
    "BNContrastiveHead",
    "C3x",
    "C3TR",
    "C3Ghost",
    "GhostBottleneck",
    "Bottleneck",
    "BottleneckCSP",
    "Proto",
    "RepC3",
    "ResNetLayer",
    "RepNCSPELAN4",
    "ADown",
    "SPPELAN",
    "CBFuse",
    "CBLinear",
    "Silence",
    "S_UniRepLKNetBlock",
    "L_UniRepLKNetBlock",
    "Smak_Block",
    "Lark_Block",
    "RepViTBlock",
    "DenseRepViTBlock",
    "DenseRepViTBlock_",
    "C3k",
    "C3k2",
    "C2PSA",
    "DeformableViTBlock",
    "Eage_detect",
    "Edge_Emphasize",
    "DenseRepViTBlock_EGA",
    "RepViTBlock_ECA",
    "RepViTBlock_ECA_Att",
    "EG_stem",
    "EGA",
    "EGA_att",
    "ConcatShuffleConv",
    "Edge_guide",
    "Fuse_Features",
    "CSP_DenseRepViTBlock",
    "CSP_DenseRepViTBlock_"
)



def get_conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias,
               attempt_use_lk_impl=True):
    kernel_size = to_2tuple(kernel_size)
    if padding is None:
        padding = (kernel_size[0] // 2, kernel_size[1] // 2)
    else:
        padding = to_2tuple(padding)
    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,
                     padding=padding, dilation=dilation, groups=groups, bias=bias)
def get_bn(dim, use_sync_bn=False):
    if use_sync_bn:
        return nn.SyncBatchNorm(dim)
    else:
        return nn.BatchNorm2d(dim)

# class Conv2d_BN(torch.nn.Sequential):
#     def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
#                  groups=1, bn_weight_init=1, resolution=-10000):
#         super().__init__()
#         self.add_module('c', torch.nn.Conv2d(
#             a, b, ks, stride, pad, dilation, groups, bias=False))
#         self.add_module('bn', torch.nn.BatchNorm2d(b))
#         torch.nn.init.constant_(self.bn.weight, bn_weight_init)
#         torch.nn.init.constant_(self.bn.bias, 0)
    
# def fuse_bn(conv, bn):
#     conv_bias = 0 if conv.bias is None else conv.bias
#     std = (bn.running_var + bn.eps).sqrt()
#     return conv.weight * (bn.weight / std).reshape(-1, 1, 1, 1), bn.bias + (conv_bias - bn.running_mean) * bn.weight / std

# def convert_dilated_to_nondilated(kernel, dilate_rate):
#     identity_kernel = torch.ones((1, 1, 1, 1)).to(kernel.device)
#     if kernel.size(1) == 1:
#         #   This is a DW kernel
#         dilated = F.conv_transpose2d(kernel, identity_kernel, stride=dilate_rate)
#         return dilated
#     else:
#         #   This is a dense or group-wise (but not DW) kernel
#         slices = []
#         for i in range(kernel.size(1)):
#             dilated = F.conv_transpose2d(kernel[:,i:i+1,:,:], identity_kernel, stride=dilate_rate)
#             slices.append(dilated)
#         return torch.cat(slices, dim=1)

# def merge_dilated_into_large_kernel(large_kernel, dilated_kernel, dilated_r):
#     large_k = large_kernel.size(2)
#     dilated_k = dilated_kernel.size(2)
#     equivalent_kernel_size = dilated_r * (dilated_k - 1) + 1
#     equivalent_kernel = convert_dilated_to_nondilated(dilated_kernel, dilated_r)
#     rows_to_pad = large_k // 2 - equivalent_kernel_size // 2
#     merged_kernel = large_kernel + F.pad(equivalent_kernel, [rows_to_pad] * 4)
#     return merged_kernel

class Bottleneck(nn.Module):
    """Standard bottleneck."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a standard bottleneck module with optional shortcut connection and configurable parameters."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """Applies the YOLO FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
    
class DFL(nn.Module):
    """
    Integral module of Distribution Focal Loss (DFL).

    Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391
    """

    def __init__(self, c1=16):
        """Initialize a convolutional layer with a given number of input channels."""
        super().__init__()
        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)
        x = torch.arange(c1, dtype=torch.float)
        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
        self.c1 = c1

    def forward(self, x):
        """Applies a transformer layer on input tensor 'x' and returns a tensor."""
        b, _, a = x.shape  # batch, channels, anchors
        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
        # return self.conv(x.view(b, self.c1, 4, a).softmax(1)).view(b, 4, a)


class Proto(nn.Module):
    """YOLOv8 mask Proto module for segmentation models."""

    def __init__(self, c1, c_=256, c2=32):
        """
        Initializes the YOLOv8 mask Proto module with specified number of protos and masks.

        Input arguments are ch_in, number of protos, number of masks.
        """
        super().__init__()
        self.cv1 = Conv(c1, c_, k=3)
        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')
        self.cv2 = Conv(c_, c_, k=3)
        self.cv3 = Conv(c_, c2)

    def forward(self, x):
        """Performs a forward pass through layers using an upsampled input image."""
        return self.cv3(self.cv2(self.upsample(self.cv1(x))))


class HGStem(nn.Module):
    """
    StemBlock of PPHGNetV2 with 5 convolutions and one maxpool2d.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2):
        """Initialize the SPP layer with input/output channels and specified kernel sizes for max pooling."""
        super().__init__()
        self.stem1 = Conv(c1, cm, 3, 2, act=nn.ReLU())
        self.stem2a = Conv(cm, cm // 2, 2, 1, 0, act=nn.ReLU())
        self.stem2b = Conv(cm // 2, cm, 2, 1, 0, act=nn.ReLU())
        self.stem3 = Conv(cm * 2, cm, 3, 2, act=nn.ReLU())
        self.stem4 = Conv(cm, c2, 1, 1, act=nn.ReLU())
        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, ceil_mode=True)

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        x = self.stem1(x)
        x = F.pad(x, [0, 1, 0, 1])
        x2 = self.stem2a(x)
        x2 = F.pad(x2, [0, 1, 0, 1])
        x2 = self.stem2b(x2)
        x1 = self.pool(x)
        x = torch.cat([x1, x2], dim=1)
        x = self.stem3(x)
        x = self.stem4(x)
        return x


class HGBlock(nn.Module):
    """
    HG_Block of PPHGNetV2 with 2 convolutions and LightConv.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2, k=3, n=6, lightconv=False, shortcut=False, act=nn.ReLU()):
        """Initializes a CSP Bottleneck with 1 convolution using specified input and output channels."""
        super().__init__()
        block = LightConv if lightconv else Conv
        self.m = nn.ModuleList(block(c1 if i == 0 else cm, cm, k=k, act=act) for i in range(n))
        self.sc = Conv(c1 + n * cm, c2 // 2, 1, 1, act=act)  # squeeze conv
        self.ec = Conv(c2 // 2, c2, 1, 1, act=act)  # excitation conv
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        y = [x]
        y.extend(m(y[-1]) for m in self.m)
        y = self.ec(self.sc(torch.cat(y, 1)))
        return y + x if self.add else y


class SPP(nn.Module):
    """Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729."""

    def __init__(self, c1, c2, k=(5, 9, 13)):
        """Initialize the SPP layer with input/output channels and pooling kernel sizes."""
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        """Forward pass of the SPP layer, performing spatial pyramid pooling."""
        x = self.cv1(x)
        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))


class SPPF(nn.Module):
    """Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher."""

    def __init__(self, c1, c2, k=5):
        """
        Initializes the SPPF layer with given input/output channels and kernel size.

        This module is equivalent to SPP(k=(5, 9, 13)).
        """
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        """Forward pass through Ghost Convolution block."""
        x = self.cv1(x)
        y1 = self.m(x)
        y2 = self.m(y1)
        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))


class C1(nn.Module):
    """CSP Bottleneck with 1 convolution."""

    def __init__(self, c1, c2, n=1):
        """Initializes the CSP Bottleneck with configurations for 1 convolution with arguments ch_in, ch_out, number."""
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.m = nn.Sequential(*(Conv(c2, c2, 3) for _ in range(n)))

    def forward(self, x):
        """Applies cross-convolutions to input in the C3 module."""
        y = self.cv1(x)
        return self.m(y) + y


class C2(nn.Module):
    """CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initializes the CSP Bottleneck with 2 convolutions module with arguments ch_in, ch_out, number, shortcut,
        groups, expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv(2 * self.c, c2, 1)  # optional act=FReLU(c2)
        # self.attention = ChannelAttention(2 * self.c)  # or SpatialAttention()
        self.m = nn.Sequential(*(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        a, b = self.cv1(x).chunk(2, 1)
        return self.cv2(torch.cat((self.m(a), b), 1))


class C2f(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))


class C3(nn.Module):
    """CSP Bottleneck with 3 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize the CSP Bottleneck with given channels, number, shortcut, groups, and expansion values."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


class C3x(C3):
    """C3 module with cross-convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3TR instance and set default parameters."""
        super().__init__(c1, c2, n, shortcut, g, e)
        self.c_ = int(c2 * e)
        self.m = nn.Sequential(*(Bottleneck(self.c_, self.c_, shortcut, g, k=((1, 3), (3, 1)), e=1) for _ in range(n)))


class RepC3(nn.Module):
    """Rep C3."""

    def __init__(self, c1, c2, n=3, e=1.0):
        """Initialize CSP Bottleneck with a single convolution using input channels, output channels, and number."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c1, c2, 1, 1)
        self.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])
        self.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()

    def forward(self, x):
        """Forward pass of RT-DETR neck layer."""
        return self.cv3(self.m(self.cv1(x)) + self.cv2(x))


class C3TR(C3):
    """C3 module with TransformerBlock()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3Ghost module with GhostBottleneck()."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)


class C3Ghost(C3):
    """C3 module with GhostBottleneck()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))


class GhostBottleneck(nn.Module):
    """Ghost Bottleneck https://github.com/huawei-noah/ghostnet."""

    def __init__(self, c1, c2, k=3, s=1):
        """Initializes GhostBottleneck module with arguments ch_in, ch_out, kernel, stride."""
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # pw
            cv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
            GhostConv(c_, c2, 1, 1, act=False),  # pw-linear
        )
        self.shortcut = (
            nn.Sequential(cv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()
        )

    def forward(self, x):
        """Applies skip connection and concatenation to input tensor."""
        return self.conv(x) + self.shortcut(x)


class BottleneckCSP(nn.Module):
    """CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initializes the CSP Bottleneck given arguments for ch_in, ch_out, number, shortcut, groups, expansion."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        """Applies a CSP bottleneck with 3 convolutions."""
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))


class ResNetBlock(nn.Module):
    """ResNet block with standard convolution layers."""

    def __init__(self, c1, c2, s=1, e=4):
        """Initialize convolution with given parameters."""
        super().__init__()
        c3 = e * c2
        self.cv1 = Conv(c1, c2, k=1, s=1, act=True)
        self.cv2 = Conv(c2, c2, k=3, s=s, p=1, act=True)
        self.cv3 = Conv(c2, c3, k=1, act=False)
        self.shortcut = nn.Sequential(Conv(c1, c3, k=1, s=s, act=False)) if s != 1 or c1 != c3 else nn.Identity()

    def forward(self, x):
        """Forward pass through the ResNet block."""
        return F.relu(self.cv3(self.cv2(self.cv1(x))) + self.shortcut(x))


class ResNetLayer(nn.Module):
    """ResNet layer with multiple ResNet blocks."""

    def __init__(self, c1, c2, s=1, is_first=False, n=1, e=4):
        """Initializes the ResNetLayer given arguments."""
        super().__init__()
        self.is_first = is_first

        if self.is_first:
            self.layer = nn.Sequential(
                Conv(c1, c2, k=7, s=2, p=3, act=True), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
            )
        else:
            blocks = [ResNetBlock(c1, c2, s, e=e)]
            blocks.extend([ResNetBlock(e * c2, c2, 1, e=e) for _ in range(n - 1)])
            self.layer = nn.Sequential(*blocks)

    def forward(self, x):
        """Forward pass through the ResNet layer."""
        return self.layer(x)


class MaxSigmoidAttnBlock(nn.Module):
    """Max Sigmoid attention block."""

    def __init__(self, c1, c2, nh=1, ec=128, gc=512, scale=False):
        """Initializes MaxSigmoidAttnBlock with specified arguments."""
        super().__init__()
        self.nh = nh
        self.hc = c2 // nh
        self.ec = Conv(c1, ec, k=1, act=False) if c1 != ec else None
        self.gl = nn.Linear(gc, ec)
        self.bias = nn.Parameter(torch.zeros(nh))
        self.proj_conv = Conv(c1, c2, k=3, s=1, act=False)
        self.scale = nn.Parameter(torch.ones(1, nh, 1, 1)) if scale else 1.0

    def forward(self, x, guide):
        """Forward process."""
        bs, _, h, w = x.shape

        guide = self.gl(guide)
        guide = guide.view(bs, -1, self.nh, self.hc)
        embed = self.ec(x) if self.ec is not None else x
        embed = embed.view(bs, self.nh, self.hc, h, w)

        aw = torch.einsum("bmchw,bnmc->bmhwn", embed, guide)
        aw = aw.max(dim=-1)[0]
        aw = aw / (self.hc**0.5)
        aw = aw + self.bias[None, :, None, None]
        aw = aw.sigmoid() * self.scale

        x = self.proj_conv(x)
        x = x.view(bs, self.nh, -1, h, w)
        x = x * aw.unsqueeze(2)
        return x.view(bs, -1, h, w)


class C2fAttn(nn.Module):
    """C2f module with an additional attn module."""

    def __init__(self, c1, c2, n=1, ec=128, nh=1, gc=512, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((3 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))
        self.attn = MaxSigmoidAttnBlock(self.c, self.c, gc=gc, ec=ec, nh=nh)

    def forward(self, x, guide):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        y.append(self.attn(y[-1], guide))
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x, guide):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        y.append(self.attn(y[-1], guide))
        return self.cv2(torch.cat(y, 1))


class ImagePoolingAttn(nn.Module):
    """ImagePoolingAttn: Enhance the text embeddings with image-aware information."""

    def __init__(self, ec=256, ch=(), ct=512, nh=8, k=3, scale=False):
        """Initializes ImagePoolingAttn with specified arguments."""
        super().__init__()

        nf = len(ch)
        self.query = nn.Sequential(nn.LayerNorm(ct), nn.Linear(ct, ec))
        self.key = nn.Sequential(nn.LayerNorm(ec), nn.Linear(ec, ec))
        self.value = nn.Sequential(nn.LayerNorm(ec), nn.Linear(ec, ec))
        self.proj = nn.Linear(ec, ct)
        self.scale = nn.Parameter(torch.tensor([0.0]), requires_grad=True) if scale else 1.0
        self.projections = nn.ModuleList([nn.Conv2d(in_channels, ec, kernel_size=1) for in_channels in ch])
        self.im_pools = nn.ModuleList([nn.AdaptiveMaxPool2d((k, k)) for _ in range(nf)])
        self.ec = ec
        self.nh = nh
        self.nf = nf
        self.hc = ec // nh
        self.k = k

    def forward(self, x, text):
        """Executes attention mechanism on input tensor x and guide tensor."""
        bs = x[0].shape[0]
        assert len(x) == self.nf
        num_patches = self.k**2
        x = [pool(proj(x)).view(bs, -1, num_patches) for (x, proj, pool) in zip(x, self.projections, self.im_pools)]
        x = torch.cat(x, dim=-1).transpose(1, 2)
        q = self.query(text)
        k = self.key(x)
        v = self.value(x)

        # q = q.reshape(1, text.shape[1], self.nh, self.hc).repeat(bs, 1, 1, 1)
        q = q.reshape(bs, -1, self.nh, self.hc)
        k = k.reshape(bs, -1, self.nh, self.hc)
        v = v.reshape(bs, -1, self.nh, self.hc)

        aw = torch.einsum("bnmc,bkmc->bmnk", q, k)
        aw = aw / (self.hc**0.5)
        aw = F.softmax(aw, dim=-1)

        x = torch.einsum("bmnk,bkmc->bnmc", aw, v)
        x = self.proj(x.reshape(bs, -1, self.ec))
        return x * self.scale + text


class ContrastiveHead(nn.Module):
    """Contrastive Head for YOLO-World compute the region-text scores according to the similarity between image and text
    features.
    """

    def __init__(self):
        """Initializes ContrastiveHead with specified region-text similarity parameters."""
        super().__init__()
        self.bias = nn.Parameter(torch.zeros([]))
        self.logit_scale = nn.Parameter(torch.ones([]) * torch.tensor(1 / 0.07).log())

    def forward(self, x, w):
        """Forward function of contrastive learning."""
        x = F.normalize(x, dim=1, p=2)
        w = F.normalize(w, dim=-1, p=2)
        x = torch.einsum("bchw,bkc->bkhw", x, w)
        return x * self.logit_scale.exp() + self.bias


class BNContrastiveHead(nn.Module):
    """
    Batch Norm Contrastive Head for YOLO-World using batch norm instead of l2-normalization.

    Args:
        embed_dims (int): Embed dimensions of text and image features.
    """

    def __init__(self, embed_dims: int):
        """Initialize ContrastiveHead with region-text similarity parameters."""
        super().__init__()
        self.norm = nn.BatchNorm2d(embed_dims)
        self.bias = nn.Parameter(torch.zeros([]))
        # use -1.0 is more stable
        self.logit_scale = nn.Parameter(-1.0 * torch.ones([]))

    def forward(self, x, w):
        """Forward function of contrastive learning."""
        x = self.norm(x)
        w = F.normalize(w, dim=-1, p=2)
        x = torch.einsum("bchw,bkc->bkhw", x, w)
        return x * self.logit_scale.exp() + self.bias


class RepBottleneck(nn.Module):
    """Rep bottleneck."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a RepBottleneck module with customizable in/out channels, shortcut option, groups and expansion
        ratio.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = RepConv(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """Forward pass through RepBottleneck layer."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


class RepCSP(nn.Module):
    """Rep CSP Bottleneck with 3 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initializes RepCSP layer with given channels, repetitions, shortcut, groups and expansion ratio."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through RepCSP layer."""
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


class RepNCSPELAN4(nn.Module):
    """CSP-ELAN."""

    def __init__(self, c1, c2, c3, c4, n=1):
        """Initializes CSP-ELAN layer with specified channel sizes, repetitions, and convolutions."""
        super().__init__()
        self.c = c3 // 2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = nn.Sequential(RepCSP(c3 // 2, c4, n), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(RepCSP(c4, c4, n), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1)

    def forward(self, x):
        """Forward pass through RepNCSPELAN4 layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))


class ADown(nn.Module):
    """ADown."""

    def __init__(self, c1, c2):
        """Initializes ADown module with convolution layers to downsample input from channels c1 to c2."""
        super().__init__()
        self.c = c2 // 2
        self.cv1 = Conv(c1 // 2, self.c, 3, 2, 1)
        self.cv2 = Conv(c1 // 2, self.c, 1, 1, 0)

    def forward(self, x):
        """Forward pass through ADown layer."""
        x = torch.nn.functional.avg_pool2d(x, 2, 1, 0, False, True)
        x1, x2 = x.chunk(2, 1)
        x1 = self.cv1(x1)
        x2 = torch.nn.functional.max_pool2d(x2, 3, 2, 1)
        x2 = self.cv2(x2)
        return torch.cat((x1, x2), 1)


class SPPELAN(nn.Module):
    """SPP-ELAN."""

    def __init__(self, c1, c2, c3, k=5):
        """Initializes SPP-ELAN block with convolution and max pooling layers for spatial pyramid pooling."""
        super().__init__()
        self.c = c3
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)
        self.cv3 = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)
        self.cv4 = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)
        self.cv5 = Conv(4 * c3, c2, 1, 1)

    def forward(self, x):
        """Forward pass through SPPELAN layer."""
        y = [self.cv1(x)]
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3, self.cv4])
        return self.cv5(torch.cat(y, 1))


class Silence(nn.Module):
    """Silence."""

    def __init__(self):
        """Initializes the Silence module."""
        super(Silence, self).__init__()

    def forward(self, x):
        """Forward pass through Silence layer."""
        return x


class CBLinear(nn.Module):
    """CBLinear."""

    def __init__(self, c1, c2s, k=1, s=1, p=None, g=1):
        """Initializes the CBLinear module, passing inputs unchanged."""
        super(CBLinear, self).__init__()
        self.c2s = c2s
        self.conv = nn.Conv2d(c1, sum(c2s), k, s, autopad(k, p), groups=g, bias=True)

    def forward(self, x):
        """Forward pass through CBLinear layer."""
        outs = self.conv(x).split(self.c2s, dim=1)
        return outs


class CBFuse(nn.Module):
    """CBFuse."""

    def __init__(self, idx):
        """Initializes CBFuse module with layer index for selective feature fusion."""
        super(CBFuse, self).__init__()
        self.idx = idx

    def forward(self, xs):
        """Forward pass through CBFuse layer."""
        target_size = xs[-1].shape[2:]
        res = [F.interpolate(x[self.idx[i]], size=target_size, mode="nearest") for i, x in enumerate(xs[:-1])]
        out = torch.sum(torch.stack(res + xs[-1:]), dim=0)
        return out


class RepVGGDW(torch.nn.Module):
    def __init__(self, ed) -> None:
        super().__init__()
        self.conv = Conv(ed, ed, 7, 1, 3, g=ed, act=False)
        self.conv1 = Conv(ed, ed, 3, 1, 1, g=ed, act=False)
        self.dim = ed
        self.act = nn.SiLU()
    
    def forward(self, x):
        return self.act(self.conv(x) + self.conv1(x))
    
    def forward_fuse(self, x):
        return self.act(self.conv(x))

    @torch.no_grad()
    def fuse(self):
        conv = fuse_conv_and_bn(self.conv.conv, self.conv.bn)
        conv1 = fuse_conv_and_bn(self.conv1.conv, self.conv1.bn)
        
        conv_w = conv.weight
        conv_b = conv.bias
        conv1_w = conv1.weight
        conv1_b = conv1.bias
        
        conv1_w = torch.nn.functional.pad(conv1_w, [2,2,2,2])

        final_conv_w = conv_w + conv1_w
        final_conv_b = conv_b + conv1_b

        conv.weight.data.copy_(final_conv_w)
        conv.bias.data.copy_(final_conv_b)

        self.conv = conv
        del self.conv1

class RepVGGDW_ViT(torch.nn.Module):
    def __init__(self, ed) -> None:
        super().__init__()
        self.conv = Conv2d_BN(ed, ed, 3, 1, 1, groups=ed)
        self.conv1 = torch.nn.Conv2d(ed, ed, 1, 1, 0, groups=ed)
        self.dim = ed
        self.bn = torch.nn.BatchNorm2d(ed)
    
    def forward(self, x):
        return self.bn((self.conv(x) + self.conv1(x)) + x)
    
    @torch.no_grad()
    def fuse(self):
        conv = self.conv.fuse()
        conv1 = self.conv1
        
        conv_w = conv.weight
        conv_b = conv.bias
        conv1_w = conv1.weight
        conv1_b = conv1.bias
        
        conv1_w = torch.nn.functional.pad(conv1_w, [1,1,1,1])

        identity = torch.nn.functional.pad(torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1,1,1,1])

        final_conv_w = conv_w + conv1_w + identity
        final_conv_b = conv_b + conv1_b

        conv.weight.data.copy_(final_conv_w)
        conv.bias.data.copy_(final_conv_b)

        bn = self.bn
        w = bn.weight / (bn.running_var + bn.eps)**0.5
        w = conv.weight * w[:, None, None, None]
        b = bn.bias + (conv.bias - bn.running_mean) * bn.weight / \
            (bn.running_var + bn.eps)**0.5
        conv.weight.data.copy_(w)
        conv.bias.data.copy_(b)
        return conv

class CIB(nn.Module):
    """Standard bottleneck."""

    def __init__(self, c1, c2, shortcut=True, e=0.5, lk=False):
        """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
        expansion.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = nn.Sequential(
            Conv(c1, c1, 3, g=c1),
            Conv(c1, 2 * c_, 1),
            Conv(2 * c_, 2 * c_, 3, g=2 * c_) if not lk else RepVGGDW(2 * c_),
            Conv(2 * c_, c2, 1),
            Conv(c2, c2, 3, g=c2),
        )

        self.add = shortcut and c1 == c2

    def forward(self, x):
        """'forward()' applies the YOLO FPN to input data."""
        return x + self.cv1(x) if self.add else self.cv1(x)

class C2fCIB(C2f):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, lk=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(CIB(self.c, self.c, shortcut, e=1.0, lk=lk) for _ in range(n))


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8,
                 attn_ratio=0.5):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.key_dim = int(self.head_dim * attn_ratio)
        self.scale = self.key_dim ** -0.5
        nh_kd = nh_kd = self.key_dim * num_heads
        h = dim + nh_kd * 2
        self.qkv = Conv(dim, h, 1, act=False)
        self.proj = Conv(dim, dim, 1, act=False)
        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)

    def forward(self, x):
        B, C, H, W = x.shape
        N = H * W
        qkv = self.qkv(x)
        q, k, v = qkv.view(B, self.num_heads, self.key_dim*2 + self.head_dim, N).split([self.key_dim, self.key_dim, self.head_dim], dim=2)

        attn = (
            (q.transpose(-2, -1) @ k) * self.scale
        )
        attn = attn.softmax(dim=-1)
        x = (v @ attn.transpose(-2, -1)).view(B, C, H, W) + self.pe(v.reshape(B, C, H, W))
        x = self.proj(x)
        return x
class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation Block proposed in SENet (https://arxiv.org/abs/1709.01507)
    We assume the inputs to this layer are (N, C, H, W)
    """
    def __init__(self, input_channels, internal_neurons):
        super(SEBlock, self).__init__()

        self.down = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons,
                              kernel_size=1, stride=1, bias=True)
        self.up = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels,
                            kernel_size=1, stride=1, bias=True)
        self.input_channels = input_channels
        self.nonlinear = nn.ReLU(inplace=True)

    def forward(self, inputs):
        x = F.adaptive_avg_pool2d(inputs, output_size=(1, 1))
        x = self.down(x)
        x = self.nonlinear(x)
        x = self.up(x)
        x = F.sigmoid(x)
        return inputs * x.view(-1, self.input_channels, 1, 1)

class PSA(nn.Module):

    def __init__(self, c1, c2, e=0.5):
        super().__init__()
        assert(c1 == c2)
        self.c = int(c1 * e)
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv(2 * self.c, c1, 1)
        
        self.attn = Attention(self.c, attn_ratio=0.5, num_heads=self.c // 64)
        self.ffn = nn.Sequential(
            Conv(self.c, self.c*2, 1),
            Conv(self.c*2, self.c, 1, act=False)
        )
        
    def forward(self, x):
        a, b = self.cv1(x).split((self.c, self.c), dim=1)
        b = b + self.attn(b)
        b = b + self.ffn(b)
        return self.cv2(torch.cat((a, b), 1))

class SCDown(nn.Module):
    def __init__(self, c1, c2, k, s):
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)

    def forward(self, x):
        return self.cv2(self.cv1(x))

class DilatedReparamBlock(nn.Module):
    """
    Dilated Reparam Block proposed in UniRepLKNet (https://github.com/AILab-CVC/UniRepLKNet)
    We assume the inputs to this block are (N, C, H, W)
    """
    def __init__(self, channels, kernel_size, deploy, use_sync_bn=False, attempt_use_lk_impl=True):
        super().__init__()
        self.lk_origin = get_conv2d(channels, channels, kernel_size, stride=1,
                                    padding=kernel_size//2, dilation=1, groups=channels, bias=deploy,
                                    attempt_use_lk_impl=attempt_use_lk_impl)
        self.attempt_use_lk_impl = attempt_use_lk_impl

        #   Default settings. We did not tune them carefully. Different settings may work better.
        '''
        æ ¹æ®å·ç§¯æ ¸å¤§å°ï¼ˆkernel_sizeï¼‰ï¼Œä¸ºè†¨èƒ€å·ç§¯é€‰æ‹©ä¸åŒçš„å·ç§¯æ ¸å°ºå¯¸å’Œè†¨èƒ€ç‡ï¼ˆdilatesï¼‰ã€‚
        è†¨èƒ€å·ç§¯é€šè¿‡åœ¨å·ç§¯æ ¸å…ƒç´ ä¹‹é—´æ’å…¥é—´éš™æ¥æ‰©å¤§æ„Ÿå—é‡ï¼Œå…·ä½“æ„Ÿå—é‡å¤§å°ç”±è†¨èƒ€ç‡å†³å®šã€‚
        ä¾‹å¦‚ï¼Œå½“ kernel_size ä¸º 17 æ—¶ï¼Œä½¿ç”¨ 5ã€9 å’Œ 3 å¤§å°çš„å·ç§¯æ ¸ï¼Œå¹¶ä»¥ä¸åŒçš„è†¨èƒ€ç‡åº”ç”¨å·ç§¯
        '''
        if kernel_size == 17:
            self.kernel_sizes = [5, 9, 3, 3, 3]
            self.dilates = [1, 2, 4, 5, 7]
        elif kernel_size == 15:
            self.kernel_sizes = [5, 7, 3, 3, 3]
            self.dilates = [1, 2, 3, 5, 7]
        elif kernel_size == 13:
            self.kernel_sizes = [5, 7, 3, 3, 3]
            self.dilates = [1, 2, 3, 4, 5]
        elif kernel_size == 11:
            self.kernel_sizes = [5, 5, 3, 3, 3]
            self.dilates = [1, 2, 3, 4, 5]
        elif kernel_size == 9:
            self.kernel_sizes = [5, 5, 3, 3]
            self.dilates = [1, 2, 3, 4]
        elif kernel_size == 7:
            self.kernel_sizes = [5, 3, 3]
            self.dilates = [1, 2, 3]
        elif kernel_size == 5:
            self.kernel_sizes = [3, 3]
            self.dilates = [1, 2]
        else:
            raise ValueError('Dilated Reparam Block requires kernel_size >= 5')
        if not deploy:
            self.origin_bn = get_bn(channels, use_sync_bn)
            for k, r in zip(self.kernel_sizes, self.dilates):
                self.__setattr__('dil_conv_k{}_{}'.format(k, r),
                                 nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=k, stride=1,
                                           padding=(r * (k - 1) + 1) // 2, dilation=r, groups=channels,
                                           bias=False))
                self.__setattr__('dil_bn_k{}_{}'.format(k, r), get_bn(channels, use_sync_bn=use_sync_bn))
    '''
    å‰å‘ä¼ æ’­ï¼š
    å¦‚æœå½“å‰å¤„äºæ¨ç†æ¨¡å¼ï¼ˆå³æ²¡æœ‰ origin_bn å±æ€§ï¼‰ï¼Œåˆ™åªä½¿ç”¨æ ‡å‡†å·ç§¯ lk_origin è¿›è¡Œå·ç§¯æ“ä½œï¼Œç›´æ¥è¿”å›å·ç§¯ç»“æœã€‚
    å¦‚æœå¤„äºè®­ç»ƒæ¨¡å¼ï¼ˆorigin_bn å­˜åœ¨ï¼‰ï¼Œé¦–å…ˆå¯¹è¾“å…¥ç‰¹å¾å›¾ x åº”ç”¨æ ‡å‡†å·ç§¯å’Œ BN å±‚ã€‚
    ç„¶åï¼Œéå†è†¨èƒ€å·ç§¯æ ¸å°ºå¯¸å’Œè†¨èƒ€ç‡ï¼Œåˆ†åˆ«å¯¹è¾“å…¥ç‰¹å¾å›¾ x åº”ç”¨å¯¹åº”çš„å·ç§¯å’Œ BN æ“ä½œï¼Œå¹¶å°†å…¶ç»“æœä¸å‰é¢çš„ç»“æœç›¸åŠ ï¼Œä»è€Œèåˆè†¨èƒ€å·ç§¯å’Œæ ‡å‡†å·ç§¯çš„ç‰¹å¾ã€‚
    '''
    def forward(self, x):
        if not hasattr(self, 'origin_bn'):      # deploy mode
            return self.lk_origin(x)
        out = self.origin_bn(self.lk_origin(x))
        for k, r in zip(self.kernel_sizes, self.dilates):
            conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))
            bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))
            out = out + bn(conv(x))
        return out
    '''
    åˆå¹¶è†¨èƒ€å·ç§¯å’Œæ ‡å‡†å·ç§¯ merge_dilated_branchesï¼š
    é¦–å…ˆï¼Œå°†æ ‡å‡†å·ç§¯ lk_origin å’Œå…¶å¯¹åº”çš„ BN å±‚èåˆä¸ºä¸€ä¸ªç­‰æ•ˆçš„å·ç§¯å±‚ï¼Œä½¿ç”¨ fuse_bn å‡½æ•°å°†å·ç§¯æƒé‡å’ŒBNæƒé‡ç»“åˆã€‚
    ç„¶åï¼Œéå†æ‰€æœ‰è†¨èƒ€å·ç§¯æ ¸å’Œå…¶å¯¹åº”çš„ BN å±‚ï¼Œé€ä¸ªå°†å®ƒä»¬åˆå¹¶åˆ°æ ‡å‡†å·ç§¯çš„æƒé‡ä¸­ã€‚merge_dilated_into_large_kernel å‡½æ•°ä¼šå°†è†¨èƒ€å·ç§¯æ ¸è½¬æ¢ä¸ºéè†¨èƒ€å·ç§¯ï¼Œå¹¶å°†å…¶ä¸å¤§æ ¸å·ç§¯èåˆã€‚
    æœ€ç»ˆï¼Œå°†æ‰€æœ‰å·ç§¯æ ¸åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€çš„å·ç§¯å±‚ merged_convï¼Œå¹¶åˆ é™¤æ‰€æœ‰å¤šä½™çš„å·ç§¯å’Œ BN åˆ†æ”¯ï¼Œä»è€Œç®€åŒ–æ¨ç†é˜¶æ®µçš„è®¡ç®—ã€‚
    '''
    def merge_dilated_branches(self):
        if hasattr(self, 'origin_bn'):
            origin_k, origin_b = fuse_bn(self.lk_origin, self.origin_bn)
            for k, r in zip(self.kernel_sizes, self.dilates):
                conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))
                bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))
                branch_k, branch_b = fuse_bn(conv, bn)
                origin_k = merge_dilated_into_large_kernel(origin_k, branch_k, r)
                origin_b += branch_b
            merged_conv = get_conv2d(origin_k.size(0), origin_k.size(0), origin_k.size(2), stride=1,
                                    padding=origin_k.size(2)//2, dilation=1, groups=origin_k.size(0), bias=True,
                                    attempt_use_lk_impl=self.attempt_use_lk_impl)
            merged_conv.weight.data = origin_k
            merged_conv.bias.data = origin_b
            self.lk_origin = merged_conv
            self.__delattr__('origin_bn')
            '''
            è¿™ä¸€æ®µä»£ç çš„ä½œç”¨æ˜¯é€šè¿‡ __delattr__ æ–¹æ³•åˆ é™¤åœ¨åˆå§‹åŒ–æ—¶ä¸ºè†¨èƒ€å·ç§¯å’Œæ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼ŒBNï¼‰åˆ›å»ºçš„å±æ€§ã€‚
            å®ƒä½äº merge_dilated_branches æ–¹æ³•çš„æœ€åéƒ¨åˆ†ï¼Œç›®çš„æ˜¯åœ¨é‡å‚æ•°åŒ–å®Œæˆåï¼Œæ¸…ç†æ‰ä¸å†éœ€è¦çš„å·ç§¯å±‚å’ŒBNå±‚ï¼Œä»è€Œç®€åŒ–æ¨¡å‹ç»“æ„ï¼Œå‡å°‘å†…å­˜å ç”¨
            self.kernel_sizes å’Œ self.dilates: è¿™äº›æ˜¯ç±»ä¸­å®šä¹‰çš„è†¨èƒ€å·ç§¯æ ¸å°ºå¯¸å’Œç›¸åº”çš„è†¨èƒ€ç‡ï¼Œåˆ†åˆ«å­˜å‚¨äº†ä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸å’Œè†¨èƒ€å·ç§¯çš„è†¨èƒ€å› å­ã€‚

            zip(self.kernel_sizes, self.dilates): é€šè¿‡ zip å‡½æ•°å°†å·ç§¯æ ¸çš„å¤§å°å’Œç›¸åº”çš„è†¨èƒ€ç‡ä¸€ä¸€é…å¯¹ã€‚ä¾‹å¦‚ï¼Œself.kernel_sizes = [5, 9, 3] å’Œ self.dilates = [1, 2, 4]ï¼Œåˆ™å®ƒä»¬ä¼šå½¢æˆå¦‚ä¸‹é…å¯¹ï¼š(5, 1)ã€(9, 2)ã€(3, 4)ã€‚

            self.__delattr__: è¿™æ˜¯ Python æä¾›çš„ç”¨äºåˆ é™¤å¯¹è±¡å±æ€§çš„å‡½æ•°ã€‚é€šè¿‡ __delattr__ï¼Œå¯ä»¥åŠ¨æ€åˆ é™¤ç±»å®ä¾‹ä¸­çš„æŒ‡å®šå±æ€§
            dil_conv_k{}_{}'.format(k, r): è¿™ä¸ªæ ¼å¼åŒ–å­—ç¬¦ä¸²å¯¹åº”è†¨èƒ€å·ç§¯å±‚çš„åç§°ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ k = 5 ä¸” r = 1ï¼Œé‚£ä¹ˆ dil_conv_k5_1 å¯¹åº”çš„æ˜¯ä¹‹å‰ä¸ºå·ç§¯æ ¸å¤§å°ä¸º5ã€è†¨èƒ€ç‡ä¸º1çš„è†¨èƒ€å·ç§¯å±‚ã€‚

            dil_bn_k{}_{}'.format(k, r): ç±»ä¼¼åœ°ï¼Œè¿™ä¸ªå­—ç¬¦ä¸²å¯¹åº”è†¨èƒ€å·ç§¯åæ¥çš„æ‰¹å½’ä¸€åŒ–å±‚ã€‚ä¾‹å¦‚ï¼Œdil_bn_k5_1 å¯¹åº”çš„æ˜¯å·ç§¯æ ¸ä¸º5ã€è†¨èƒ€ç‡ä¸º1çš„è†¨èƒ€å·ç§¯åçš„BNå±‚ã€‚
            '''
            for k, r in zip(self.kernel_sizes, self.dilates):
                self.__delattr__('dil_conv_k{}_{}'.format(k, r))
                self.__delattr__('dil_bn_k{}_{}'.format(k, r))

class NCHWtoNHWC(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.permute(0, 2, 3, 1)


class NHWCtoNCHW(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.permute(0, 3, 1, 2)

class GRNwithNHWC(nn.Module):
    """ GRN (Global Response Normalization) layer
    Originally proposed in ConvNeXt V2 (https://arxiv.org/abs/2301.00808)
    This implementation is more efficient than the original (https://github.com/facebookresearch/ConvNeXt-V2)
    We assume the inputs to this layer are (N, H, W, C)
    """
    def __init__(self, dim, use_bias=True):
        super() .__init__()
        self.use_bias = use_bias
        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))
        if self.use_bias:
            self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))

    def forward(self, x):
        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)
        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)
        if self.use_bias:
            return (self.gamma * Nx + 1) * x + self.beta
        else:
            return (self.gamma * Nx + 1) * x

class L_UniRepLKNetBlock(nn.Module):
    def __init__(self,
                 c1,
                 c2,
                 num=0,
                 judge=True,
                 e=0.5,
                 s=1,
                 k=17,
                 shortcut=True,
                 deploy=False,
                 attempt_use_lk_impl=True,
                 use_sync_bn=False,
                 layer_scale_init_value=1e-5
                 ):
        super().__init__()
        assert c1 == c2
        self.c_ = int(c1*e)
        self.c__ = int((1-e)*c1)
        dilated_block = DilatedReparamBlock(self.c_,17,deploy=deploy,attempt_use_lk_impl=attempt_use_lk_impl)
        self.cv = dilated_block
        self.pwconv1 = nn.Sequential(
            NCHWtoNHWC(),
            nn.Linear(self.c_,self.c_*2)
        )
        self.act = nn.Sequential(
            nn.GELU(),
            GRNwithNHWC(self.c_*2, use_bias=not deploy)
        )
        if deploy:
            self.pwconv2 = nn.Sequential(
                nn.Linear(self.c_*2, self.c_),
                NHWCtoNCHW()
            )
        else:
            self.pwconv2 = nn.Sequential(
                nn.Linear(self.c_*2, self.c_, bias=False),
                NHWCtoNCHW(),
                get_bn(self.c_, use_sync_bn=use_sync_bn)
            )
        if deploy or k == 0:
            self.norm = nn.Identity()
        else:
            self.norm = get_bn(self.c_)
        self.se = SEBlock(self.c_, self.c_ // 2)
        self.ffn = nn.Sequential(
            self.pwconv1,
            self.act,
            self.pwconv2
)

        self.shortcut = nn.Sequential(Conv(c1, c2, k=1, s=s, act=False)) if s != 1 or c1 != c2 else nn.Identity()
        self.gamma = nn.Parameter(
            layer_scale_init_value * torch.ones(self.c_),
            requires_grad=True
        ) if not deploy else None



    def forward(self,x):
        y1,y2 = x.split((self.c_,self.c__),dim=1)
        # print("Input shape:", x.shape)
        y = self.se(self.norm(self.cv(y1)))
        # print("Input after:", x.shape)
        
        y = self.ffn(y)
        # print("Input shape afterffn:", x.shape)

        if self.gamma is not None:
            y = self.gamma.view(1, -1, 1, 1) * y
        
        y1 = y
        y = torch.cat((y1,y2),1)
        return y + self.shortcut(x)

    def reparameterize(self):
        if hasattr(self.cv, 'merge_dilated_branches'):
            self.cv.merge_dilated_branches()

        if hasattr(self.norm, 'running_var'):
            std = (self.norm.running_var + self.norm.eps).sqrt()
            if hasattr(self.cv, 'lk_origin'):
                self.cv.lk_origin.weight.data *= (self.norm.weight / std).view(-1, 1, 1, 1)
                self.cv.lk_origin.bias.data = self.norm.bias + (
                        self.cv.lk_origin.bias - self.norm.running_mean) * self.norm.weight / std
            else:
                conv = nn.Conv2d(self.cv.in_channels, self.cv.out_channels, self.cv.kernel_size,
                             padding=self.cv.padding, groups=self.cv.groups, bias=True)
                conv.weight.data = self.cv.weight * (self.norm.weight / std).view(-1, 1, 1, 1)
                conv.bias.data = self.norm.bias - self.norm.running_mean * self.norm.weight / std
                self.cv = conv
            self.norm = nn.Identity()

        if self.gamma is not None:
            final_scale = self.gamma.data
            self.gamma = None
        else:
            final_scale = 1

        if self.act[1].use_bias and len(self.pwconv2) == 3:
            grn_bias = self.act[1].beta.data 
            self.act[1].__delattr__('beta')
            self.act[1].use_bias = False
            linear = self.pwconv2[0]
            grn_bias_projected_bias = (linear.weight.data @ grn_bias.view(-1, 1)).squeeze()
            bn = self.pwconv2[2]
            std = (bn.running_var + bn.eps).sqrt()
            new_linear = nn.Linear(linear.in_features, linear.out_features, bias=True)
            new_linear.weight.data = linear.weight * (bn.weight / std * final_scale).view(-1, 1)
            linear_bias = 0 if linear.bias is None else linear.bias.data
            linear_bias += grn_bias_projected_bias
            new_linear.bias.data = (bn.bias + (linear_bias - bn.running_mean) * bn.weight / std) * final_scale
            self.pwconv2 = nn.Sequential(new_linear, self.pwconv2[1])

class S_UniRepLKNetBlock(nn.Module):
    def __init__(self,
                 c1,
                 c2,
                 shortcut=True,
                 s=1,
                 ):
        super().__init__()
        assert c1 == c2
        self.cv = RepVGGDW_ViT(c1)
        self.bn = nn.BatchNorm2d(c1)
        self.act = nn.GELU()
        self.ffn = nn.Sequential(
    Conv(c1, c1 * 2, 1),
    nn.GELU(),
    Conv(c1 * 2, c1, 1, act=False)
)

        self.shortcut = nn.Sequential(Conv(c1, c2, k=1, s=s, act=False)) if s != 1 or c1 != c2 else nn.Identity()
    
    def forward(self,x):
        y = self.ffn(self.bn(self.cv(x)))
        return y+self.shortcut(x)


    
class Lark_Block(nn.Module):
    def __init__(self,c1,c2,shortcut=True,g=1,e=3/8,s=1,k=17):
        super().__init__()
                # ä½¿ç”¨ nn.Sequential æ¥ç»„ç»‡æ¨¡å—

        self.cv1 = L_UniRepLKNetBlock(c1,c2,e=e,shortcut=shortcut)
        self.cv2 = S_UniRepLKNetBlock(c1,c2,shortcut=shortcut)
                # ä½¿ç”¨ nn.Sequential æ¥ç»„ç»‡æ¨¡å—
    def forward(self,x):
        y = self.cv2(self.cv1(x))
        return y

        
class Smak_Block(nn.Module):
    def __init__(self,c1,c2,s=1,shortcut=True):
        super().__init__()
        self.cv1 = S_UniRepLKNetBlock(c1, c2, shortcut=shortcut)   # ç¬¬ä¸€ä¸ªå°æ ¸å—
        self.cv2 = S_UniRepLKNetBlock(c2, c2, shortcut=shortcut)   # ç¬¬äºŒä¸ªå°æ ¸å—
    def forward(self,x):
        y = self.cv2(self.cv1(x))
        return y

import torch
import torch.nn as nn
import torch.nn.functional as F
import kornia # å¯¼å…¥ kornia åº“
import kornia.color as KC # ä»…ç”¨äº COA.forward ä¸­å¯èƒ½çš„åˆå§‹ç°åº¦è½¬æ¢ï¼ˆå¦‚æœéœ€è¦å¤‡ç”¨æ–¹æ¡ˆï¼‰
import kornia.filters as KF # ç”¨äº harris_response å’Œ dilation

# -----------------------------------------------------------------------------
# å‡½æ•°ï¼šharris_kornia_optimized (å¤„ç†å•é€šé“å›¾åƒæ‰¹æ¬¡)
# -----------------------------------------------------------------------------
def harris_kornia_optimized(
    img_single_channel_batch: torch.Tensor, # æ˜ç¡®è¿™æ˜¯å•é€šé“è¾“å…¥ (N, 1, H, W)
    k: float = 0.04,
    threshold_ratio: float = 0.01, # ç»éªŒå€¼ï¼Œéœ€è¦æ ¹æ®korniaçš„è¾“å‡ºèŒƒå›´è°ƒæ•´
    dilation_kernel_size: int = 3,
    gaussian_kernel_size: int = 5, # kornia harris_response å†…éƒ¨é«˜æ–¯æ¨¡ç³Šå‚æ•°
    gaussian_sigma: float = 1.5,   # kornia harris_response å†…éƒ¨é«˜æ–¯æ¨¡ç³Šå‚æ•°
    harris_window_size: int = 5    # kornia harris_response å†…éƒ¨çª—å£å¤§å°å‚æ•°
    ) -> torch.Tensor:
    """
    Harrisè§’ç‚¹æ£€æµ‹çš„ Kornia GPUä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¤„ç†å•é€šé“å›¾åƒæ‰¹æ¬¡ã€‚
    img_single_channel_batch: è¾“å…¥çš„å•é€šé“å›¾åƒæ‰¹æ¬¡ï¼Œå½¢çŠ¶ (N, 1, H, W)ã€‚
    k: Harris è§’ç‚¹æ£€æµ‹å™¨è‡ªç”±å‚æ•°ã€‚
    threshold_ratio: ç”¨äºå¯¹Harriså“åº”è¿›è¡Œé˜ˆå€¼å¤„ç†çš„æ¯”ä¾‹å› å­ï¼ˆç›¸å¯¹äºæœ€å¤§å“åº”ï¼‰ã€‚
    dilation_kernel_size: ç”¨äºå¯¹æ£€æµ‹åˆ°çš„è§’ç‚¹è¿›è¡Œè†¨èƒ€çš„æ ¸å¤§å° (0è¡¨ç¤ºä¸è†¨èƒ€)ã€‚
    gaussian_kernel_size: Korniaå†…éƒ¨é«˜æ–¯æ¨¡ç³Šçš„æ ¸å¤§å°ã€‚
    gaussian_sigma: Korniaå†…éƒ¨é«˜æ–¯æ¨¡ç³Šçš„sigmaã€‚
    harris_window_size: Korniaå†…éƒ¨è®¡ç®—åæ–¹å·®çŸ©é˜µçš„çª—å£å¤§å°ã€‚
    """
    if not (img_single_channel_batch.ndim == 4 and img_single_channel_batch.shape[1] == 1):
        raise ValueError(
            f"harris_kornia_optimized expects a single channel input (N, 1, H, W), "
            f"got {img_single_channel_batch.shape}"
        )
    device = img_single_channel_batch.device

    # 1. ä½¿ç”¨ kornia.filters.harris_response è®¡ç®—Harriså“åº”
    harris_responses = KF.harris_response(
        img_single_channel_batch, # ç›´æ¥ä½¿ç”¨å•é€šé“è¾“å…¥
        k=torch.tensor(k, device=device, dtype=img_single_channel_batch.dtype), # k éœ€è¦æ˜¯tensorï¼Œä¸”dtypeåŒ¹é…
        gaussian_kernel_size=(gaussian_kernel_size, gaussian_kernel_size),
        gaussian_sigma=(gaussian_sigma, gaussian_sigma),
        window_size=harris_window_size
    ) # è¾“å‡ºå½¢çŠ¶ (N, 1, H, W)

    # 2. é˜ˆå€¼åŒ– R
    att_maps = torch.zeros_like(harris_responses) # åˆå§‹åŒ–è¾“å‡º (N, 1, H, W)
    for i in range(harris_responses.shape[0]): # éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå›¾åƒçš„å“åº”
        R_single = harris_responses[i] # (1, H, W)
        
        current_max = R_single.max()
        if current_max > 1e-8: # é¿å…åœ¨éå¸¸å°çš„å“åº”ä¸Šè®¾ç½®é˜ˆå€¼
            threshold_val = threshold_ratio * current_max
            corner_mask = (R_single > threshold_val).to(img_single_channel_batch.dtype) # ç¡®ä¿dtypeä¸€è‡´
        else: # å¦‚æœæœ€å¤§å“åº”æ¥è¿‘0ï¼Œåˆ™æ²¡æœ‰è§’ç‚¹
            corner_mask = torch.zeros_like(R_single, dtype=img_single_channel_batch.dtype)
            
        att_maps[i] = corner_mask

    # 3. è†¨èƒ€æ“ä½œ (Dilation)
    if dilation_kernel_size > 0:
        dilation_kernel = torch.ones(dilation_kernel_size, dilation_kernel_size,
                                     device=device, dtype=img_single_channel_batch.dtype)
        # kornia.morphology.dilation çš„è¾“å…¥æ˜¯ (B, C, H, W) å’Œ kernel (H_k, W_k)
        att_maps = KF.dilation(att_maps, kernel=dilation_kernel, border_type='replicate')
        # dilation åå¯èƒ½ä¸æ˜¯ä¸¥æ ¼çš„0å’Œ1ï¼Œå¦‚æœéœ€è¦ä¸¥æ ¼äºŒå€¼ï¼Œå¯ä»¥å†æ¬¡é˜ˆå€¼
        # att_maps = (att_maps > 0).to(img_single_channel_batch.dtype)

    return att_maps # è¿”å›å½¢çŠ¶ (N, 1, H, W) çš„æ³¨æ„åŠ›å›¾æ‰¹æ¬¡

# -----------------------------------------------------------------------------
# æ¨¡å—ï¼šCOA (Corner-Oriented Attention)
# -----------------------------------------------------------------------------
class COA(nn.Module):
    def __init__(self, 
                 channel: int, # è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•° (ä¾‹å¦‚ï¼ŒRGBå›¾åƒä¸º3)
                 harris_k: float = 0.04, 
                 harris_threshold_ratio: float = 0.01, # ç»éªŒå€¼ï¼Œå¯èƒ½éœ€è¦ä¸ºkorniaçš„è¾“å‡ºè°ƒæ•´
                 harris_dilation_kernel: int = 3,      # 0 è¡¨ç¤ºä¸è¿›è¡Œè†¨èƒ€
                 harris_gaussian_kernel: int = 5,    # harris_responseå‚æ•°
                 harris_gaussian_sigma: float = 1.5,   # harris_responseå‚æ•°
                 harris_window_sz: int = 5             # harris_responseå‚æ•°
                 ):
        super(COA, self).__init__()
        if channel <= 0:
            raise ValueError("Number of input channels must be positive.")
            
        # 1x1 å·ç§¯ï¼Œç”¨äºç‰¹å¾å˜æ¢
        self.conv_transform = nn.Sequential(
                nn.Conv2d(channel, channel, kernel_size=1, padding=0, bias=True),
                nn.ReLU(inplace=True),
        )
        
        self.num_input_channels = channel # ä¿å­˜è¾“å…¥é€šé“æ•°ï¼Œç”¨äºéå†
        
        # Harrisè§’ç‚¹æ£€æµ‹ç›¸å…³çš„å‚æ•°
        self.harris_k = harris_k
        self.harris_threshold_ratio = harris_threshold_ratio
        self.harris_dilation_kernel_size = harris_dilation_kernel
        self.harris_gaussian_kernel_size = harris_gaussian_kernel
        self.harris_gaussian_sigma = harris_gaussian_sigma
        self.harris_window_size = harris_window_sz

    def forward(self, x: torch.Tensor) -> torch.Tensor: # x å½¢çŠ¶ (N, C, H, W)
        if x.shape[1] != self.num_input_channels:
            raise ValueError(
                f"Input tensor has {x.shape[1]} channels, but COA was initialized with "
                f"{self.num_input_channels} channels."
            )

        # 1. å¯¹è¾“å…¥ç‰¹å¾xè¿›è¡Œ1x1å·ç§¯å˜æ¢
        y = self.conv_transform(x) # y å½¢çŠ¶ (N, C, H, W)

        # 2. å¯¹æ¯ä¸ªé¢œè‰²/è¾“å…¥é€šé“ç‹¬ç«‹è®¡ç®—Harrisè§’ç‚¹æ³¨æ„åŠ›å›¾
        if self.num_input_channels > 0:
            per_channel_att_maps_list = []
            for i in range(self.num_input_channels):
                # æå–å•ä¸ªé€šé“çš„æ•°æ®
                single_channel_data = x[:, i:i+1, :, :] # å½¢çŠ¶ (N, 1, H, W)
                
                # ä¸ºè¯¥é€šé“è®¡ç®—Harrisæ³¨æ„åŠ›å›¾
                channel_specific_harris_att = harris_kornia_optimized(
                    single_channel_data,
                    k=self.harris_k,
                    threshold_ratio=self.harris_threshold_ratio,
                    dilation_kernel_size=self.harris_dilation_kernel_size,
                    gaussian_kernel_size=self.harris_gaussian_kernel_size,
                    gaussian_sigma=self.harris_gaussian_sigma,
                    harris_window_size=self.harris_window_size
                ) # è¾“å‡ºå½¢çŠ¶ (N, 1, H, W)
                per_channel_att_maps_list.append(channel_specific_harris_att)
            
            # 3. åˆå¹¶æ¥è‡ªå„ä¸ªé€šé“çš„æ³¨æ„åŠ›å›¾
            if per_channel_att_maps_list:
                # å°†åˆ—è¡¨ä¸­çš„ (N, 1, H, W) å¼ é‡åœ¨é€šé“ç»´åº¦(dim=1)ä¸Šæ‹¼æ¥èµ·æ¥
                # å¾—åˆ° (N, C, H, W) çš„å¤šé€šé“æ³¨æ„åŠ›å›¾
                concatenated_att_maps = torch.cat(per_channel_att_maps_list, dim=1)
                
                # åœ¨æ‹¼æ¥åçš„é€šé“ç»´åº¦ä¸Šå–æœ€å¤§å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªæœ€ç»ˆçš„å•é€šé“ç©ºé—´æ³¨æ„åŠ›å›¾
                # [0] æ˜¯å› ä¸º .max() è¿”å› (values, indices)
                final_spatial_att_map = concatenated_att_maps.max(dim=1, keepdim=True)[0] # å½¢çŠ¶ (N, 1, H, W)
                # å…¶ä»–åˆå¹¶ç­–ç•¥ä¹Ÿå¯ä»¥è€ƒè™‘ï¼Œä¾‹å¦‚:
                # final_spatial_att_map = concatenated_att_maps.mean(dim=1, keepdim=True)
            else:
                # å¦‚æœç”±äºæŸç§åŸå› åˆ—è¡¨ä¸ºç©ºï¼ˆä¾‹å¦‚è¾“å…¥é€šé“ä¸º0ï¼Œå°½ç®¡æ„é€ å‡½æ•°ä¼šé˜»æ­¢ï¼‰ï¼Œ
                # åˆ›å»ºä¸€ä¸ªä¸èµ·å¢å¼ºä½œç”¨çš„æ³¨æ„åŠ›å›¾ï¼ˆå…¨1ï¼‰
                final_spatial_att_map = torch.ones_like(x[:, 0:1, :, :], dtype=x.dtype, device=x.device)
        else:
             # å¦‚æœè¾“å…¥é€šé“ä¸º0
             final_spatial_att_map = torch.ones_like(x[:, 0:1, :, :], dtype=x.dtype, device=x.device)


        # 4. å°†å•é€šé“ç©ºé—´æ³¨æ„åŠ›å›¾åº”ç”¨äºå¤šé€šé“ç‰¹å¾yï¼Œå¹¶æ·»åŠ æ®‹å·®è¿æ¥
        # PyTorchçš„å¹¿æ’­æœºåˆ¶: (N, 1, H, W) * (N, C, H, W) -> (N, C, H, W)
        out = final_spatial_att_map * y + x
        
        return out

class Conv2d_BN(torch.nn.Sequential):
    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
                 groups=1, bn_weight_init=1, resolution=-10000):
        super().__init__()
        self.add_module('c', torch.nn.Conv2d(
            a, b, ks, stride, pad, dilation, groups, bias=False))
        self.add_module('bn', torch.nn.BatchNorm2d(b))
        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
        torch.nn.init.constant_(self.bn.bias, 0)

    @torch.no_grad()
    def fuse(self):
        c, bn = self._modules.values()
        w = bn.weight / (bn.running_var + bn.eps)**0.5
        w = c.weight * w[:, None, None, None]
        b = bn.bias - bn.running_mean * bn.weight / \
            (bn.running_var + bn.eps)**0.5
        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(
            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups,
            device=c.weight.device)
        m.weight.data.copy_(w)
        m.bias.data.copy_(b)
        return m

class Residual(torch.nn.Module):
    def __init__(self, m, drop=0.):
        super().__init__()
        self.m = m
        self.drop = drop

    def forward(self, x):
        if self.training and self.drop > 0:
            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,
                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()
        else:
            return x + self.m(x)
    
    @torch.no_grad()
    def fuse(self):
        if isinstance(self.m, Conv2d_BN):
            m = self.m.fuse()
            assert(m.groups == m.in_channels)
            identity = torch.ones(m.weight.shape[0], m.weight.shape[1], 1, 1)
            identity = torch.nn.functional.pad(identity, [1,1,1,1])
            m.weight += identity.to(m.weight.device)
            return m
        elif isinstance(self.m, torch.nn.Conv2d):
            m = self.m
            assert(m.groups != m.in_channels)
            identity = torch.ones(m.weight.shape[0], m.weight.shape[1], 1, 1)
            identity = torch.nn.functional.pad(identity, [1,1,1,1])
            m.weight += identity.to(m.weight.device)
            return m
        else:
            return self

from timm.models.layers import SqueezeExcite

def _make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v

from torch.nn.modules.batchnorm import _BatchNorm

def channel_shuffle(x: Tensor, groups: int) -> Tensor:
    batchsize, num_channels, height, width = x.size()
    channels_per_group = num_channels // groups

    # reshape
    x = x.view(batchsize, groups, channels_per_group, height, width)

    x = torch.transpose(x, 1, 2).contiguous()

    # flatten
    x = x.view(batchsize, -1, height, width)

    return x

class RepViTBlock_ECA_Att(nn.Module):
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True):
        """
        Args:
            c1: è¾“å…¥é€šé“æ•°ï¼ˆè‡ªåŠ¨ä»ä¸Šä¸€å±‚è·å–ï¼‰
            c2: è¾“å‡ºé€šé“æ•°
            stride: æ­¥é•¿ï¼Œé»˜è®¤1
            use_se: æ˜¯å¦ä½¿ç”¨SEæ¨¡å—ï¼Œé»˜è®¤True
            use_hs: æ˜¯å¦ä½¿ç”¨GELUæ¿€æ´»ï¼Œé»˜è®¤True
        """
        super(RepViTBlock_ECA_Att, self).__init__()
        assert stride in [1, 2]
        
        self.ega_att = EGA_att(dim=c1)
        
        self.identity = stride == 1 and c1 == c2
        hidden_dim = 2 * c1  # éšè—å±‚é€šé“æ•°å›ºå®šä¸ºè¾“å…¥é€šé“æ•°çš„2å€

        if stride == 2:
            self.token_mixer = nn.Sequential(
                Conv2d_BN(c1, c1, 3, stride, 1, groups=c1),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
                Conv2d_BN(c1, c2, 1, 1, 0)
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c2, 2 * c2, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(2 * c2, c2, 1, 1, 0, bn_weight_init=0),
                ))
        else:
            assert(self.identity)
            self.token_mixer = nn.Sequential(
                RepVGGDW_ViT(c1),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c1, hidden_dim, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(hidden_dim, c2, 1, 1, 0, bn_weight_init=0),
                ))
        
    def forward(self, x):
        x = self.ega_att(x)
        return  self.channel_mixer(self.token_mixer(x))

class RepViTBlock_ECA(nn.Module):
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True):
        """
        Args:
            c1: è¾“å…¥é€šé“æ•°
            c2: è¾“å‡ºé€šé“æ•°
            stride: æ­¥é•¿
            use_se: æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æ¨¡å— (ç°åœ¨å®ƒæ§åˆ¶æ˜¯å¦ä½¿ç”¨ECA)
            use_hs: æ˜¯å¦ä½¿ç”¨GELUæ¿€æ´»
        """
        super(RepViTBlock_ECA, self).__init__()
        assert stride in [1, 2]
        
        
        self.identity = stride == 1 and c1 == c2
        hidden_dim = 2 * c1

        # æ ¹æ®æ­¥é•¿æ„å»ºä¸åŒçš„ token_mixer åˆ†æ”¯
        if stride == 2:
            self.token_mixer = nn.Sequential(
                Conv2d_BN(c1, c1, 3, stride, 1, groups=c1),
                # ä¿®æ”¹ç‚¹: å°† SqueezeExcite æ›¿æ¢ä¸º ECA
                ECA(c1) if use_se else nn.Identity(),
                Conv2d_BN(c1, c2, 1, 1, 0)
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c2, 2 * c2, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(2 * c2, c2, 1, 1, 0, bn_weight_init=0),
                ))
        else:
            assert(self.identity)
            self.token_mixer = nn.Sequential(
                RepVGGDW_ViT(c1),
                # ä¿®æ”¹ç‚¹: å°† SqueezeExcite æ›¿æ¢ä¸º ECA
                ECA(c1) if use_se else nn.Identity(),
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c1, hidden_dim, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(hidden_dim, c2, 1, 1, 0, bn_weight_init=0),
                ))
        
    def forward(self, x):
        # forward é€»è¾‘ä¿æŒä¸å˜
        return self.channel_mixer(self.token_mixer(x))

import math


class ECA(nn.Module):
    """
    é«˜æ•ˆé€šé“æ³¨æ„åŠ› (Efficient Channel Attention - ECA) æ¨¡å—ã€‚
    
    è¿™æ˜¯å¯¹æ‚¨ `Edge_Emphasize` æ¨¡å—ä¸­ä¸€ç»´å·ç§¯æ³¨æ„åŠ›é€»è¾‘çš„ç‹¬ç«‹å°è£…ã€‚
    å®ƒçš„ä½œç”¨æ˜¯æ¥æ”¶ä¸€ä¸ªç‰¹å¾å›¾ï¼Œç„¶åå­¦ä¹ æ¯ä¸ªé€šé“çš„é‡è¦æ€§æƒé‡ï¼Œæœ€åå°†æƒé‡ä¹˜å›
    åŸå§‹ç‰¹å¾å›¾ä¸Šï¼Œä»è€Œå®ç°å¯¹ç‰¹å¾çš„åŠ¨æ€å¢å¼ºã€‚
    """
    def __init__(self, channel):
        """
        Args:
            channel (int): è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚
        """
        super(ECA, self).__init__()
        
        t = int(abs((math.log(channel, 2) + 1) / 2))
        k = t if t % 2 else t + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        
        self.conv1d = nn.Conv1d(
            in_channels=1, 
            out_channels=1, 
            kernel_size=k, 
            padding=(k - 1) // 2,  
            bias=False
        )
        
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        x: è¾“å…¥ç‰¹å¾å›¾ï¼Œshapeä¸º [B, C, H, W]
        """
        y = self.avg_pool(x)
        y = y.squeeze(-1).transpose(-1, -2)
        y = self.conv1d(y)
        y = y.transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y

class RepVGGDW_ViT_LK(torch.nn.Module):
    """æ–°ç‰ˆæœ¬ï¼šæ”¯æŒè‡ªå®šä¹‰å·ç§¯æ ¸å¤§å° (Large Kernel)ã€‚"""
    def __init__(self, ed, kernel_size=7) -> None:
        super().__init__()
        self.kernel_size = kernel_size
        padding = kernel_size // 2
        self.conv = Conv2d_BN(ed, ed, kernel_size, 1, padding, groups=ed)
        self.conv1 = torch.nn.Conv2d(ed, ed, 1, 1, 0, groups=ed, bias=True)
        self.dim = ed
        self.bn = torch.nn.BatchNorm2d(ed)
    
    def forward(self, x):
        return self.bn((self.conv(x) + self.conv1(x)) + x)
    
    @torch.no_grad()
    def fuse(self):
        # ... (ä¸ä¸Šä¸€è½®å›å¤ä¸­ä¿®æ”¹åçš„å¤§æ ¸ç‰ˆæœ¬èåˆé€»è¾‘ä¸€è‡´)
        conv = self.conv.fuse()
        conv1 = self.conv1
        conv_w = conv.weight
        conv_b = conv.bias
        conv1_w = conv1.weight
        conv1_b = conv1.bias if conv1.bias is not None else torch.zeros_like(conv_b)
        k_pad = self.kernel_size // 2
        conv1_w = torch.nn.functional.pad(conv1_w, [k_pad, k_pad, k_pad, k_pad])
        identity = torch.nn.functional.pad(torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [k_pad, k_pad, k_pad, k_pad])
        final_conv_w = conv_w + conv1_w + identity
        final_conv_b = conv_b + conv1_b
        conv.weight.data.copy_(final_conv_w)
        conv.bias.data.copy_(final_conv_b)
        bn = self.bn
        w = bn.weight / (bn.running_var + bn.eps)**0.5
        w = conv.weight * w[:, None, None, None]
        b = bn.bias + (conv.bias - bn.running_mean) * bn.weight / (bn.running_var + bn.eps)**0.5
        conv.weight.data.copy_(w)
        conv.bias.data.copy_(b)
        return conv

class RepViTBlock_LK(nn.Module):
    """
    æœ€ç»ˆç‰ˆæœ¬ï¼š
    - æ—  Residual è¾…åŠ©ç±»ï¼Œæ®‹å·®è¿æ¥åœ¨ forward æ–¹æ³•ä¸­ç›´æ¥å®ç°ã€‚
    """
    def __init__(self, c1, c2, kernel_size=7, use_se=True, stride=1, use_hs=True):
        super(RepViTBlock_LK, self).__init__()
        assert stride in [1, 2]
        
        # self.identity = stride == 1 and c1 == c2  # No longer needed after removing residuals
        
        padding = kernel_size // 2
        if stride == 2:
            self.token_mixer = nn.Sequential(
                Conv2d_BN(c1, c1, kernel_size, stride, padding, groups=c1),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
            )
        else:
            self.token_mixer = nn.Sequential(
                RepVGGDW_ViT_LK(c1, kernel_size=kernel_size),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
            )
        
        self.channel_mixer = nn.Sequential(
                Conv2d_BN(c1, 2 * c1, 1, 1, 0),
                nn.GELU(),
                Conv2d_BN(2 * c1, c2, 1, 1, 0, bn_weight_init=0),
            )

    def forward(self, x):
        # Removed all residual connections as requested.
        x = self.token_mixer(x)
        x = self.channel_mixer(x)
        return x

# class RepViTBlock_LK(nn.Module):
#     """
#     æœ€ç»ˆç‰ˆæœ¬ï¼š
#     - æ—  Residual è¾…åŠ©ç±»ï¼Œæ®‹å·®è¿æ¥åœ¨ forward æ–¹æ³•ä¸­ç›´æ¥å®ç°ã€‚
#     """
#     def __init__(self, c1, c2, kernel_size=7, use_se=True, stride=1, use_hs=True):
#         super(RepViTBlock_LK, self).__init__()
#         assert stride in [1, 2]
        
#         self.identity = stride == 1 and c1 == c2
        
#         padding = kernel_size // 2
#         if stride == 2:
#             self.token_mixer = nn.Sequential(
#                 Conv2d_BN(c1, c1, kernel_size, stride, padding, groups=c1),
#                 SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
#             )
#         else:
#             self.token_mixer = nn.Sequential(
#                 RepVGGDW_ViT_LK(c1, kernel_size=kernel_size),
#                 SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
#             )
        
#         self.channel_mixer = nn.Sequential(
#                 Conv2d_BN(c1, 2 * c1, 1, 1, 0),
#                 nn.GELU(),
#                 Conv2d_BN(2 * c1, c2, 1, 1, 0, bn_weight_init=0),
#             )

#     def forward(self, x):
#         identity_input = x
        
#         if self.identity:
#             x_after_token_mixer = self.token_mixer(x) + x
#             x_after_channel_mixer = self.channel_mixer(x_after_token_mixer) + x_after_token_mixer
#             return x_after_channel_mixer 
#         else:
#             # å½“ stride=2 æˆ– c1!=c2 æ—¶ï¼Œæ²¡æœ‰æ®‹å·®è¿æ¥
#             x_after_token_mixer = self.token_mixer(x)
#             x_after_channel_mixer = self.channel_mixer(x_after_token_mixer)
#             return x_after_channel_mixer

    

# class RepViTBlock_LK(nn.Module):
#     """
#     ä¿®æ”¹åçš„ç‰ˆæœ¬ï¼š
#     - å®ç°äº† CSP (Cross Stage Partial) ç»“æ„ã€‚
#     - è¾“å…¥ç»è¿‡1x1å·ç§¯ååˆ†å‰²æˆä¸¤è·¯ï¼Œä¸€è·¯æ·±åº¦å¤„ç†ï¼Œä¸€è·¯çŸ­è·¯è¿æ¥ï¼Œæœ€åèåˆã€‚
#     """
#     def __init__(self, c1, c2, kernel_size=7, e=0.5, use_se=True):
#         """
#         Args:
#             c1 (int): è¾“å…¥é€šé“æ•°ã€‚
#             c2 (int): è¾“å‡ºé€šé“æ•°ã€‚
#             kernel_size (int): Token Mixer ä¸­å¤§æ ¸å·ç§¯çš„æ ¸å¤§å°ã€‚
#             e (float): åˆ†å‰²å’Œæ‰©å±•æ¯”ä¾‹ï¼Œç”¨äºç¡®å®šä¸­é—´å¤„ç†éƒ¨åˆ†çš„é€šé“æ•°ã€‚
#             use_se (bool): æ˜¯å¦åœ¨ Token Mixer ä¸­ä½¿ç”¨ Squeeze-and-Excite æ³¨æ„åŠ›æ¨¡å—ã€‚
#         """
#         super(RepViTBlock_LK, self).__init__()
        
#         # è®¡ç®—æ¯ä¸ªåˆ†å‰²åˆ†æ”¯çš„ä¸­é—´é€šé“æ•°
#         self.c_ = int(c2 * e)  # intermediate channels for the deep branch

#         # 1. åˆå§‹ 1x1 å·ç§¯ï¼Œç”¨äºç”Ÿæˆå¯ä¾›åˆ†å‰²çš„ç‰¹å¾ï¼Œè¾“å‡ºé€šé“æ•°ä¸º c_ * 2
#         self.initial_conv = Conv2d_BN(c1, self.c_ * 2, 1, 1, 0)
        
#         # 2. ç¬¬ä¸€ä¸ªåˆ†å‰²åˆ†æ”¯çš„ä¸»å¤„ç†è·¯å¾„ï¼ˆæ·±åº¦è·¯å¾„ï¼‰
#         # è¯¥è·¯å¾„æ¥æ”¶ c_ é€šé“ï¼Œå¹¶è¾“å‡º c_ é€šé“
#         self.token_mixer = nn.Sequential(
#             RepVGGDW_ViT_LK(self.c_, kernel_size=kernel_size),
#             SqueezeExcite(self.c_, 0.25) if use_se else nn.Identity(),
#         )
        
#         # channel_mixer æ¥æ”¶ c_ é€šé“ï¼Œå†…éƒ¨å…ˆæ‰©å±•å†å‹ç¼©å› c_ é€šé“
#         self.channel_mixer = nn.Sequential(
#                 Conv2d_BN(self.c_, 2 * self.c_, 1, 1, 0),
#                 nn.GELU(),
#                 Conv2d_BN(2 * self.c_, self.c_, 1, 1, 0, bn_weight_init=0),
#             )
        
#         # 3. æœ€ç»ˆçš„ 1x1 å·ç§¯ï¼Œç”¨äºèåˆæ‹¼æ¥åçš„ç‰¹å¾
#         # å®ƒæ¥æ”¶æ‹¼æ¥åçš„å¼ é‡ï¼Œå°ºå¯¸ä¸º (c_ + c_) = 2 * c_ï¼Œå¹¶è¾“å‡ºæœ€ç»ˆçš„ c2 é€šé“æ•°ã€‚
#         self.fusion_conv = Conv2d_BN(self.c_ * 2, c2, 1, 1, 0)

#     def forward(self, x):
#         """
#         å®ç° CSP ç»“æ„çš„å‰å‘ä¼ æ’­ã€‚
#         """
#         # åº”ç”¨åˆå§‹å·ç§¯
#         x_initial = self.initial_conv(x)
        
#         # åœ¨é€šé“ç»´åº¦ä¸Šå°†å¼ é‡åˆ†å‰²æˆä¸¤éƒ¨åˆ†
#         part1, part2 = x_initial.chunk(2, dim=1)
        
#         # å¯¹ part1 åº”ç”¨æ·±åº¦å¤„ç†è·¯å¾„
#         x_token_mixed = self.token_mixer(part1)
#         # åº”ç”¨å¸¦æœ‰å†…éƒ¨æ®‹å·®è¿æ¥çš„ channel mixer
#         part1_processed = self.channel_mixer(x_token_mixed) + x_token_mixed

#         # å°†å¤„ç†è¿‡çš„ part1 å’Œæœªå¤„ç†çš„ part2 è¿›è¡Œæ‹¼æ¥
#         x_concatenated = torch.cat((part1_processed, part2), dim=1)
        
#         # åº”ç”¨æœ€ç»ˆçš„èåˆå·ç§¯
#         output = self.fusion_conv(x_concatenated)
        
#         return output





class EdgeConvSobelX(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(EdgeConvSobelX, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=True)
        self.scale = nn.Parameter(torch.randn(out_channels, 1, 1, 1) * 1e-3)
        self.bias = nn.Parameter(torch.randn(out_channels))
        template = torch.zeros(out_channels, 1, 3, 3)
        for i in range(out_channels):
            template[i, 0, 0, 0] = 1.0; template[i, 0, 1, 0] = 2.0; template[i, 0, 2, 0] = 1.0
            template[i, 0, 0, 2] = -1.0; template[i, 0, 1, 2] = -2.0; template[i, 0, 2, 2] = -1.0
        self.template = nn.Parameter(template, requires_grad=False)
    def forward(self, x):
        y0 = self.conv1x1(x)
        return F.conv2d(y0, self.scale * self.template, self.bias, 1, 1, groups=y0.shape[1])

class EdgeConvSobelY(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(EdgeConvSobelY, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=True)
        self.scale = nn.Parameter(torch.randn(out_channels, 1, 1, 1) * 1e-3)
        self.bias = nn.Parameter(torch.randn(out_channels))
        template = torch.zeros(out_channels, 1, 3, 3)
        for i in range(out_channels):
            template[i, 0, 0, 0] = 1.0; template[i, 0, 0, 1] = 2.0; template[i, 0, 0, 2] = 1.0
            template[i, 0, 2, 0] = -1.0; template[i, 0, 2, 1] = -2.0; template[i, 0, 2, 2] = -1.0
        self.template = nn.Parameter(template, requires_grad=False)
    def forward(self, x):
        y0 = self.conv1x1(x)
        return F.conv2d(y0, self.scale * self.template, self.bias, 1, 1, groups=y0.shape[1])

class MAFM_Fusion(nn.Module):
    """
    Multi-scale Attentional Feature Fusion Module.
    Takes two parallel feature branches and fuses them using a dynamic, 
    softmax-based attention mechanism.
    """
    def __init__(self, channels, r=2, L=32):
        super(MAFM_Fusion, self).__init__()
        # Calculate the intermediate dimension 'd'
        d = max(int(channels / r), L)

        # FC layer for squeezing global information
        self.fc = nn.Linear(channels, d)
        
        # Two independent FC layers to generate attention vectors for each branch
        self.fcs = nn.ModuleList([nn.Linear(d, channels) for _ in range(2)])
        
        # Softmax to force competition between branches
        self.softmax = nn.Softmax(dim=1)

    def forward(self, branch1_features, branch2_features):
        # branch1_features: dw_out [B, C, H, W]
        # branch2_features: edge_out [B, C, H, W]

        # 1. Package the two branches into a single tensor
        # New shape: [B, 2, C, H, W] where dim=1 represents the branches
        features_packed = torch.cat([
            branch1_features.unsqueeze(1), 
            branch2_features.unsqueeze(1)
        ], dim=1)

        # 2. Generate global information descriptor by fusing and squeezing
        # Summing across branches and then applying global average pooling
        global_descriptor = torch.sum(features_packed, dim=1).mean((2, 3)) # Shape: [B, C]

        # 3. Compress information
        global_descriptor_compressed = self.fc(global_descriptor) # Shape: [B, d]

        # 4. Generate attention vectors for each branch independently
        attention_vectors = [fc(global_descriptor_compressed) for fc in self.fcs]
        attention_vectors_packed = torch.stack(attention_vectors, dim=1) # Shape: [B, 2, C]

        # 5. Apply softmax competition
        attention_weights = self.softmax(attention_vectors_packed)
        # Reshape for broadcasting: [B, 2, C, 1, 1]
        attention_weights = attention_weights.unsqueeze(-1).unsqueeze(-1)

        # 6. Apply weights and fuse by summation
        # Element-wise multiplication followed by summing across the branch dimension
        fused_output = (features_packed * attention_weights).sum(dim=1)

        return fused_output

class BasicBlock(nn.Sequential):
    def __init__(
            self, in_channels, out_channels, kernel_size, stride=1, bias=False,
            bn=True, act=nn.ReLU(True)):

        m = [nn.Conv2d(
            in_channels, out_channels, kernel_size,
            padding=(kernel_size // 2), stride=stride, bias=bias)
        ]
        if bn: m.append(nn.BatchNorm2d(out_channels))
        if act is not None:
            act = nn.GELU()
            m.append(act)
        super(BasicBlock, self).__init__(*m)

class ResidualBlock(nn.Module):
    """
    A standard residual block that includes BatchNorm layers.
    Structure: Conv-BN-Act -> Conv-BN -> + -> Act
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, bias=False, act=nn.PReLU()):
        super(ResidualBlock, self).__init__()
        
        # We use two BasicBlocks for the main path
        self.block1 = BasicBlock(in_channels, out_channels, kernel_size, stride=stride, bias=bias, bn=True, act=act)
        # The second block's activation is applied after the residual connection
        self.block2 = BasicBlock(out_channels, out_channels, kernel_size, stride=1, bias=bias, bn=True, act=None)

        # Shortcut for residual connection if dimensions change
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=bias),
                nn.BatchNorm2d(out_channels)
            )
            
        # Final activation after adding the shortcut
        self.final_act = act if act is not None else nn.Identity()

    def forward(self, x):
        identity = self.shortcut(x)
        out = self.block1(x)
        out = self.block2(out)
        return self.final_act(identity + out)

class RepViTBlock_edge(nn.Module):
    """
    è½»é‡åŒ–ç‰ˆæœ¬ï¼š
    - å®ç°äº† CSP (Cross Stage Partial) ç»“æ„ã€‚
    - è¾“å…¥ç»è¿‡1x1å·ç§¯ååˆ†å‰²æˆä¸¤è·¯ï¼Œä¸€è·¯è¿›è¡ŒåŒ…å«è¾¹ç¼˜å’Œä¸»å¹²åˆ†æ”¯çš„æ·±åº¦å¤„ç†ï¼Œ
      å¦ä¸€è·¯ä½œä¸ºçŸ­è·¯è¿æ¥ï¼Œæœ€åå°†ä¸¤è·¯ç»“æœèåˆã€‚
    - ä¸»å¹²åˆ†æ”¯ä½¿ç”¨äº†ä¸€ä¸ªæ ‡å‡†çš„ ResidualBlockã€‚
    """
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True, e=0.5):
        """
        Args:
            c1 (int): è¾“å…¥é€šé“æ•°ã€‚
            c2 (int): è¾“å‡ºé€šé“æ•°ã€‚
            use_se (bool): æ˜¯å¦åœ¨ç‰¹å¾èåˆæ—¶ä½¿ç”¨ MAFM_Fusion æ¨¡å—ã€‚
            e (float): åˆ†å‰²å’Œæ‰©å±•æ¯”ä¾‹ï¼Œç”¨äºç¡®å®šä¸­é—´å¤„ç†éƒ¨åˆ†çš„é€šé“æ•°ã€‚
        """
        super(RepViTBlock_edge, self).__init__()
        
        # è®¡ç®—æ·±åº¦å¤„ç†åˆ†æ”¯çš„ä¸­é—´é€šé“æ•°
        self.c_ = int(c2 * e)

        # 1. åˆå§‹ 1x1 å·ç§¯ï¼Œç”Ÿæˆå¯ä¾›åˆ†å‰²çš„ç‰¹å¾
        self.initial_conv = Conv2d_BN(c1, self.c_ * 2, 1, 1, 0)

        # 2. æ·±åº¦å¤„ç†è·¯å¾„ (æ“ä½œäº self.c_ é€šé“ä¸Š)
        # 2a. è¾¹ç¼˜åˆ†æ”¯ (æ— å˜åŒ–)
        self.edge_sobel_x = EdgeConvSobelX(self.c_, self.c_)
        self.edge_sobel_y = EdgeConvSobelY(self.c_, self.c_)
        self.conv_reduce = Conv2d_BN(self.c_ * 2, self.c_, ks=1)
        self.sa = SAM()
        
        # 2b. ä¸»å¹²åˆ†æ”¯ä¸èåˆé€»è¾‘ (æ ¸å¿ƒæ”¹åŠ¨)
        self.use_se = use_se
        if self.use_se:
            self.fusion = MAFM_Fusion(self.c_)
        
        # å°† dw_conv æ›¿æ¢ä¸º ResidualBlock
        # å¹¶å°†å˜é‡é‡å‘½åä¸º conv_branch ä»¥åæ˜ å…¶ç±»å‹å˜åŒ–
        self.conv_branch = ResidualBlock(self.c_, self.c_, act=nn.GELU())

        # 2c. Channel Mixer (æ— å˜åŒ–)
        hidden_dim = 2 * self.c_
        self.channel_mixer = nn.Sequential(
            Conv2d_BN(self.c_, hidden_dim, ks=1),
            nn.GELU(),
            Conv2d_BN(hidden_dim, self.c_, ks=1, bn_weight_init=0),
        )

        # 3. æœ€ç»ˆçš„ 1x1 å·ç§¯ï¼Œç”¨äºèåˆæ‹¼æ¥åçš„ç‰¹å¾ (æ— å˜åŒ–)
        self.fusion_conv = Conv2d_BN(self.c_ * 2, c2, 1, 1, 0)
    
    def forward(self, x):
        # 1. åˆå§‹å·ç§¯å’Œåˆ†å‰²
        x_initial = self.initial_conv(x)
        part1, part2 = x_initial.chunk(2, dim=1)

        # 2. å¯¹ part1 è¿›è¡Œæ·±åº¦å¤„ç†
        # 2a. è¾¹ç¼˜åˆ†æ”¯
        edge_x = self.edge_sobel_x(part1)
        edge_y = self.edge_sobel_y(part1)
        edge_cat = torch.cat([edge_x, edge_y], dim=1)
        edge_reduced = self.conv_reduce(edge_cat)
        edge_out = self.sa(edge_reduced) * edge_reduced
        
        # 2b. ä¸»å¹²å·ç§¯åˆ†æ”¯ (æ ¸å¿ƒæ”¹åŠ¨)
        conv_out = self.conv_branch(part1)
        
        # 2c. åŠ¨æ€ç‰¹å¾èåˆ
        if self.use_se:
            fused_features = self.fusion(conv_out, edge_out)
        else:
            fused_features = conv_out + edge_out
        
        # 2d. Channel Mixer å’Œæ®‹å·®è¿æ¥
        part1_processed = fused_features + self.channel_mixer(fused_features)

        # 3. æ‹¼æ¥ä¸èåˆ
        x_concatenated = torch.cat((part1_processed, part2), dim=1)
        output = self.fusion_conv(x_concatenated)
        
        return output


class RepViTBlock(nn.Module):
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True):
        """
        Args:
            c1: è¾“å…¥é€šé“æ•°ï¼ˆè‡ªåŠ¨ä»ä¸Šä¸€å±‚è·å–ï¼‰
            c2: è¾“å‡ºé€šé“æ•°
            stride: æ­¥é•¿ï¼Œé»˜è®¤1
            use_se: æ˜¯å¦ä½¿ç”¨SEæ¨¡å—ï¼Œé»˜è®¤True
            use_hs: æ˜¯å¦ä½¿ç”¨GELUæ¿€æ´»ï¼Œé»˜è®¤True
        """
        super(RepViTBlock, self).__init__()
        assert stride in [1, 2]
        
        self.identity = stride == 1 and c1 == c2
        hidden_dim = 2 * c1  # éšè—å±‚é€šé“æ•°å›ºå®šä¸ºè¾“å…¥é€šé“æ•°çš„2å€

        if stride == 2:
            self.token_mixer = nn.Sequential(
                Conv2d_BN(c1, c1, 3, stride, 1, groups=c1),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
                Conv2d_BN(c1, c2, 1, 1, 0)
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c2, 2 * c2, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(2 * c2, c2, 1, 1, 0, bn_weight_init=0),
                ))
        else:
            assert(self.identity)
            self.token_mixer = nn.Sequential(
                RepVGGDW_ViT(c1),
                SqueezeExcite(c1, 0.25) if use_se else nn.Identity(),
            )
            self.channel_mixer = Residual(nn.Sequential(
                    Conv2d_BN(c1, hidden_dim, 1, 1, 0),
                    nn.GELU() if use_hs else nn.GELU(),
                    Conv2d_BN(hidden_dim, c2, 1, 1, 0, bn_weight_init=0),
                ))
        
    def forward(self, x):
        return  self.channel_mixer(self.token_mixer(x))




import torch.nn as nn
import torch
from torch.nn import functional as F

class Channel_Att(nn.Module):
    def __init__(self, channels, t=16):
        super(Channel_Att, self).__init__()
        self.channels = channels
      
        self.bn2 = nn.BatchNorm2d(self.channels, affine=True)


    def forward(self, x):
        residual = x

        x = self.bn2(x)
        weight_bn = self.bn2.weight.data.abs() / torch.sum(self.bn2.weight.data.abs())
        x = x.permute(0, 2, 3, 1).contiguous()
        x = torch.mul(weight_bn, x)
        x = x.permute(0, 3, 1, 2).contiguous()
        
        x = torch.sigmoid(x) * residual #
        
        return x


class Att(nn.Module):
    def __init__(self, channels,shape, out_channels=None, no_spatial=True):
        super(Att, self).__init__()
        self.Channel_Att = Channel_Att(channels)
  
    def forward(self, x):
        x_out1=self.Channel_Att(x)
 
        return x_out1  

class DenseRepViTLayer(nn.Module):
    def __init__(self, c1, c2,use_se=True,stride=1,use_hs=True):
        super().__init__()
        self.denseblock = nn.Sequential(
            RepViTBlock(c1,c2,use_se=use_se,stride=stride,use_hs=use_hs),
            RepViTBlock(c2,c2,use_se=use_se,stride=stride,use_hs=use_hs)
        )

    
    def forward(self,x: Tensor) -> Tensor:
        y = self.denseblock(x)
        return y


class DenseRepViTLayer_Edge(nn.Module):
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True,e=0.5):
        super().__init__()
        
        # æ„å»ºä¸€ä¸ªåŒ…å«ä¸åŒç±»å‹å—çš„ Sequential æ¨¡å—
        self.denseblock = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå—ï¼šä½¿ç”¨æˆ‘ä»¬åŠŸèƒ½æ›´å¼ºå¤§çš„è¾¹ç¼˜æ„ŸçŸ¥ç‰ˆæœ¬
            RepViTBlock_edge(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs, e=e),
            
            # ç¬¬äºŒä¸ªå—ï¼šä½¿ç”¨åŸå§‹çš„ã€æ›´è½»é‡çš„ç‰ˆæœ¬
            RepViTBlock(c2, c2, use_se=use_se, stride=stride, use_hs=use_hs)
        )

    def forward(self, x: Tensor) -> Tensor:
        y = self.denseblock(x)
        return y

class DenseRepViTLayer_LK(nn.Module):
    # æ–°å¢ kernel_size å’Œ reduction_ratio å‚æ•°
    def __init__(self, c1, c2, use_se=True, stride=1, use_hs=True, kernel_size=7):
        super().__init__()
        # å°† RepViTBlock æ›¿æ¢ä¸º RepViTBlock_LKï¼Œå¹¶ä¼ é€’æ‰€æœ‰å‚æ•°
        self.denseblock = nn.Sequential(
            RepViTBlock(c1,c2,use_se=use_se,stride=stride,use_hs=use_hs),
            RepViTBlock_LK(c2, c2, kernel_size, use_se, stride, use_hs=use_hs)
        )

    def forward(self, x: Tensor) -> Tensor:
        return self.denseblock(x)
    

class CSP_DenseRepViTBlock_LK(nn.Module):
    # __init__ éœ€è¦åŒ…å« Hybrid Layer æ‰€éœ€çš„æ‰€æœ‰å‚æ•°
    def __init__(self, c1, c2, num_layers, use_se, csp_frac=0.5, stride=1, use_hs=True, kernel_size=7):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same"
        self.num_layers = num_layers

        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy
        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        self.layers = nn.ModuleList()
        # åªæœ‰ç¬¬ä¸€ä¸ª Hybrid Layer å¯èƒ½æœ‰ stride=2
        if num_layers > 0:
            self.layers.append(
                DenseRepViTLayer_LK(
                    self.c_heavy, self.c_heavy, use_se, stride, use_hs, kernel_size
                )
            )
        # åç»­æ‰€æœ‰ Hybrid Layer çš„ stride éƒ½å¿…é¡»æ˜¯ 1
        for _ in range(num_layers - 1):
            self.layers.append(
                DenseRepViTLayer_LK(
                    self.c_heavy, self.c_heavy, use_se, stride, use_hs, kernel_size
                )
            )
            
        # å¯†é›†è¿æ¥å’Œèåˆéƒ¨åˆ†çš„ä»£ç ä¿æŒä¸å˜
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)

    def forward(self, input_features: Tensor) -> Tensor:
        # forward æ–¹æ³•çš„é€»è¾‘å®Œå…¨ä¸éœ€è¦æ”¹å˜
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)
        out_light = self.conv_shortcut(x_light)

        features = [x_heavy]
        current_input_to_dense_layer = x_heavy
        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=len(features))
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)
        
        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class CSP_DenseRepViTBlock_lK(nn.Module):
    # åœ¨ __init__ ä¸­æ·»åŠ  kernel_size å’Œ reduction_ratio å‚æ•°
    def __init__(self, c1, c2, num_layers, use_se, csp_frac=0.5, stride=1, use_hs=True, kernel_size=7):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same for CSP DenseRepViTBlock_"
        self.num_layers = num_layers

        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy
        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        self.layers = nn.ModuleList()
        if num_layers > 0:
            # å°† RepViTBlock æ›¿æ¢ä¸º RepViTBlock_LK
            # å¹¶ä¼ é€’æ–°å¢çš„å‚æ•° kernel_size å’Œ reduction_ratio
            self.layers.append(
                RepViTBlock_LK(
                    self.c_heavy, self.c_heavy, 
                    kernel_size=kernel_size, 
                    use_se=use_se, 
                    stride=stride, 
                    use_hs=use_hs
                )
            )
        for _ in range(num_layers - 1):
            # å¯¹åç»­å±‚ä¹Ÿè¿›è¡ŒåŒæ ·çš„æ“ä½œ
            self.layers.append(
                RepViTBlock_LK(
                    self.c_heavy, self.c_heavy, 
                    kernel_size=kernel_size, 
                    use_se=use_se, 
                    stride=1, 
                    use_hs=use_hs
                )
            )

        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)

    # forward æ–¹æ³•ä¸éœ€è¦æ”¹å˜
    def forward(self, input_features: Tensor) -> Tensor:
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)
        out_light = self.conv_shortcut(x_light)

        features = [x_heavy]
        current_input_to_dense_layer = x_heavy

        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                num_parts_for_shuffle = len(features)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)

            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)

        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)

        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class DenseRepViTBlock_Edge(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se,stride=1,use_hs=True,e=0.5):
        super().__init__()
        assert c1 == c2
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.constant = constant

        for _ in range(num_layers):
            self.layers.append (
                DenseRepViTLayer_Edge(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs,e=e
                )
            )
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers-1):
            num_tensors_to_concatenate = i + 2
            in_channels_for_reduction = num_tensors_to_concatenate * constant

            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.constant, kernel_size=1, bias=False),
                    nn.BatchNorm2d(constant),
                    nn.GELU() # æˆ–è€… nn.ReLU(inplace=True)
                )
            )
        final_in_channels = (num_layers + 1) * constant
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels, self.constant, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.constant),
            nn.GELU()
        )


    def forward(self, input_features: Tensor) -> Tensor:

        features = [input_features]
        current_input_to_dense_layer = input_features

        for i in range(self.num_layers):
            if i > 0:
               concat_for_shuffle = torch.cat(features, dim=1)
               num_parts_for_shuffle = len(features)
               shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)

               current_input_to_dense_layer = self.reduction_modules[i-1](shuffled_features)

            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        final_output = self.final_reduction(shuffled_final_output)

        return final_output

class DenseRepViTBlock(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se,stride=1,use_hs=True):
        super().__init__()
        assert c1 == c2
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.constant = constant

        for _ in range(num_layers):
            self.layers.append (
                DenseRepViTLayer(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs
                )
            )
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers-1):
            num_tensors_to_concatenate = i + 2
            in_channels_for_reduction = num_tensors_to_concatenate * constant

            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.constant, kernel_size=1, bias=False),
                    nn.BatchNorm2d(constant),
                    nn.GELU() # æˆ–è€… nn.ReLU(inplace=True)
                )
            )
        final_in_channels = (num_layers + 1) * constant
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels, self.constant, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.constant),
            nn.GELU()
        )


    def forward(self, input_features: Tensor) -> Tensor:

        features = [input_features]
        current_input_to_dense_layer = input_features

        for i in range(self.num_layers):
            if i > 0:
               concat_for_shuffle = torch.cat(features, dim=1)
               num_parts_for_shuffle = len(features)
               shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)

               current_input_to_dense_layer = self.reduction_modules[i-1](shuffled_features)

            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        final_output = self.final_reduction(shuffled_final_output)

        return final_output

class CSP_DenseRepViTBlock_Edge(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se, csp_frac=0.5,stride=1,use_hs=True,e=0.5):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same for CSP DenseRepViTBlock"
        self.num_layers = num_layers
        self.constant = constant  # Keep original parameter assignment

        # --- CSP Architecture Setup ---
        # 1. Store channel splits as attributes using the new `csp_frac` parameter
        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy

        # 2. Define the light path (a simple 1x1 convolution)
        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        # 3. Define the heavy path (the original dense logic, adapted for c_heavy channels)
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            self.layers.append(
                DenseRepViTLayer_Edge(self.c_heavy, self.c_heavy, use_se=use_se, stride=stride, use_hs=use_hs,e=e)
            )

        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        # Final reduction for the heavy path, outputting c_heavy channels
        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        # 4. Final fusion layer to combine light and heavy paths
        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)


    def forward(self, input_features: Tensor) -> Tensor:
        # Split input for CSP using stored attributes
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)

        # --- Process Light Path ---
        out_light = self.conv_shortcut(x_light)

        # --- Process Heavy Path ---
        features = [x_heavy]
        current_input_to_dense_layer = x_heavy

        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                num_parts_for_shuffle = len(features)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)

            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)

        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)

        # --- Concatenate and Fuse ---
        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class DenseRepViTBlock_edge(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se,stride=1,use_hs=True,e=0.5):
        super().__init__()
        assert c1 == c2
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.constant = constant

        for _ in range(num_layers):
            self.layers.append (
                RepViTBlock_edge(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs, e=e
                )
            )
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers-1):
            num_tensors_to_concatenate = i + 2
            in_channels_for_reduction = num_tensors_to_concatenate * constant

            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.constant, kernel_size=1, bias=False),
                    nn.BatchNorm2d(constant),
                    nn.GELU() # æˆ–è€… nn.ReLU(inplace=True)
                )
            )
        final_in_channels = (num_layers + 1) * constant
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels, self.constant, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.constant),
            nn.GELU()
        )


    def forward(self, input_features: Tensor) -> Tensor:

        features = [input_features]
        current_input_to_dense_layer = input_features

        for i in range(self.num_layers):
            if i > 0:
               concat_for_shuffle = torch.cat(features, dim=1)
               num_parts_for_shuffle = len(features)
               shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)

               current_input_to_dense_layer = self.reduction_modules[i-1](shuffled_features)

            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        final_output = self.final_reduction(shuffled_final_output)
        
        return final_output

class CSP_DenseRepViTBlock_edge(nn.Module):
    # æˆ‘ä»¬åªä¿®æ”¹ RepViTBlock çš„è°ƒç”¨ï¼Œå…¶ä»–æ‰€æœ‰é€»è¾‘ä¿æŒä¸å˜
    def __init__(self, c1, c2, constant, num_layers, use_se, csp_frac=0.5, stride=1, use_hs=True):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same for this block"
        self.num_layers = num_layers
        self.constant = constant

        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy

        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        self.layers = nn.ModuleList()
        if num_layers > 0:
            # --- æ ¸å¿ƒæ”¹åŠ¨ç‚¹ 1 ---
            self.layers.append(
                RepViTBlock_edge(self.c_heavy, self.c_heavy, use_se=use_se, stride=stride, use_hs=use_hs)
            )
        for _ in range(num_layers - 1):
            # --- æ ¸å¿ƒæ”¹åŠ¨ç‚¹ 2 ---
            self.layers.append(
                RepViTBlock_edge(self.c_heavy, self.c_heavy, use_se=use_se, stride=1, use_hs=use_hs)
            )
        
        # --- å…¶ä½™æ‰€æœ‰ä»£ç ä¸æ‚¨æä¾›çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒ ---
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)

    def forward(self, input_features: Tensor) -> Tensor:
        # forward æ–¹æ³•çš„é€»è¾‘ä¹Ÿå®Œå…¨ä¸éœ€è¦æ”¹å˜
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)
        out_light = self.conv_shortcut(x_light)

        features = [x_heavy]
        current_input_to_dense_layer = x_heavy
        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=len(features))
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)
            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)

        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)

        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class CSP_DenseRepViTBlock(nn.Module):
    def __init__(self, c1, c2, num_layers,use_se, csp_frac=0.5,stride=1,use_hs=True):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same for CSP DenseRepViTBlock"
        self.num_layers = num_layers

        # --- CSP Architecture Setup ---
        # 1. Store channel splits as attributes using the new `csp_frac` parameter
        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy

        # 2. Define the light path (a simple 1x1 convolution)
        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        # 3. Define the heavy path (the original dense logic, adapted for c_heavy channels)
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            self.layers.append(
                DenseRepViTLayer(self.c_heavy, self.c_heavy, use_se=use_se, stride=stride, use_hs=use_hs)
            )

        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        # Final reduction for the heavy path, outputting c_heavy channels
        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        # 4. Final fusion layer to combine light and heavy paths
        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)


    def forward(self, input_features: Tensor) -> Tensor:
        # Split input for CSP using stored attributes
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)

        # --- Process Light Path ---
        out_light = self.conv_shortcut(x_light)

        # --- Process Heavy Path ---
        features = [x_heavy]
        current_input_to_dense_layer = x_heavy

        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                num_parts_for_shuffle = len(features)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)

            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)

        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)

        # --- Concatenate and Fuse ---
        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class CSP_DenseRepViTBlock_(nn.Module):
    def __init__(self, c1, c2, num_layers,use_se, csp_frac=0.5,stride=1,use_hs=True):
        super().__init__()
        assert c1 == c2, "Input and output channels must be the same for CSP DenseRepViTBlock_"
        self.num_layers = num_layers

        # --- CSP Architecture Setup ---
        # 1. Store channel splits as attributes using the new `csp_frac` parameter
        self.c_heavy = int(c1 * csp_frac)
        self.c_light = c1 - self.c_heavy

        # 2. Define the light path (a simple 1x1 convolution)
        self.conv_shortcut = Conv(self.c_light, self.c_light, 1, 1)

        # 3. Define the heavy path (the original dense logic, adapted for c_heavy channels)
        self.layers = nn.ModuleList()
        # The first layer of the heavy path might have a stride
        if num_layers > 0:
            self.layers.append(
                RepViTBlock(self.c_heavy, self.c_heavy, use_se=use_se, stride=stride, use_hs=use_hs)
            )
        # Subsequent layers have stride=1
        for _ in range(num_layers - 1):
            self.layers.append(
                RepViTBlock(self.c_heavy, self.c_heavy, use_se=use_se, stride=1, use_hs=use_hs)
            )

        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers - 1):
            # All concatenated features in the heavy path have c_heavy channels
            in_channels_for_reduction = (i + 2) * self.c_heavy
            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.c_heavy, kernel_size=1, bias=False),
                    nn.BatchNorm2d(self.c_heavy),
                    nn.GELU()
                )
            )

        # Final reduction for the heavy path, outputting c_heavy channels
        final_in_channels_heavy = (num_layers + 1) * self.c_heavy
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels_heavy, self.c_heavy, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.c_heavy),
            nn.GELU()
        )

        # 4. Final fusion layer to combine light and heavy paths
        self.final_fusion = Conv(self.c_light + self.c_heavy, c2, 1, 1)
        self.final_conv = Conv(c2, c2, 1, 1)


    def forward(self, input_features: Tensor) -> Tensor:
        # Split input for CSP using attributes stored during initialization.
        x_light, x_heavy = input_features.split([self.c_light, self.c_heavy], dim=1)

        # --- Process Light Path ---
        out_light = self.conv_shortcut(x_light)

        # --- Process Heavy Path ---
        features = [x_heavy]
        current_input_to_dense_layer = x_heavy

        for i in range(self.num_layers):
            if i > 0:
                concat_for_shuffle = torch.cat(features, dim=1)
                num_parts_for_shuffle = len(features)
                shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)
                current_input_to_dense_layer = self.reduction_modules[i - 1](shuffled_features)

            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)

        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        out_heavy = self.final_reduction(shuffled_final_output)

        # --- Concatenate and Fuse ---
        out = torch.cat((out_light, out_heavy), dim=1)
        return self.final_conv(self.final_fusion(out))


class DenseRepViTBlock_EGA(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se,stride=1,use_hs=True):
        super().__init__()
        assert c1 == c2
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.constant = constant

        for _ in range(num_layers):
            self.layers.append (
                RepViTBlock_ECA(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs
                )
            )
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers-1):
            num_tensors_to_concatenate = i + 2
            in_channels_for_reduction = num_tensors_to_concatenate * constant

            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.constant, kernel_size=1, bias=False),
                    nn.BatchNorm2d(constant),
                    nn.GELU() # æˆ–è€… nn.ReLU(inplace=True)
                )
            )
        final_in_channels = (num_layers + 1) * constant
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels, self.constant, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.constant),
            nn.GELU()
        )


    def forward(self, input_features: Tensor) -> Tensor:

        features = [input_features]
        current_input_to_dense_layer = input_features

        for i in range(self.num_layers):
            if i > 0:
               concat_for_shuffle = torch.cat(features, dim=1)
               num_parts_for_shuffle = len(features)
               shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)

               current_input_to_dense_layer = self.reduction_modules[i-1](shuffled_features)

            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        final_output = self.final_reduction(shuffled_final_output)
        
        return final_output

class DenseRepViTBlock_(nn.Module):
    def __init__(self, c1, c2, constant, num_layers,use_se,stride=1,use_hs=True):
        super().__init__()
        assert c1 == c2
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.constant = constant

        for _ in range(num_layers):
            self.layers.append (
                RepViTBlock(c1, c2, use_se=use_se, stride=stride, use_hs=use_hs
                )
            )
        self.reduction_modules = nn.ModuleList()
        for i in range(num_layers-1):
            num_tensors_to_concatenate = i + 2
            in_channels_for_reduction = num_tensors_to_concatenate * constant

            self.reduction_modules.append(
                nn.Sequential(
                    nn.Conv2d(in_channels_for_reduction, self.constant, kernel_size=1, bias=False),
                    nn.BatchNorm2d(constant),
                    nn.GELU() # æˆ–è€… nn.ReLU(inplace=True)
                )
            )
        final_in_channels = (num_layers + 1) * constant
        self.final_reduction = nn.Sequential(
            nn.Conv2d(final_in_channels, self.constant, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.constant),
            nn.GELU()
        )


    def forward(self, input_features: Tensor) -> Tensor:

        features = [input_features]
        current_input_to_dense_layer = input_features

        for i in range(self.num_layers):
            if i > 0:
               concat_for_shuffle = torch.cat(features, dim=1)
               num_parts_for_shuffle = len(features)
               shuffled_features = channel_shuffle(concat_for_shuffle, groups=num_parts_for_shuffle)

               current_input_to_dense_layer = self.reduction_modules[i-1](shuffled_features)

            
            layer_output = self.layers[i](current_input_to_dense_layer)
            features.append(layer_output)
        
        final_block_output = torch.cat(features, dim=1)
        shuffled_final_output = channel_shuffle(final_block_output, groups=len(features))
        final_output = self.final_reduction(shuffled_final_output)
        
        return final_output


class ChannelAttention(nn.Module):
    """
    è®ºæ–‡ä¸­æè¿°çš„é€šé“æ³¨æ„åŠ›æ¨¡å— (Channel Attention Module, CA)ã€‚
    è¿™ä¸ªå®ç°ä¸¥æ ¼éµå¾ªäº†å…¬å¼(5)çš„é€»è¾‘ã€‚
    """
    def __init__(self, in_channels, reduction_ratio=16):
        """
        åˆå§‹åŒ–é€šé“æ³¨æ„åŠ›æ¨¡å—ã€‚
        :param in_channels: è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚
        :param reduction_ratio: é€šé“ç¼©å‡ç‡rï¼Œç”¨äºMLPçš„ç“¶é¢ˆå±‚ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œä½†16æ˜¯å¸¸ç”¨å€¼ã€‚
        """
        super(ChannelAttention, self).__init__()
        # æ£€æŸ¥ç¼©å‡ç‡æ˜¯å¦åˆç†
        if in_channels <= reduction_ratio:
            # å¦‚æœè¾“å…¥é€šé“æ•°æœ¬èº«å°±å¾ˆå°ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥é€šé“æ•°çš„ä¸€åŠæˆ–è€…1ä½œä¸ºä¸­é—´é€šé“
            # é¿å…é™ç»´åé€šé“æ•°ä¸º0æˆ–è´Ÿæ•°
            mip_channels = in_channels // 2 if in_channels > 1 else 1
        else:
            mip_channels = in_channels // reduction_ratio
        # 1. Squeeze æ“ä½œ: å…¨å±€å¹³å‡æ± åŒ–å’Œå…¨å±€æœ€å¤§æ± åŒ–
        # è¿™ä¸¤ä¸ªæ“ä½œåœ¨forwardå‡½æ•°ä¸­ç›´æ¥è°ƒç”¨F.adaptive_avg_pool2då’ŒF.adaptive_max_pool2då®ç°
        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦å®šä¹‰å±‚
        # 2. Shared MLP: ä¸€ä¸ªå…±äº«çš„å¤šå±‚æ„ŸçŸ¥æœº
        # ä½¿ç”¨1x1å·ç§¯æ¥å®ç°å…¨è¿æ¥å±‚ï¼Œè¿™æ˜¯CNNä¸­çš„æ ‡å‡†åšæ³•
        self.shared_mlp = nn.Sequential(
            # ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼Œå¯¹åº” W0ï¼Œç”¨äºé™ç»´
            nn.Conv2d(in_channels, mip_channels, kernel_size=1, bias=False),
            # ReLUæ¿€æ´»å‡½æ•°ï¼Œå¯¹åº” Î´
            nn.ReLU(inplace=True),
            # ç¬¬äºŒä¸ª1x1å·ç§¯ï¼Œå¯¹åº” W1ï¼Œç”¨äºå‡ç»´
            nn.Conv2d(mip_channels, in_channels, kernel_size=1, bias=False)
        )
        
        # 3. Sigmoid æ¿€æ´»å‡½æ•°ï¼Œå¯¹åº” Ïƒ
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚
        :param x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå°ºå¯¸ (B, C, H, W)
        :return: ç»è¿‡é€šé“æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾å›¾ï¼Œå°ºå¯¸ (B, C, H, W)
        """
        # ä¿å­˜åŸå§‹è¾“å…¥ï¼Œç”¨äºæœ€åçš„ä¹˜æ³•
        original_input = x
        # è·å–è¾“å…¥å°ºå¯¸
        B, C, H, W = x.size()
        # Squeeze æ“ä½œ
        # å…¨å±€å¹³å‡æ± åŒ– -> (B, C, 1, 1)
        avg_pool_out = F.adaptive_avg_pool2d(x, (1, 1))
        # å…¨å±€æœ€å¤§æ± åŒ– -> (B, C, 1, 1)
        max_pool_out = F.adaptive_max_pool2d(x, (1, 1))
        # Shared MLP æ“ä½œ
        # åˆ†åˆ«é€šè¿‡å…±äº«çš„MLP
        avg_mlp_out = self.shared_mlp(avg_pool_out)
        max_mlp_out = self.shared_mlp(max_pool_out)
        # Merge æ“ä½œ: å…ƒç´ çº§ç›¸åŠ 
        merged_out = avg_mlp_out + max_mlp_out
        # Excitation æ“ä½œ: Sigmoid
        attention_weights = self.sigmoid(merged_out)
        # Reweight æ“ä½œ: å…ƒç´ çº§ç›¸ä¹˜
        # åˆ©ç”¨å¹¿æ’­æœºåˆ¶ (B, C, 1, 1) -> (B, C, H, W)
        output = original_input * attention_weights
        
        return output

class FeatureFusionAttention(nn.Module):
    """
    A module to fuse two input feature maps, process them through a 
    depthwise separable convolution, and apply channel attention.
    """
    def __init__(self, c1, c_out):
        """
        Initializes the feature fusion and attention module.
        :param c1: The combined number of input channels from the two sources (auto-calculated by parser).
        :param c_out: The desired number of output channels.
        """
        super(FeatureFusionAttention, self).__init__()
        self.dw_conv = DWConv(c1, c_out)
        self.channel_attention = ChannelAttention(c_out)

    def forward(self, inputs):
        """
        Forward pass for the fusion module.
        :param inputs: A list of two tensors [x1, x2]. 
                       x1 is assumed to have the target spatial dimensions (e.g., H/4, W/4).
        :return: The processed feature map.
        """
        x1, x2 = inputs
        
        # Resize the second input (x2) to match the spatial dimensions of the first (x1)
        if x2.shape[2:] != x1.shape[2:]:
            x2_resized = F.interpolate(x2, size=x1.shape[2:], mode='bilinear', align_corners=False)
        else:
            x2_resized = x2
            
        # Concatenate the features along the channel dimension
        fused_features = torch.cat([x1, x2_resized], dim=1)
        
        # Pass through the depthwise separable convolution
        extracted_features = self.dw_conv(fused_features)
        
        # Apply channel attention
        final_output = self.channel_attention(extracted_features)
        
        return final_output

class LoGFilter(nn.Module):
    """
    é«˜æ–¯-æ‹‰æ™®æ‹‰æ–¯æ»¤æ³¢å™¨æ¨¡å—ã€‚
    è¿™ä¸ªå®ç°å€Ÿé‰´è‡ªlegnet.pyï¼Œä½†è¿›è¡Œäº†ç®€åŒ–å’Œé€‚é…ï¼Œä½¿å…¶è‡ªåŒ…å«ã€‚
    """
    def __init__(self, in_c, out_c, kernel_size, sigma):
        super(LoGFilter, self).__init__()
        
        # --- åˆ›å»ºå¹¶å›ºå®šLoGå·ç§¯æ ¸ ---
        ax = torch.arange(-(kernel_size // 2), (kernel_size // 2) + 1, dtype=torch.float32)
        xx, yy = torch.meshgrid(ax, ax, indexing='ij')
        
        # é«˜æ–¯-æ‹‰æ™®æ‹‰æ–¯å…¬å¼
        term1 = (xx**2 + yy**2 - 2 * sigma**2) / (2 * math.pi * sigma**4)
        term2 = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))
        kernel = term1 * term2
        
        # å½’ä¸€åŒ–
        kernel = kernel - kernel.mean()
        # if torch.sum(kernel) != 0:
        #     kernel = kernel / torch.sum(kernel)
        l1_norm = torch.sum(torch.abs(kernel))
        if l1_norm > 1e-6: # å¢åŠ ä¸€ä¸ªå°çš„epsilonæ¥åˆ¤æ–­
            kernel = kernel / l1_norm
            
        log_kernel = kernel.unsqueeze(0).unsqueeze(0).repeat(out_c, 1, 1, 1)

        # LoGæ»¤æ³¢å±‚
        self.log_conv = nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=1, padding=kernel_size // 2, groups=out_c, bias=False)
        self.log_conv.weight.data = log_kernel
        self.log_conv.weight.requires_grad = False

        # åç»­å¤„ç†å±‚ (ç¡¬ç¼–ç )
        self.norm = nn.BatchNorm2d(out_c)
        self.act = nn.GELU()

    def forward(self, x):
        log_features = self.log_conv(x)
        log_edge = self.act(self.norm(log_features))
        return log_edge
    
class EG_stem(nn.Module):
    """
    A simplified stem block that applies a single depthwise convolution
    and adds the result back to the input (residual connection).
    """
    def __init__(self, channels):
        super(EG_stem, self).__init__()
        self.dw_conv = DWConv(channels, channels)

    def forward(self, x):
        return x + self.dw_conv(x)
    

class C3k(C3):
    """C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):
        """Initializes the C3k module with specified channels, number of layers, and configurations."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))

class C3k2(C2f):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):
        """Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks."""
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(
            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)
        )

class PSABlock(nn.Module):
    """
    PSABlock class implementing a Position-Sensitive Attention block for neural networks.

    This class encapsulates the functionality for applying multi-head attention and feed-forward neural network layers
    with optional shortcut connections.

    Attributes:
        attn (Attention): Multi-head attention module.
        ffn (nn.Sequential): Feed-forward neural network module.
        add (bool): Flag indicating whether to add shortcut connections.

    Methods:
        forward: Performs a forward pass through the PSABlock, applying attention and feed-forward layers.

    Examples:
        Create a PSABlock and perform a forward pass
        >>> psablock = PSABlock(c=128, attn_ratio=0.5, num_heads=4, shortcut=True)
        >>> input_tensor = torch.randn(1, 128, 32, 32)
        >>> output_tensor = psablock(input_tensor)
    """

    def __init__(self, c, attn_ratio=0.5, num_heads=4, shortcut=True) -> None:
        """Initializes the PSABlock with attention and feed-forward layers for enhanced feature extraction."""
        super().__init__()

        self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)
        self.ffn = nn.Sequential(Conv(c, c * 2, 1), Conv(c * 2, c, 1, act=False))
        self.add = shortcut

    def forward(self, x):
        """Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor."""
        x = x + self.attn(x) if self.add else self.attn(x)
        x = x + self.ffn(x) if self.add else self.ffn(x)
        return x
    
class C2PSA(nn.Module):
    """
    C2PSA module with attention mechanism for enhanced feature extraction and processing.

    This module implements a convolutional block with attention mechanisms to enhance feature extraction and processing
    capabilities. It includes a series of PSABlock modules for self-attention and feed-forward operations.

    Attributes:
        c (int): Number of hidden channels.
        cv1 (Conv): 1x1 convolution layer to reduce the number of input channels to 2*c.
        cv2 (Conv): 1x1 convolution layer to reduce the number of output channels to c.
        m (nn.Sequential): Sequential container of PSABlock modules for attention and feed-forward operations.

    Methods:
        forward: Performs a forward pass through the C2PSA module, applying attention and feed-forward operations.

    Notes:
        This module essentially is the same as PSA module, but refactored to allow stacking more PSABlock modules.

    Examples:
        >>> c2psa = C2PSA(c1=256, c2=256, n=3, e=0.5)
        >>> input_tensor = torch.randn(1, 256, 64, 64)
        >>> output_tensor = c2psa(input_tensor)
    """

    def __init__(self, c1, c2, n=1, e=0.5):
        """Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio."""
        super().__init__()
        assert c1 == c2
        self.c = int(c1 * e)
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv(2 * self.c, c1, 1)

        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))

    def forward(self, x):
        """Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor."""
        a, b = self.cv1(x).split((self.c, self.c), dim=1)
        b = self.m(b)
        return self.cv2(torch.cat((a, b), 1))

class to_channels_first(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.permute(0, 3, 1, 2)


class to_channels_last(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.permute(0, 2, 3, 1)

from other_model.InternImage.detection.ops_dcnv3 import modules as dcnv3
# å‡è®¾ä»¥ä¸‹æ¨¡å—å·²æŒ‰éœ€å¯¼å…¥
import torch
import torch.nn as nn

# --- è¾…åŠ©æ¨¡å— (ç¡®ä¿è¿™äº›å®šä¹‰å­˜åœ¨) ---

class to_channels_first(nn.Module):
    """å°†å¼ é‡ä» (B, H, W, C) è½¬æ¢ä¸º (B, C, H, W)"""
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return x.permute(0, 3, 1, 2)

class to_channels_last(nn.Module):
    """å°†å¼ é‡ä» (B, C, H, W) è½¬æ¢ä¸º (B, H, W, C)"""
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return x.permute(0, 2, 3, 1)

class DeformableDW(nn.Module):
    def __init__(self, c1, c2, stride, offset_scale):
        super().__init__()
        self.to_last = to_channels_last()
        self.to_first = to_channels_first()
        # DCNæœ¬èº«ä¸æ”¹å˜é€šé“æ•°ï¼Œæ‰€ä»¥å®ƒåœ¨ c1 é€šé“ä¸Šå·¥ä½œ
        self.dcn = dcnv3.DCNv3(
            channels=c1,
            kernel_size=3,
            stride=stride,
            pad=1,
            group=c1, # æ·±åº¦å¯åˆ†ç¦»
            offset_scale=offset_scale
        )
        self.bn_dcn = nn.BatchNorm2d(c1)
        
        # ä½¿ç”¨ä¸€ä¸ª 1x1 å·ç§¯æ¥æ”¹å˜é€šé“æ•°ä» c1 -> c2
        self.pw_conv = nn.Conv2d(c1, c2, kernel_size=1, bias=False)
        self.bn_pw = nn.BatchNorm2d(c2)

    def forward(self, x):
        # DCN (c1 -> c1)
        x_last = self.to_last(x)
        deformed_x_last = self.dcn(x_last)
        deformed_x = self.to_first(deformed_x_last)
        x_dcn_bn = self.bn_dcn(deformed_x)
        
        # Pointwise Conv (c1 -> c2)
        output = self.pw_conv(x_dcn_bn)
        return self.bn_pw(output)

class DeformableViTBlock(nn.Module):
    """
    ä¸€ä¸ªé›†æˆäº†å¯å˜å‹æ·±åº¦å·ç§¯å’Œ FFN çš„å®Œæ•´å—ã€‚
    æ­¤ç‰ˆæœ¬ä¸ºé€‚é… YOLOv8 çš„é…ç½®æ–‡ä»¶å’Œè§£æå™¨è€Œç‰¹åˆ«ä¿®æ”¹ã€‚
    """
    def __init__(self, c1, c2, stride=1, mlp_ratio=2, use_se=True, offset_scale=1.0, act_layer=nn.GELU):
        """
        Args:
            c1 (int): è¾“å…¥é€šé“æ•°.
            c2 (int): è¾“å‡ºé€šé“æ•°.
            stride (int): æ­¥é•¿, 1 æˆ– 2.
            mlp_ratio (float): FFN éšè—å±‚çš„æ‰©å±•æ¯”ä¾‹.
            offset_scale (float): DCNv3 çš„åç§»ç¼©æ”¾ç³»æ•°.
            act_layer (nn.Module): æ¿€æ´»å‡½æ•°.
        """
        super().__init__()
        # ç¡®ä¿ stride=1 æ—¶è¾“å…¥è¾“å‡ºé€šé“ä¸€è‡´ï¼Œè¿™æ˜¯ YOLO ä¸­éä¸‹é‡‡æ ·å—çš„å¸¸è§çº¦æŸ
        if stride == 1:
            assert c1 == c2, "stride=1æ—¶, c1 å’Œ c2 å¿…é¡»ç›¸ç­‰ï¼"
        
        hidden_channels = int(c2 * mlp_ratio)

        # 1. å¯å˜å‹å·ç§¯éƒ¨åˆ† (Token Mixer)
        #    å®ƒçš„ä»»åŠ¡æ˜¯å°† c1 -> c2ï¼ŒåŒæ—¶æ ¹æ® stride è¿›è¡Œä¸‹é‡‡æ ·
        self.token_mixer = nn.Sequential(
            DeformableDW(c1, c2, stride, offset_scale),
            # SEæ¨¡å—åœ¨ DeformableDW çš„è¾“å‡º c2 ä¸Šå·¥ä½œ
            SqueezeExcite(c2) if use_se else nn.Identity() 
        )

        # 2. FFN éƒ¨åˆ† (Channel Mixer)
        #    å®ƒåœ¨ c2 é€šé“ä¸Šå·¥ä½œï¼Œå¹¶å¸¦æœ‰æ®‹å·®è¿æ¥
        self.norm = nn.BatchNorm2d(c2)
        self.ffn = nn.Sequential(
            nn.Conv2d(c2, hidden_channels, kernel_size=1, bias=False),
            act_layer(),
            nn.Conv2d(hidden_channels, c2, kernel_size=1, bias=False)
        )

    def forward(self, x):
        # 1. Token Mixer (c1 -> c2)
        x = self.token_mixer(x)
        
        # 2. FFN with residual (åœ¨ c2 ä¸Šå·¥ä½œ)
        x = x + self.ffn(self.norm(x))
        
        return x

import torch
import torch.nn as nn
import torch.nn.functional as F


class ConvBNR(nn.Module):
    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):
        super(ConvBNR, self).__init__()

        self.block = nn.Sequential(
            nn.Conv2d(inplanes, planes, kernel_size, stride=stride, padding=dilation, dilation=dilation, bias=bias),
            nn.BatchNorm2d(planes),
            nn.GELU()
        )

    def forward(self, x):
        return self.block(x)

class Conv1x1(nn.Module):
    def __init__(self, inplanes, planes):
        super(Conv1x1, self).__init__()
        self.conv = nn.Conv2d(inplanes, planes, 1)
        self.bn = nn.BatchNorm2d(planes)
        self.Gelu = nn.GELU()

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.Gelu(x)

        return x

class Eage_detect(nn.Module):
    """
    è¾¹ç¼˜é—¨æ§æ¨¡å— (å·²é‡æ„ä¸ºä½¿ç”¨SobelGateå­æ¨¡å—)ã€‚
    
    åŠŸèƒ½: å¯¹è¾“å…¥çš„å•ä¸ªç‰¹å¾å›¾ï¼Œé€šè¿‡SobelGateæ¨¡å—è·å–è¾¹ç¼˜é—¨æ§ä¿¡å·ï¼Œ
          ä¸åŸå§‹è¾“å…¥ç›¸ä¹˜ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªDWå·ç§¯å¤„ç†ã€‚
    """
    def __init__(self, c1):
        """
        Args:
            c1 (int): è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚
        """
        super(Eage_detect, self).__init__()
        
        # 1. å®ä¾‹åŒ–Sobelé—¨æ§ä¿¡å·ç”Ÿæˆå™¨
        self.sobel_gate = SobelGate(c1)
        
        # 2. æœ€åçš„æ·±åº¦å·ç§¯ï¼Œè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸å˜
        self.dw_conv_out = DWConv(c1, c1, kernel_size=3)

    def forward(self, x):
        """
        Args:
            x (Tensor): è¾“å…¥ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º [B, C, H, W]ã€‚
        """
        # a. ä½¿ç”¨SobelGateæ¨¡å—è·å–é—¨æ§ä¿¡å·
        gate_signal = self.sobel_gate(x)
        
        # b. ä¸åŸå§‹è¾“å…¥ç›¸ä¹˜
        gated_features = x * gate_signal
        
        # c. ç»è¿‡ä¸€ä¸ªDWå·ç§¯
        output = self.dw_conv_out(gated_features)
        
        return output

class Edge_guide(nn.Module):
    """
    (å·²ä¿®æ”¹)
    A module to fuse two input feature maps. It aligns the spatial dimensions of
    the first input (x1) to match the second (x2) using fixed 2x pooling.
    The pooling combines both max and average pooling for richer features.
    """
    def __init__(self, c_in1, c_in2, c_out):
        """
        Initializes the Edge_guide module.
        Args:
            c_in1 (int): Number of channels for the first input (x1).
            c_in2 (int): Number of channels for the second input (x2).
            c_out (int): Number of output channels.
        """
        super(Edge_guide, self).__init__()
        # 1. Reduction convolution for the resized x1.
        # Input will be concatenation of max-pooled x1 and avg-pooled x1,
        # so the channel count is 2 * c_in1. Output should match original x1.
        self.resize_reduction = Conv(c_in1 * 2, c_in1, 1)

        # 2. DWConv receives the concatenation of resized x1 (c_in1) and x2 (c_in2).
        self.dw_conv = DWConv(c_in1 + c_in2, c_out)
        self.channel_attention = ECA(c_out)
        self.sa = SAM()

    def forward(self, inputs):
        """
        Forward pass for the fusion module.
        :param inputs: A list of two tensors [x1, x2].
                       x2 is assumed to have the target spatial dimensions.
        """
        x1, x2 = inputs

        # --- æ ¸å¿ƒä¿®æ”¹: å°ºå¯¸å¯¹é½é€»è¾‘ ---
        if x1.shape[2:] != x2.shape[2:]:
            # If downsampling is needed (assuming 2x)
            if x1.shape[2] > x2.shape[2]:
                # 1. Create two feature maps using fixed 2x pooling
                x1_max = F.max_pool2d(x1, kernel_size=2, stride=2)
                x1_avg = F.avg_pool2d(x1, kernel_size=2, stride=2)
                
                # 2. Concatenate them along the channel dimension
                x1_cat = torch.cat([x1_max, x1_avg], dim=1)
                
                # 3. Use a 1x1 convolution to reduce dimensions and fuse information
                x1_resized = self.resize_reduction(x1_cat)

            # If upsampling is needed (fallback case)
            else:
                x1_resized = F.interpolate(x1, size=x2.shape[2:], mode='bilinear', align_corners=False)
        else:
            # If shapes are already the same, no resizing is needed
            x1_resized = x1

        # --- åç»­åŠŸèƒ½ (ä¿æŒä¸å˜) ---
        fused_features = torch.cat([x1_resized, x2], dim=1)
        extracted_features = self.dw_conv(fused_features)
        final_output = self.channel_attention(extracted_features)
        final_output = final_output*self.sa(final_output)

        return final_output

# class Edge_guide(nn.Module):
#     """
#     (å·²ä¿®æ”¹)
#     A module to fuse two input feature maps, ensuring the output spatial 
#     dimensions match the SECOND input (x2), which is assumed to be P2 (H/4, W/4).
#     """
#     def __init__(self, c1, c_out):
#         super(Edge_guide, self).__init__()
#         self.dw_conv = DWConv(c1, c_out)
#         self.channel_attention = ChannelAttention(c_out)

#     def forward(self, inputs):
#         """
#         Forward pass for the fusion module.
#         :param inputs: A list of two tensors [x1, x2].
#                        x2 is now assumed to have the target spatial dimensions (H/4, W/4).
#         """
#         x1, x2 = inputs
        
#         # --- æ ¸å¿ƒä¿®æ”¹ ---
#         # ä»¥ x2 çš„å°ºå¯¸ä¸ºåŸºå‡†ï¼Œå¯¹é½ x1
#         if x1.shape[2:] != x2.shape[2:]:
#             # x1 (æ¥è‡ªEage_detect, P1å°ºå¯¸) é€šå¸¸æ¯” x2 (æ¥è‡ªä¸»å¹², P2å°ºå¯¸) å¤§ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯ä¸‹é‡‡æ ·
#             if x1.shape[2] > x2.shape[2]:
#                 if x1.shape[2] % x2.shape[2] == 0:
#                     stride = x1.shape[2] // x2.shape[2]
#                     x1_resized = F.max_pool2d(x1, kernel_size=stride, stride=stride)
#                 else:
#                     x1_resized = F.adaptive_max_pool2d(x1, x2.shape[2:])
#             # å¦‚æœx1æ›´å°ï¼Œåˆ™ä¸Šé‡‡æ · (ä»¥é˜²ä¸‡ä¸€)
#             else:
#                 x1_resized = F.interpolate(x1, size=x2.shape[2:], mode='bilinear', align_corners=False)
#         else:
#             x1_resized = x1
            
#         # Concatenate a resized x1 with the original x2
#         # æ³¨æ„æ‹¼æ¥é¡ºåºå¯ä»¥è°ƒæ•´ï¼Œè¿™é‡Œä¿æŒx2åœ¨åï¼Œä¸è¾“å…¥é¡ºåºä¸€è‡´
#         fused_features = torch.cat([x1_resized, x2], dim=1)
        
#         extracted_features = self.dw_conv(fused_features)
#         final_output = self.channel_attention(extracted_features)
        
#         return final_output


class Fuse_Features(nn.Module):
    """
    (å·²é‡æ„)
    A robust module to fuse two input feature maps (x1, x2).
    It aligns the spatial dimensions of x2 to match x1. If downsampling is needed,
    it uses a combination of max and average pooling for richer feature preservation.
    Finally, it processes the fused tensor with a reduction convolution and attention.
    """
    def __init__(self, c_in1, c_in2, c_out):
        """
        Initializes the Fuse_Features module.
        :param c_in1: Number of channels for the first input feature map (x1).
        :param c_in2: Number of channels for the second input feature map (x2).
        :param c_out: The desired number of output channels.
        """
        super().__init__()
        # Convolution to process the multi-pooled (max + avg) x2 during resizing.
        # Input is 2 * c_in2, output is c_in2.
        self.resize_reduction = Conv(c_in2 * 2, c_in2, 1)

        # Main reduction convolution for the final fused features.
        # Input is concatenation of x1 (c_in1) and aligned x2 (c_in2).
        self.reduction = Conv(c_in1 + c_in2, c_out, 1)
        
        # Standard CBAM-style attention: ECA (channel) then SAM (spatial)
        self.eca = ECA(c_out)
        self.sa = SAM()

    def _align_features(self, x1, x2):
        """
        Aligns the spatial dimensions of x2 to match x1.
        """
        h1, w1 = x1.shape[2:]
        h2, w2 = x2.shape[2:]

        if h1 == h2 and w1 == w2:
            return x2

        # Downsample x2 if it's larger than x1, using the multi-pool strategy
        if h2 > h1 or w2 > w1:
            # Use fixed pooling if it's a simple integer downscale, otherwise adaptive
            if h2 % h1 == 0 and w2 % w1 == 0:
                stride_h, stride_w = h2 // h1, w2 // w1
                x2_max = F.max_pool2d(x2, kernel_size=(stride_h, stride_w), stride=(stride_h, stride_w))
                x2_avg = F.avg_pool2d(x2, kernel_size=(stride_h, stride_w), stride=(stride_h, stride_w))
            else: # Fallback to adaptive pooling for non-integer scales
                x2_max = F.adaptive_max_pool2d(x2, (h1, w1))
                x2_avg = F.adaptive_avg_pool2d(x2, (h1, w1))
            
            # Concatenate pooled features and reduce
            x2_cat = torch.cat([x2_max, x2_avg], dim=1)
            return self.resize_reduction(x2_cat)

        # Upsample x2 if it's smaller than x1
        else:
            return F.interpolate(x2, size=(h1, w1), mode='bilinear', align_corners=False)

    def forward(self, x):
        """
        Forward pass for the fusion module.
        """
        x1, x2 = x
        
        # 1. Align the spatial dimensions of x2 to match x1 FIRST.
        # This is done on the original x2 and incorporates the multi-pool logic.
        x2_aligned = self._align_features(x1, x2)
            
        # 2. Concatenate the original x1 and the aligned x2.
        fused = self.reduction(torch.cat([x1, x2_aligned], 1))
        
        # 3. Apply attention mechanisms sequentially (CBAM style).
        fused_after_eca = self.eca(fused)
        attention_map_sa = self.sa(fused_after_eca)
        final_output = fused_after_eca * attention_map_sa

        return final_output


class Edge_Emphasize(nn.Module):
    def __init__(self, channel):
        super(Edge_Emphasize, self).__init__()
        t = int(abs((math.log(channel, 2) + 1) / 2))
        k = t if t % 2 else t + 1
        self.conv2d = nn.Sequential(
            # æ·±åº¦å·ç§¯ (Depthwise)
            nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False),
            nn.BatchNorm2d(channel),
            nn.GELU(),
            
            # é€ç‚¹å·ç§¯ (Pointwise)
            nn.Conv2d(channel, channel, kernel_size=1, bias=False),
            nn.BatchNorm2d(channel),
            nn.GELU()
        )
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv1d = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
        self.SAM = SAM()
        self.final_conv = ConvBNR(channel, channel)

    def forward(self, inputs):
        # The model parser provides inputs as a list for multi-source layers.
        # We unpack it here to match the user's desired `(c, att)` signature.
        in1, in2 = inputs
        # Intelligently assign feature map 'c' and attention 'att' based on channel count
        if in1.shape[1] == 1 and in2.shape[1] > 1:
            att, c = in1, in2
        else: # Default to assuming [feature, attention] order, which is the corrected YAML order
            c, att = in1, in2

        # This is the user-specified forward logic.
        # The spatial dimension check is more robust than comparing the full tensor size tuples.
        if c.shape[2:] != att.shape[2:]:
            att = F.interpolate(att, c.size()[2:], mode='bilinear', align_corners=False)
        
        x = c * att + c
        x = self.conv2d(x)
        wei = self.avg_pool(x)
        wei = self.conv1d(wei.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        wei = self.sigmoid(wei)
        x = x * wei
        SAM_wei = self.SAM(x)
        out = x * SAM_wei

        out = out+c
        out = self.final_conv(out)

        return out


import torch
import torch.nn as nn

class DWConv(nn.Module):
    """
    ä¸€ä¸ªå·²ç»å®šä¹‰å¥½çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¨¡å— (å‡è®¾)ã€‚
    æ³¨æ„ï¼šè¿™é‡Œæˆ‘å‡è®¾ DWConv æ¥æ”¶ in_channels å’Œ out_channelsã€‚
    å¦‚æœæ‚¨çš„ DWConv å®ç°ä¸åŒ (ä¾‹å¦‚ï¼Œåªæ¥æ”¶ä¸€ä¸ªchannelå‚æ•°)ï¼Œæ‚¨å¯èƒ½éœ€è¦ç¨ä½œè°ƒæ•´ã€‚
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, act=nn.GELU):
        super(DWConv, self).__init__()
        # è‡ªåŠ¨è®¡ç®—paddingä»¥ä¿æŒç©ºé—´å°ºå¯¸
        padding = (kernel_size - 1) // 2
        
        self.sequential_ops = nn.Sequential(
            # æ·±åº¦å·ç§¯
            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=False),
            nn.BatchNorm2d(in_channels),
            act(),
            
            # é€ç‚¹å·ç§¯
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            act()
        )

    def forward(self, x):
        return self.sequential_ops(x)

from torch.cuda.amp import autocast
class Scharr(nn.Module):
    def __init__(self, channel):
        super(Scharr, self).__init__()
        self.epsilon = 1e-6
        # å®šä¹‰Scharræ»¤æ³¢å™¨
        scharr_x = torch.tensor([[-3., 0., 3.], [-10., 0., 10.], [-3., 0., 3.]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        scharr_y = torch.tensor([[-3., -10., -3.], [0., 0., 0.], [3., 10., 3.]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        # ä½¿ç”¨å›ºå®šçš„ã€ä¸å¯å­¦ä¹ çš„å·ç§¯å±‚æ¥å®ç°Scharræ»¤æ³¢
        self.conv_x = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        self.conv_y = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        
        self.conv_x.weight.data = scharr_x.repeat(channel, 1, 1, 1)
        self.conv_y.weight.data = scharr_y.repeat(channel, 1, 1, 1)
        
        # æƒé‡ä¸å¯å­¦ä¹ 
        self.conv_x.weight.requires_grad = False
        self.conv_y.weight.requires_grad = False

        # ä½¿ç”¨æ ‡å‡†çš„PyTorchå±‚
        self.norm = nn.BatchNorm2d(channel)
        self.act = nn.GELU() # æˆ–è€… nn.ReLU()
        self.conv_extra = DWConv(channel, channel) # ä½¿ç”¨è‡ªåŒ…å«çš„DWConv

        # self.register_forward_hook(inspect_forward_output)
        # print(f"âœ… Forward hook registered on instance of {self.__class__.__name__} with {channel} channels.")

    def forward(self, x):
        # ==========================================================
        # forward æ–¹æ³•æ˜¯å”¯ä¸€è¢«ä¿®æ”¹çš„éƒ¨åˆ†
        # æ ¸å¿ƒæ€æƒ³ï¼šæ˜¾å¼åœ°å°†è®¡ç®—è¿‡ç¨‹è½¬æ¢åˆ° float32ï¼Œå®Œæˆåå†è½¬æ¢å›åŸå§‹ç±»å‹ã€‚
        # ==========================================================
        # --- æ ¸å¿ƒæ”¹åŠ¨ 1: ä¿å­˜è¾“å…¥çš„åŸå§‹æ•°æ®ç±»å‹ ---
        # è¿™æ ·æ¨¡å—åœ¨è®¡ç®—å®Œæˆåå¯ä»¥æ¢å¤å®ƒï¼Œå¯¹å¤–éƒ¨ç½‘ç»œé€æ˜ã€‚
        input_dtype = x.dtype
        # --- æ ¸å¿ƒæ”¹åŠ¨ 2: å°†è¾“å…¥æ˜¾å¼è½¬æ¢ä¸º float32 ---
        # è¿™æ˜¯ç¡®ä¿åç»­æ‰€æœ‰è®¡ç®—éƒ½åœ¨é«˜ç²¾åº¦ä¸‹è¿›è¡Œçš„æœ€å¯é æ–¹æ³•ã€‚
        x_f32 = x.to(torch.float32)
        # (ç§»é™¤äº†åŸæœ‰çš„ with autocast(...) å—ï¼Œå› ä¸ºå®ƒä¸èµ·ä½œç”¨)
        # 1. Scharr å·ç§¯ (ç°åœ¨è¾“å…¥æ˜¯ x_f32ï¼Œæ‰€ä»¥è®¡ç®—æ˜¯ float32)
        edges_x = self.conv_x(x_f32)
        edges_y = self.conv_y(x_f32)
    
        # 2. è®¡ç®—å¹³æ–¹å’Œ (åœ¨ float32 ä¸Šè¿›è¡Œï¼Œå®‰å…¨æ— æº¢å‡ºé£é™©)
        gradient_magnitude_squared = edges_x.pow(2) + edges_y.pow(2)
    
        # 3. å¼€æ–¹ (åœ¨ float32 ä¸Šè¿›è¡Œï¼Œç»“æœæ›´ç²¾ç¡®)
        gradient_magnitude = torch.sqrt(gradient_magnitude_squared + self.epsilon)
    
        # 4. åç»­å¤„ç† (ä»åœ¨ float32 ä¸‹è¿›è¡Œ)
        scharr_edge = self.act(self.norm(gradient_magnitude))
        
        # 5. æ®‹å·®è¿æ¥ (ä¸¤ä¸ªæ“ä½œæ•° x_f32 å’Œ scharr_edge éƒ½æ˜¯ float32)
        fused_output = self.conv_extra(x_f32 + scharr_edge)
        
        # --- æ ¸å¿ƒæ”¹åŠ¨ 3: å°†æœ€ç»ˆè¾“å‡ºè½¬æ¢å›åŸå§‹è¾“å…¥ç±»å‹ ---
        # è¿™ä½¿å¾—æ¨¡å—æˆä¸ºä¸€ä¸ªâ€œè‰¯å¥½å…¬æ°‘â€ï¼Œæ— ç¼è¡”æ¥å¤–éƒ¨çš„AMPä¸Šä¸‹æ–‡ã€‚
        # å¦‚æœå¤–éƒ¨æ˜¯float16ï¼Œå®ƒå°±è¾“å‡ºfloat16ï¼›å¦‚æœå¤–éƒ¨æ˜¯float32ï¼Œå®ƒå°±è¾“å‡ºfloat32ã€‚
        return fused_output.to(input_dtype)

class Sobel(nn.Module):
    def __init__(self, channel):
        super(Sobel, self).__init__()
        self.epsilon = 1e-6
        
        # å®šä¹‰Sobelæ»¤æ³¢å™¨
        sobel_x = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        sobel_y = torch.tensor([[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        # ä½¿ç”¨å›ºå®šçš„ã€ä¸å¯å­¦ä¹ çš„å·ç§¯å±‚æ¥å®ç°Sobelæ»¤æ³¢
        self.conv_x = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        self.conv_y = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        
        self.conv_x.weight.data = sobel_x.repeat(channel, 1, 1, 1)
        self.conv_y.weight.data = sobel_y.repeat(channel, 1, 1, 1)
        
        # æƒé‡ä¸å¯å­¦ä¹ 
        self.conv_x.weight.requires_grad = False
        self.conv_y.weight.requires_grad = False

        # ä½¿ç”¨æ ‡å‡†çš„PyTorchå±‚
        self.norm = nn.BatchNorm2d(channel)
        self.act = nn.GELU() # æˆ–è€… nn.ReLU()
        self.conv_extra = DWConv(channel, channel)

    def forward(self, x):
        # forward æ–¹æ³•ä¸æ‚¨çš„ Scharr æ¨¡å—å®Œå…¨ç›¸åŒï¼Œä»¥ç¡®ä¿æ··åˆç²¾åº¦ä¸‹çš„ç¨³å®šæ€§
        
        # 1. ä¿å­˜è¾“å…¥çš„åŸå§‹æ•°æ®ç±»å‹
        input_dtype = x.dtype
        
        # 2. å°†è¾“å…¥æ˜¾å¼è½¬æ¢ä¸º float32
        x_f32 = x.to(torch.float32)
        
        # 3. Sobel å·ç§¯ (åœ¨ float32 ä¸Šè¿›è¡Œ)
        edges_x = self.conv_x(x_f32)
        edges_y = self.conv_y(x_f32)
    
        # 4. è®¡ç®—æ¢¯åº¦å¹…å€¼çš„å¹³æ–¹ (åœ¨ float32 ä¸Šè¿›è¡Œ)
        gradient_magnitude_squared = edges_x.pow(2) + edges_y.pow(2)
    
        # 5. å¼€æ–¹ (åœ¨ float32 ä¸Šè¿›è¡Œ)
        gradient_magnitude = torch.sqrt(gradient_magnitude_squared + self.epsilon)
    
        # 6. åç»­å¤„ç† (ä»åœ¨ float32 ä¸‹è¿›è¡Œ)
        sobel_edge = self.act(self.norm(gradient_magnitude))
        
        # 7. æ®‹å·®è¿æ¥ (ä¸¤ä¸ªæ“ä½œæ•° x_f32 å’Œ sobel_edge éƒ½æ˜¯ float32)
        fused_output = self.conv_extra(x_f32 + sobel_edge)
        
        # 8. å°†æœ€ç»ˆè¾“å‡ºè½¬æ¢å›åŸå§‹è¾“å…¥ç±»å‹
        return fused_output.to(input_dtype)


class Conv_Extra(nn.Module):
    def __init__(self, channel, norm_layer, act_layer):
        super(Conv_Extra, self).__init__()
        self.block = nn.Sequential(nn.Conv2d(channel, 64, 1),
                                   build_norm_layer(norm_layer, 64)[1],
                                   act_layer(),
                                   nn.Conv2d(64, 64, 3, stride=1, padding=1, dilation=1, bias=False),
                                   build_norm_layer(norm_layer, 64)[1],
                                   act_layer(),
                                   nn.Conv2d(64, channel, 1),
                                   build_norm_layer(norm_layer, channel)[1])
    def forward(self, x):
        out = self.block(x)
        return out

# class Scharr(nn.Module):
#     # __init__ æ–¹æ³•ä¿æŒå’Œä¸Šæ¬¡ä¸€æ ·ï¼Œæˆ‘ä»¬åªä¿®æ”¹ forward
#     def __init__(self, dim):
#         super(Scharr, self).__init__()
#         # ... __init__ çš„æ‰€æœ‰å†…å®¹ä¿æŒä¸å˜ ...
#         scharr_x = torch.tensor([[-3., 0., 3.], [-10., 0., 10.], [-3., 0., 3.]], dtype=torch.float32).view(1, 1, 3, 3)
#         scharr_y = torch.tensor([[-3., -10., -3.], [0., 0., 0.], [3., 10., 3.]], dtype=torch.float32).view(1, 1, 3, 3)
#         self.conv_x = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim, bias=False)
#         self.conv_y = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim, bias=False)
#         self.conv_x.weight.data.copy_(scharr_x.repeat(dim, 1, 1, 1))
#         self.conv_y.weight.data.copy_(scharr_y.repeat(dim, 1, 1, 1))
#         self.conv_x.weight.requires_grad = False
#         self.conv_y.weight.requires_grad = False
        
#         # The try-except block has been removed to eliminate the mmcv dependency.
#         # We now directly use the PyTorch-native layers from the original 'except' block.
#         # self.norm = nn.BatchNorm2d(dim)
#         self.act = nn.GELU()
#         self.conv_extra = DWConv(dim, dim) 
        
#         self.epsilon = 1e-6
#         # æš‚æ—¶ç§»é™¤Hookï¼Œå› ä¸ºæˆ‘ä»¬å°†æ‰‹åŠ¨è¿›è¡Œæ›´è¯¦ç»†çš„è°ƒè¯•
#         # self.register_forward_hook(inspect_forward_output)
#     def forward(self, x):
#         # ä½ çš„ Scharr.forward ä»£ç åº”è¯¥çœ‹èµ·æ¥åƒè¿™æ ·ï¼ŒåŒ…å«äº†æ‰€æœ‰çš„è°ƒè¯•ç‚¹
        
#         # ä½¿ç”¨ autocast æ¥ç¡®ä¿åœ¨æ­£ç¡®çš„ç²¾åº¦ä¸‹è¿è¡Œ
#         with autocast(enabled=True): 
#             # ç¡®ä¿è®¡ç®—åœ¨ float32 ä¸‹è¿›è¡Œï¼Œä»¥è·å¾—æœ€å¤§ç²¾åº¦å’ŒèŒƒå›´
#             x_f32 = x.to(torch.float32)
#             debug_tensor("Input 'x' (converted to float32)", x_f32) # è°ƒè¯•ç‚¹0
#             edges_x = self.conv_x(x_f32)
#             debug_tensor("Step 1: 'edges_x' after conv_x", edges_x) # è°ƒè¯•ç‚¹1
#             edges_y = self.conv_y(x_f32)
#             debug_tensor("Step 2: 'edges_y' after conv_y", edges_y) # è°ƒè¯•ç‚¹2
            
#             gradient_magnitude_squared = edges_x.float()**2 + edges_y.float()**2 + 1e-6
#             debug_tensor("Step 3: Gradient Magnitude Squared", gradient_magnitude_squared) # è°ƒè¯•ç‚¹3
#             # è¿™é‡Œæˆ‘ä»¬å…ˆç”¨å›æœ€åŸå§‹çš„ sqrtï¼Œå› ä¸ºæˆ‘ä»¬æƒ³æ‰¾åˆ°é—®é¢˜çš„æ ¹æº
#             # ä½ å¯ä»¥å…ˆæ³¨é‡Šæ‰ clamp ç‰ˆæœ¬ï¼Œç”¨å›è¿™ä¸ªæ¥å¤ç°é—®é¢˜
#             # gradient_magnitude = torch.sqrt(gradient_magnitude_squared + self.epsilon) 
#             gradient_magnitude = torch.sqrt(gradient_magnitude_squared.clamp(min=1e-6)) # æˆ–è€…å°±ç”¨clampç‰ˆæœ¬
#             debug_tensor("Step 4: Gradient Magnitude after sqrt", gradient_magnitude) # è°ƒè¯•ç‚¹4
#             # norm_output = self.norm(gradient_magnitude)
#             # debug_tensor("Step 5: Output of norm_layer", norm_output) # è°ƒè¯•ç‚¹5
            
#             act_output = self.act(gradient_magnitude)
#             debug_tensor("Step 5: Output of activation", act_output) # è°ƒè¯•ç‚¹6
            
#             out_f32 = self.conv_extra(x_f32 + act_output)
#             debug_tensor("Step 6: Final Output (float32)", out_f32) # è°ƒè¯•ç‚¹7
#         # æ£€æŸ¥æœ€ç»ˆè¾“å‡ºæ˜¯å¦æœ‰NaN
#         if torch.isnan(out_f32).any():
#             # æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¡®ä¿¡ï¼Œå¦‚æœè§¦å‘äº†è¿™ä¸ªï¼Œä¸Šé¢çš„æ—¥å¿—ä¸€å®šå·²ç»è¢«æ‰“å°å‡ºæ¥äº†
#             raise RuntimeError("NaN DETECTED! Stopping training for inspection. Check the logs above.")
#         # å°†è¾“å‡ºè½¬æ¢å›è¾“å…¥çš„åŸå§‹ç±»å‹
#         return out_f32.to(x.dtype)

class EGA(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # åˆ†æ”¯ A: è¾¹ç¼˜åˆ†æ”¯
        self.edge_sobel_x = EdgeConvSobelX(dim, dim)
        self.edge_sobel_y = EdgeConvSobelY(dim, dim)
        self.conv_reduce = Conv(dim * 2, dim, k=1, act=nn.GELU())
        self.sa = SAM()

        # åˆ†æ”¯ B: æ ‡å‡† 3x3 å·ç§¯åˆ†æ”¯
        self.standard_conv_branch = Conv(dim, dim, k=3, s=1, p=None, act=True)

        # èåˆä¸¤ä¸ªåˆ†æ”¯çš„ 3x3 å·ç§¯
        self.fusion_conv = Conv(dim * 2, dim, k=3, s=1, p=None, act=True)

        # åç»­çš„ ECA æ³¨æ„åŠ›æ¨¡å—
        self.eca = ECA(dim)

    def forward(self, x):
        # 1. è®¡ç®—è¾¹ç¼˜åˆ†æ”¯ (SAM å·²ä»æ­¤ç§»é™¤)
        edge_x = self.edge_sobel_x(x)
        edge_y = self.edge_sobel_y(x)
        edge_cat = torch.cat([edge_x, edge_y], dim=1)
        edge_features = self.conv_reduce(edge_cat)

        # 2. è®¡ç®—æ ‡å‡†å·ç§¯åˆ†æ”¯
        conv_features = self.standard_conv_branch(x)

        # 3. æ‹¼æ¥å¹¶ç”¨ 3x3 å·ç§¯èåˆ
        concatenated_features = torch.cat([conv_features, edge_features], dim=1)
        fused = self.fusion_conv(concatenated_features)

        # 4. å…ˆè¿›è¡Œæ®‹å·®è¿æ¥
        residual_out = fused + x

        # 5. å†åº”ç”¨ ECA å’Œ SA æ³¨æ„åŠ›
        eca_output = self.eca(residual_out)
        sa_attention = self.sa(eca_output)
        final_output = eca_output * sa_attention

        return final_output


# class EGA(nn.Module):
#     def __init__(self, dim):
#         super().__init__()
        
#         # --- 1. å®šä¹‰å¹¶è¡Œçš„ä¸¤ä¸ªåˆ†æ”¯ ---
#         # åˆ†æ”¯ A: è¾¹ç¼˜åˆ†æ”¯ (æ— å˜åŒ–)
#         self.edge_sobel_x = EdgeConvSobelX(dim, dim)
#         self.edge_sobel_y = EdgeConvSobelY(dim, dim)
#         # è¿™é‡Œçš„ Conv2d_BN åº”è¯¥ä¹Ÿæ˜¯æ‚¨é¡¹ç›®ä¸­çš„ä¸€ä¸ªæ ‡å‡†æ¨¡å—ï¼Œè¿™é‡Œå‡è®¾å®ƒå­˜åœ¨
#         self.conv_reduce = Conv2d_BN(dim * 2, dim, ks=1) 
#         self.sa = SAM()
        
#         # --- åˆ†æ”¯ B: æ ‡å‡†å·ç§¯åˆ†æ”¯ (å·²ä¿®æ”¹) ---
#         # ä½¿ç”¨æ‚¨æä¾›çš„æ ‡å‡†åŒ– Conv æ¨¡å—
#         # k=3 (3x3å·ç§¯), s=1, p=None (è®© autopad è‡ªåŠ¨è®¡ç®— padding)
#         # act=True ä½¿ç”¨é»˜è®¤çš„ SiLU æ¿€æ´»å‡½æ•°
#         self.standard_conv_branch = ResidualBlock(dim, dim, act=nn.GELU())
        
#         # --- 2. å®šä¹‰èåˆæ¨¡å— (æ— å˜åŒ–) ---
#         self.fusion = MAFM_Fusion(dim)
        
#         # --- 3. å®šä¹‰åç»­çš„æ³¨æ„åŠ›æ¨¡å— (æ— å˜åŒ–) ---
#         self.eca = ECA(dim)

#     def forward(self, x):
#         # 1. å¹¶è¡Œè®¡ç®—ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡º
#         edge_x = self.edge_sobel_x(x)
#         edge_y = self.edge_sobel_y(x)
#         edge_cat = torch.cat([edge_x, edge_y], dim=1)
#         edge_reduced = self.conv_reduce(edge_cat)
#         edge_features = self.sa(edge_reduced) * edge_reduced
        
#         conv_features = self.standard_conv_branch(x)
        
#         # 2. ä½¿ç”¨ MAFM åŠ¨æ€èåˆä¸¤ä¸ªåˆ†æ”¯
#         fused = self.fusion(conv_features, edge_features)
        
#         # 3. é€šè¿‡åç»­çš„æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œç‰¹å¾ç²¾ç‚¼
#         eca_att = self.eca(fused)
#         # sam_attention = self.sa(eca_att)
#         # temp_output = eca_att * sam_attention
        
#         # 4. æœ€ç»ˆçš„æ®‹å·®è¿æ¥
#         final_output = eca_att + x

#         return final_output

class EGA_Conv(nn.Module):
    def __init__(self, dim):
        super().__init__()
        
        # --- æ ¸å¿ƒæ”¹åŠ¨ ---
        # åŸæ¥çš„ self.sobel è¢«æ›¿æ¢ä¸ºä¸€ä¸ª 3x3 çš„ DWConv
        # æˆ‘ä»¬ç§°ä¹‹ä¸º feature_extractor ä»¥è¡¨æ˜å…¶é€šç”¨æ€§
        self.feature_extractor = DWConv(dim, dim)
        
        # åç»­çš„ dwconv ä¿æŒä¸å˜ï¼Œå¯ä»¥ç§°ä¹‹ä¸º fusion_conv
        self.fusion_conv = DWConv(dim, dim)
        
        # æ³¨æ„åŠ›æ¨¡å—ä¿æŒä¸å˜
        self.sam = SAM()
        self.eca = ECA(channel=dim)

    def forward(self, x):
        # 1. ä½¿ç”¨ DWConv æå–ç‰¹å¾ (æ›¿ä»£äº† Sobel)
        extracted_features = self.feature_extractor(x)

        # 2. ç‰¹å¾äº¤äº’ (é€»è¾‘ä¸åŸç‰ˆç›¸åŒ)
        # å°†åŸå§‹è¾“å…¥ x ä¸æå–å‡ºçš„ç‰¹å¾è¿›è¡Œäº¤äº’
        att = x * extracted_features + x
        
        # 3. ä¿¡æ¯èåˆ (é€»è¾‘ä¸åŸç‰ˆç›¸åŒ)
        fused = self.fusion_conv(att)

        # 4. åç»­æ³¨æ„åŠ›ä¸æ®‹å·®è¿æ¥ (é€»è¾‘ä¸åŸç‰ˆç›¸åŒ)
        eca_att = self.eca(fused)
        sam_attention = self.sam(eca_att)
        temp_output = eca_att * sam_attention
        final_output = temp_output + x

        return final_output

class EGA_singel(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # __init__ æ–¹æ³•ä¿æŒä¸å˜
        self.sobel = Sobel(dim)
        self.dwconv = DWConv(dim, dim)
        self.sam = SAM()
        self.eca = ECA(dim)

    def forward(self, x):
        edge_features = self.sobel(x)

        att = x * edge_features + x
        fused = self.dwconv(att)

        output = fused

        eca_att = self.eca(output)
        sam_attention = self.sam(eca_att)
        temp_output = eca_att * sam_attention
        final_output = temp_output + x

        return final_output


class EGA_att(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.scharr = Scharr(dim)
        self.dwconv = DWConv(dim, dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        edge_features = self.scharr(x)
        attention = self.sigmoid(edge_features)
        out = attention * x
        out = self.dwconv(out)
        return out


import torch
import torch.nn as nn
 
 
class CBAM(nn.Module):
    def __init__(self, channel, reduction=16, spatial_kernel=7):
        super(CBAM, self).__init__()
 
        # channel attention å‹ç¼©H,Wä¸º1
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
 
        # shared MLP
        self.mlp = nn.Sequential(
            # Conv2dæ¯”Linearæ–¹ä¾¿æ“ä½œ
            # nn.Linear(channel, channel // reduction, bias=False)
            nn.Conv2d(channel, channel // reduction, 1, bias=False),
            # inplace=Trueç›´æ¥æ›¿æ¢ï¼ŒèŠ‚çœå†…å­˜
            nn.ReLU(inplace=True),
            # nn.Linear(channel // reduction, channel,bias=False)
            nn.Conv2d(channel // reduction, channel, 1, bias=False)
        )
 
        # spatial attention
        self.conv = nn.Conv2d(2, 1, kernel_size=spatial_kernel,
                              padding=spatial_kernel // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
 
    def forward(self, x):
        max_out = self.mlp(self.max_pool(x))
        avg_out = self.mlp(self.avg_pool(x))
        channel_out = self.sigmoid(max_out + avg_out)
        x = channel_out * x
 
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        avg_out = torch.mean(x, dim=1, keepdim=True)
        spatial_out = self.sigmoid(self.conv(torch.cat([max_out, avg_out], dim=1)))
        x = spatial_out * x
        return x

class SAM(nn.Module):
    def __init__(self, kernel_size=7):
        super(SAM, self).__init__()
 
        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'  
        padding = 3 if kernel_size == 7 else 1
 
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)  # 7,3     3,1
        self.sigmoid = nn.Sigmoid()
 
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)


class MLCA(nn.Module):
    def __init__(self, in_size,local_size=5,gamma = 2, b = 1,local_weight=0.5):
        super(MLCA, self).__init__()

        # ECA è®¡ç®—æ–¹æ³•
        self.local_size=local_size
        self.gamma = gamma
        self.b = b
        t = int(abs(math.log(in_size, 2) + self.b) / self.gamma)   # eca  gamma=2
        k = t if t % 2 else t + 1

        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)
        self.conv_local = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)

        self.local_weight=local_weight

        self.local_arv_pool = nn.AdaptiveAvgPool2d(local_size)
        self.global_arv_pool=nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        local_arv=self.local_arv_pool(x)
        global_arv=self.global_arv_pool(local_arv)

        b,c,m,n = x.shape
        b_local, c_local, m_local, n_local = local_arv.shape

        # (b,c,local_size,local_size) -> (b,c,local_size*local_size)-> (b,local_size*local_size,c)-> (b,1,local_size*local_size*c)
        temp_local= local_arv.view(b, c_local, -1).transpose(-1, -2).reshape(b, 1, -1)
        temp_global = global_arv.view(b, c, -1).transpose(-1, -2)

        y_local = self.conv_local(temp_local)
        y_global = self.conv(temp_global)


        # (b,c,local_size,local_size) <- (b,c,local_size*local_size)<-(b,local_size*local_size,c) <- (b,1,local_size*local_size*c)
        y_local_transpose=y_local.reshape(b, self.local_size * self.local_size,c).transpose(-1,-2).view(b,c, self.local_size , self.local_size)
        # y_global_transpose = y_global.view(b, -1).transpose(-1, -2).unsqueeze(-1)
        y_global_transpose = y_global.view(b, -1).unsqueeze(-1).unsqueeze(-1)  # ä»£ç ä¿®æ­£
        # print(y_global_transpose.size())
        # åæ± åŒ–
        att_local = y_local_transpose.sigmoid()
        att_global = F.adaptive_avg_pool2d(y_global_transpose.sigmoid(),[self.local_size, self.local_size])
        # print(att_local.size())
        # print(att_global.size())
        att_all = F.adaptive_avg_pool2d(att_global*(1-self.local_weight)+(att_local*self.local_weight), [m, n])
        # print(att_all.size())
        x=x*att_all
        return x
    
# class Context_Exploration_Block(nn.Module):
#     def __init__(self, input_channels):
#         super(Context_Exploration_Block, self).__init__()
#         self.input_channels = input_channels
#         self.channels_single = int(input_channels / 4)

#         self.p1_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p2_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p3_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p4_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.input_channels, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())

#         self.p1 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p1_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=1, dilation=1),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())

#         self.p2 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p2_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=2, dilation=2),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())

#         self.p3 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 5, 1, 2),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p3_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=4, dilation=4),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())

#         self.p4 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 7, 1, 3),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())
#         self.p4_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, kernel_size=3, stride=1, padding=8, dilation=8),
#             nn.GroupNorm(num_groups=32, num_channels=self.channels_single), nn.ReLU())

#         self.fusion = nn.Sequential(nn.Conv2d(self.input_channels, self.input_channels, 3, 1, 1),
#                                     nn.GroupNorm(num_groups=32, num_channels=self.input_channels), nn.ReLU())

#     def forward(self, x):
#         p1_input = self.p1_channel_reduction(x)
#         p1 = self.p1(p1_input)
#         p1_dc = self.p1_dc(p1)

#         p2_input = self.p2_channel_reduction(x) + p1_dc
#         p2 = self.p2(p2_input)
#         p2_dc = self.p2_dc(p2)

#         p3_input = self.p3_channel_reduction(x) + p2_dc
#         p3 = self.p3(p3_input)
#         p3_dc = self.p3_dc(p3)

#         p4_input = self.p4_channel_reduction(x) + p3_dc
#         p4 = self.p4(p4_input)
#         p4_dc = self.p4_dc(p4)

#         ce = self.fusion(torch.cat((p1_dc, p2_dc, p3_dc, p4_dc), 1))

#         return ce
    
class Features_enhanced(nn.Module):
    """
    ç‰¹å¾å¢å¼ºæ¨¡å—ã€‚
    èåˆä¸»å¹²ç‰¹å¾å’Œè¾¹ç¼˜å›¾ï¼Œå¹¶é€šè¿‡å†…åµŒçš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—è¿›è¡Œå¢å¼ºã€‚
    """
    def __init__(self, input_channels, edge_channels=256):
        """
        Args:
            input_channels (int): ä¸»å¹²è¾“å…¥ç‰¹å¾ x çš„é€šé“æ•°ã€‚
            edge_channels (int): è¾¹ç¼˜æ¨¡å—è¾“å‡ºçš„é€šé“æ•° (é»˜è®¤ä¸º256)ã€‚
        """
        super().__init__()
        
        # å®šä¹‰å¯¹é½å±‚ï¼šå¦‚æœè¾¹ç¼˜å›¾çš„é€šé“æ•°ä¸è¾“å…¥ç‰¹å¾ä¸åŒ¹é…ï¼Œåˆ™ä½¿ç”¨1x1å·ç§¯è¿›è¡Œè°ƒæ•´
        self.align_conv = nn.Identity()
        if edge_channels != input_channels:
            self.align_conv = nn.Conv2d(edge_channels, input_channels, kernel_size=1)
        
        # æ ¸å¿ƒå¤„ç†å±‚ï¼šç›´æ¥åœ¨æ­¤å®šä¹‰æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        self.dws_conv = nn.Sequential(
            # 1. æ·±åº¦å·ç§¯ (Depthwise Conv)
            nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, padding=1, groups=input_channels, bias=False),
            nn.GroupNorm(num_groups=min(32, input_channels), num_channels=input_channels),
            nn.ReLU(),
            # 2. é€ç‚¹å·ç§¯ (Pointwise Conv)
            nn.Conv2d(input_channels, input_channels, kernel_size=1, bias=False),
            nn.GroupNorm(num_groups=min(32, input_channels), num_channels=input_channels),
            nn.ReLU(),
        )
        
        # æœ€åä¸€æ­¥ï¼šä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—
        self.ce_block = Context_Exploration_Block(input_channels)

    def forward(self, x, edge_output):
        """
        Args:
            x: ä¸»å¹²ç½‘ç»œè¾“å…¥ç‰¹å¾, e.g., shape: [B, C, H, W]
            edge_output: Eage_detectæ¨¡å—çš„è¾“å‡º, e.g., shape: [B, C_edge, H_edge, W_edge]

        Returns:
            å¢å¼ºåçš„ç‰¹å¾å›¾, shape: [B, C, H, W]
        """
        # --- æ­¥éª¤ 1: å¯¹é½è¾¹ç¼˜å›¾å¹¶ä¸è¾“å…¥ç›¸åŠ  ---
        
        # a. å¯¹é½é€šé“æ•°
        edge_aligned = self.align_conv(edge_output)
        
        # b. å¯¹é½ç©ºé—´å°ºå¯¸ (H, W)ï¼Œä»¥è¾“å…¥xä¸ºåŸºå‡†
        if edge_aligned.shape[2:] != x.shape[2:]:
            edge_aligned = F.interpolate(edge_aligned, size=x.shape[2:], mode='bilinear', align_corners=False)
            
        # c. ç›¸åŠ èåˆ
        fused_features = x + edge_aligned
        
        # --- æ­¥éª¤ 2: é€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯ ---
        dws_out = self.dws_conv(fused_features)
        
        # --- æ­¥éª¤ 3: åŠ å…¥æ®‹å·®è¿æ¥ (å°†è¾“å…¥xä¸dws_outç›¸åŠ ) ---
        residual_out = x + dws_out
        
        # --- æ­¥éª¤ 4: é€šè¿‡ä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—è¿›è¡Œæœ€ç»ˆå¢å¼º ---
        final_output = self.ce_block(residual_out)
        
        return final_output

class Features_enhance(nn.Module):
    """
    ç‰¹å¾å¢å¼ºæ¨¡å— (æœ€æ–°ç‰ˆ)ã€‚
    é€šè¿‡å¯¹é½é€šé“ -> concat -> channel_shuffle -> é™ç»´çš„æ–¹å¼è¿›è¡Œæ—©æœŸç‰¹å¾èåˆï¼Œ
    å†é€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—è¿›è¡Œå¢å¼ºã€‚
    """
    def __init__(self, input_channels, edge_channels=256):
        """
        Args:
            input_channels (int): ä¸»å¹²è¾“å…¥ç‰¹å¾ x çš„é€šé“æ•°ã€‚
            edge_channels (int): è¾¹ç¼˜æ¨¡å—è¾“å‡ºçš„é€šé“æ•° (é»˜è®¤ä¸º256)ã€‚
        """
        super().__init__()
        
        # æ­¥éª¤1.a: å®šä¹‰è¾¹ç¼˜ç‰¹å¾çš„é€šé“å¯¹é½å±‚
        # è¿™æ˜¯ä¸ºäº†ç¡®ä¿concatçš„ä¸¤ä¸ªéƒ¨åˆ†é€šé“æ•°ç›¸ç­‰ï¼Œä»è€Œè®©channel_shuffleæœ‰æ•ˆå·¥ä½œ
        self.edge_align_conv = nn.Identity()
        if edge_channels != input_channels:
            self.edge_align_conv = nn.Conv2d(edge_channels, input_channels, kernel_size=1)
        
        # æ­¥éª¤1.d: å®šä¹‰é™ç»´å±‚
        # å°†concatå’Œshuffleåçš„é€šé“ (2 * input_channels) é™ç»´å› input_channels
        self.channel_adjust_conv = nn.Conv2d(2 * input_channels, input_channels, kernel_size=1)

        # æ­¥éª¤2: æ ¸å¿ƒå¤„ç†å±‚ - æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        self.dws_conv = nn.Sequential(
            nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, padding=1, groups=input_channels, bias=False),
            nn.GroupNorm(num_groups=min(32, input_channels), num_channels=input_channels),
            nn.ReLU(),
            nn.Conv2d(input_channels, input_channels, kernel_size=1, bias=False),
            nn.GroupNorm(num_groups=min(32, input_channels), num_channels=input_channels),
            nn.ReLU(),
        )
        
        # æ­¥éª¤4: æœ€åä¸€æ­¥ - ä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—
        self.ce_block = Context_Exploration_Block(input_channels)

    def forward(self, x, edge_output):
        """
        Args:
            x (Tensor): ä¸»å¹²ç½‘ç»œè¾“å…¥ç‰¹å¾, e.g., shape: [B, C_in, H, W]
            edge_output (Tensor): Eage_detectæ¨¡å—çš„è¾“å‡º, e.g., shape: [B, C_edge, H_edge, W_edge]

        Returns:
            Tensor: å¢å¼ºåçš„ç‰¹å¾å›¾, shape: [B, C_in, H, W]
        """
        # --- æ­¥éª¤ 1: æ—©æœŸèåˆ (å¯¹é½ -> Concat -> Shuffle -> é™ç»´) ---
        
        # a. å¯¹é½è¾¹ç¼˜å›¾çš„é€šé“æ•°
        edge_temp = self.edge_align_conv(edge_output)
        
        # b. å¯¹é½è¾¹ç¼˜å›¾çš„ç©ºé—´å°ºå¯¸ (H, W)ï¼Œä»¥è¾“å…¥xä¸ºåŸºå‡†
        if edge_temp.shape[2:] != x.shape[2:]:
            edge_aligned = F.interpolate(edge_temp, size=x.shape[2:], mode='bilinear', align_corners=False)
        else:
            edge_aligned = edge_temp
            
        # c. æ‹¼æ¥ä¸¤ä¸ªé€šé“æ•°ç›¸åŒçš„ç‰¹å¾
        concatenated_features = torch.cat([x, edge_aligned], dim=1)
        
        # d. é€šé“æ··æ´— (groups=2 å› ä¸ºæˆ‘ä»¬æ‹¼æ¥äº†ä¸¤ä¸ªç‰¹å¾æº)
        shuffled_features = channel_shuffle(concatenated_features, groups=2)
        
        # e. ä½¿ç”¨1x1å·ç§¯é™ç»´ï¼Œå¾—åˆ°åˆæ­¥èåˆçš„ç‰¹å¾
        fused_features = self.channel_adjust_conv(shuffled_features)
        
        # --- æ­¥éª¤ 2: é€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯æå–ç‰¹å¾ ---
        dws_out = self.dws_conv(fused_features)
        
        # --- æ­¥éª¤ 3: åŠ å…¥æ®‹å·®è¿æ¥ (å°†åŸå§‹è¾“å…¥xä¸dws_outç›¸åŠ ) ---
        residual_out = x + dws_out
        
        # --- æ­¥éª¤ 4: é€šè¿‡ä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—è¿›è¡Œæœ€ç»ˆå¢å¼º ---
        final_output = self.ce_block(residual_out)
        
        return final_output

# class Context_Exploration_Block(nn.Module):
#     """
#     é›†æˆäº† C2f ç»“æ„ã€GELU æ¿€æ´»å‡½æ•°å’Œ ECA æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å—ã€‚
#     """
#     # æ›´æ”¹ 1: å°† e çš„é»˜è®¤å€¼è®¾ä¸º 0.6
#     def __init__(self, c1, c2, e=0.6):
#         super(Context_Exploration_Block, self).__init__()
#         assert c1 == c2, "C1 and C2 should be equal for Context_Exploration_Block"
        
#         self.c_ = int(c2 * e)  # ç”¨äºå¤„ç†å’Œç›´è¿è·¯å¾„çš„é€šé“æ•°

#         self.cv1 = Conv(c1, 2 * self.c_, k=1, s=1)

#         self.channels_single = self.c_ // 4
        
#         def get_safe_gn_groups(num_channels, prefer=32):
#             num_channels = int(num_channels)
#             if num_channels == 0:
#                 return 1
#             for divisor in [prefer, 16, 8, 4, 2, 1]:
#                 if num_channels % divisor == 0:
#                     return divisor
#             return 1

#         gn_groups_single = get_safe_gn_groups(self.channels_single)
#         gn_groups_c = get_safe_gn_groups(self.c_)

#         # æ›´æ”¹ 2: å°†æ‰€æœ‰ nn.ReLU() æ›¿æ¢ä¸º nn.GELU()
#         act = nn.GELU
#         self.p1_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.c_, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p2_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.c_, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p3_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.c_, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p4_channel_reduction = nn.Sequential(
#             nn.Conv2d(self.c_, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())

#         self.p1 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 1, 1, 0),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p1_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1, dilation=1),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())

#         self.p2 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p2_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 2, dilation=2),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())

#         self.p3 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 5, 1, 2),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p3_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 4, dilation=4),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())

#         self.p4 = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 7, 1, 3),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())
#         self.p4_dc = nn.Sequential(
#             nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 8, dilation=8),
#             nn.GroupNorm(gn_groups_single, self.channels_single), act())

#         fusion_in_channels = 4 * self.channels_single
#         self.fusion = nn.Sequential(nn.Conv2d(fusion_in_channels, self.c_, 3, 1, 1),
#                             nn.GroupNorm(gn_groups_c, self.c_), act())
        
#         self.cv2 = Conv(2 * self.c_, c2, k=1, s=1)
        
#         # æ›´æ”¹ 3: åœ¨æ¨¡å—æœ«å°¾åˆå§‹åŒ– ECA å±‚
#         self.eca = ECA(c2)

#     def forward(self, x):
#         y = list(self.cv1(x).chunk(2, 1))

#         y_process = y[1]
#         p1_input = self.p1_channel_reduction(y_process)
#         p1 = self.p1(p1_input)
#         p1_dc = self.p1_dc(p1)

#         p2_input = self.p2_channel_reduction(y_process) + p1_dc
#         p2 = self.p2(p2_input)
#         p2_dc = self.p2_dc(p2)

#         p3_input = self.p3_channel_reduction(y_process) + p2_dc
#         p3 = self.p3(p3_input)
#         p3_dc = self.p3_dc(p3)

#         p4_input = self.p4_channel_reduction(y_process) + p3_dc
#         p4 = self.p4(p4_input)
#         p4_dc = self.p4_dc(p4) 
        
#         ce_out = self.fusion(torch.cat((p1_dc, p2_dc, p3_dc, p4_dc), 1))

#         out = self.cv2(torch.cat((y[0], ce_out), 1))
        
#         # æ›´æ”¹ 3: åœ¨æœ€ç»ˆè¾“å‡ºå‰åº”ç”¨ ECA
#         return self.eca(out)

class ScharrAttention(nn.Module):
    """
    ä¸€ä¸ªä½¿ç”¨Scharrç®—å­ç”Ÿæˆç©ºé—´æ³¨æ„åŠ›çš„æ¨¡å—ã€‚
    å®ƒå¯ä»¥ä½œä¸ºä¸€ç§è½»é‡çº§çš„ã€æ— å‚æ•°çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ã€‚
    """
    def __init__(self, use_sigmoid=True):
        """
        åˆå§‹åŒ–Scharræ³¨æ„åŠ›æ¨¡å—ã€‚
        :param use_sigmoid: bool, å¦‚æœä¸ºTrueï¼Œä½¿ç”¨Sigmoidå°†è¾¹ç¼˜å›¾è½¬åŒ–ä¸º[0,1]çš„è½¯æ€§æ³¨æ„åŠ›æƒé‡ (æµæ´¾äºŒ)ã€‚
                                å¦‚æœä¸ºFalseï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è¾¹ç¼˜å›¾ä½œä¸ºé—¨æ§ä¿¡å· (æµæ´¾ä¸€)ã€‚
        """
        super(ScharrAttention, self).__init__()
        self.use_sigmoid = use_sigmoid
        # å®šä¹‰Scharrå·ç§¯æ ¸ (ä¸å¯è®­ç»ƒ)
        # Gx
        scharr_x = torch.tensor([[-3., 0., 3.], [-10., 0., 10.], [-3., 0., 3.]])
        # Gy
        scharr_y = torch.tensor([[-3., -10., -3.], [0., 0., 0.], [3., 10., 3.]])
        # å°†æ ¸å˜å½¢ä¸º (out_channels, in_channels/groups, H, W) çš„æ ¼å¼
        # è¿™é‡Œ out_channels=1, in_channels/groups=1
        self.kernel_x = scharr_x.float().unsqueeze(0).unsqueeze(0)
        self.kernel_y = scharr_y.float().unsqueeze(0).unsqueeze(0)
        # æ³¨å†Œä¸º bufferï¼Œè¿™æ ·å®ƒä¼šéšæ¨¡å‹ç§»åŠ¨åˆ°CPU/GPUï¼Œä½†ä¸ä¼šè¢«è§†ä¸ºæ¨¡å‹å‚æ•°
        self.register_buffer('scharr_kernel_x', self.kernel_x)
        self.register_buffer('scharr_kernel_y', self.kernel_y)
        if self.use_sigmoid:
            self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ã€‚
        :param x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå°ºå¯¸ä¸º (B, C, H, W)
        :return: ç»è¿‡Scharræ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾å›¾ï¼Œå°ºå¯¸ä¸å˜ã€‚
        """
        B, C, H, W = x.size()
        # ä¸ºäº†å¯¹æ¯ä¸ªé€šé“ç‹¬ç«‹åº”ç”¨Scharrç®—å­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†ç»„å·ç§¯ (grouped convolution)
        # å°†è¾“å…¥é€šé“ C ä½œä¸ºåˆ†ç»„æ•°ï¼Œè¿™æ ·æ¯ä¸ªå·ç§¯æ ¸åªä½œç”¨äºä¸€ä¸ªè¾“å…¥é€šé“
        # æ‰©å±•å·ç§¯æ ¸ä»¥åŒ¹é…è¾“å…¥é€šé“æ•°
        kernel_x = self.scharr_kernel_x.repeat(C, 1, 1, 1)
        kernel_y = self.scharr_kernel_y.repeat(C, 1, 1, 1)
        # ä½¿ç”¨ F.conv2d è¿›è¡Œå·ç§¯æ“ä½œ
        # padding=1 ä¿æŒå°ºå¯¸ä¸å˜
        grad_x = F.conv2d(x, kernel_x, bias=None, stride=1, padding=1, groups=C)
        grad_y = F.conv2d(x, kernel_y, bias=None, stride=1, padding=1, groups=C)
        # è®¡ç®—æ¢¯åº¦å¹…åº¦ (è¾¹ç¼˜å¼ºåº¦å›¾)
        # æ·»åŠ  epsilon é˜²æ­¢ sqrt(0) çš„NaNæ¢¯åº¦
        edge_map = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)
        if self.use_sigmoid:
            # æµæ´¾äºŒ: è½¯æ€§æ³¨æ„åŠ›
            attention_map = self.sigmoid(edge_map)
        else:
            # æµæ´¾ä¸€: ç¡¬æ€§é—¨æ§
            attention_map = edge_map
        
        # å°†æ³¨æ„åŠ›å›¾ä¸åŸå§‹ç‰¹å¾å›¾ç›¸ä¹˜
        # (B, C, H, W) * (B, C, H, W) -> (B, C, H, W)
        return x * attention_map

class ChannelAttention(nn.Module):
    """
    è®ºæ–‡ä¸­æè¿°çš„é€šé“æ³¨æ„åŠ›æ¨¡å— (Channel Attention Module, CA)ã€‚
    è¿™ä¸ªå®ç°ä¸¥æ ¼éµå¾ªäº†å…¬å¼(5)çš„é€»è¾‘ã€‚
    """
    def __init__(self, in_channels, reduction_ratio=4):
        """
        åˆå§‹åŒ–é€šé“æ³¨æ„åŠ›æ¨¡å—ã€‚
        :param in_channels: è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ã€‚
        :param reduction_ratio: é€šé“ç¼©å‡ç‡rï¼Œç”¨äºMLPçš„ç“¶é¢ˆå±‚ã€‚è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®ç»™å‡ºï¼Œä½†16æ˜¯å¸¸ç”¨å€¼ã€‚
        """
        super(ChannelAttention, self).__init__()
        # æ£€æŸ¥ç¼©å‡ç‡æ˜¯å¦åˆç†
        if in_channels <= reduction_ratio:
            # å¦‚æœè¾“å…¥é€šé“æ•°æœ¬èº«å°±å¾ˆå°ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥é€šé“æ•°çš„ä¸€åŠæˆ–è€…1ä½œä¸ºä¸­é—´é€šé“
            # é¿å…é™ç»´åé€šé“æ•°ä¸º0æˆ–è´Ÿæ•°
            mip_channels = in_channels // 2 if in_channels > 1 else 1
        else:
            mip_channels = in_channels // reduction_ratio
        # 1. Squeeze æ“ä½œ: å…¨å±€å¹³å‡æ± åŒ–å’Œå…¨å±€æœ€å¤§æ± åŒ–
        # è¿™ä¸¤ä¸ªæ“ä½œåœ¨forwardå‡½æ•°ä¸­ç›´æ¥è°ƒç”¨F.adaptive_avg_pool2då’ŒF.adaptive_max_pool2då®ç°
        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦å®šä¹‰å±‚
        # 2. Shared MLP: ä¸€ä¸ªå…±äº«çš„å¤šå±‚æ„ŸçŸ¥æœº
        # ä½¿ç”¨1x1å·ç§¯æ¥å®ç°å…¨è¿æ¥å±‚ï¼Œè¿™æ˜¯CNNä¸­çš„æ ‡å‡†åšæ³•
        self.shared_mlp = nn.Sequential(
            # ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼Œå¯¹åº” W0ï¼Œç”¨äºé™ç»´
            nn.Conv2d(in_channels, mip_channels, kernel_size=1, bias=False),
            # ReLUæ¿€æ´»å‡½æ•°ï¼Œå¯¹åº” Î´
            nn.ReLU(inplace=True),
            # ç¬¬äºŒä¸ª1x1å·ç§¯ï¼Œå¯¹åº” W1ï¼Œç”¨äºå‡ç»´
            nn.Conv2d(mip_channels, in_channels, kernel_size=1, bias=False)
        )
        
        # 3. Sigmoid æ¿€æ´»å‡½æ•°ï¼Œå¯¹åº” Ïƒ
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚
        :param x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå°ºå¯¸ (B, C, H, W)
        :return: ç»è¿‡é€šé“æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾å›¾ï¼Œå°ºå¯¸ (B, C, H, W)
        """
        # ä¿å­˜åŸå§‹è¾“å…¥ï¼Œç”¨äºæœ€åçš„ä¹˜æ³•
        original_input = x
        # è·å–è¾“å…¥å°ºå¯¸
        B, C, H, W = x.size()
        # Squeeze æ“ä½œ
        # å…¨å±€å¹³å‡æ± åŒ– -> (B, C, 1, 1)
        avg_pool_out = F.adaptive_avg_pool2d(x, (1, 1))
        # å…¨å±€æœ€å¤§æ± åŒ– -> (B, C, 1, 1)
        max_pool_out = F.adaptive_max_pool2d(x, (1, 1))
        # Shared MLP æ“ä½œ
        # åˆ†åˆ«é€šè¿‡å…±äº«çš„MLP
        avg_mlp_out = self.shared_mlp(avg_pool_out)
        max_mlp_out = self.shared_mlp(max_pool_out)
        # Merge æ“ä½œ: å…ƒç´ çº§ç›¸åŠ 
        merged_out = avg_mlp_out + max_mlp_out
        # Excitation æ“ä½œ: Sigmoid
        attention_weights = self.sigmoid(merged_out)
        # Reweight æ“ä½œ: å…ƒç´ çº§ç›¸ä¹˜
        # åˆ©ç”¨å¹¿æ’­æœºåˆ¶ (B, C, 1, 1) -> (B, C, H, W)
        output = original_input * attention_weights
        
        return output


class SobelGate(nn.Module):
    """
    Sobelé—¨æ§ä¿¡å·ç”Ÿæˆå™¨ã€‚
    
    è¿™ä¸ªæ¨¡å—å°è£…äº†ä»è¾“å…¥ç‰¹å¾å›¾ç”Ÿæˆé—¨æ§ä¿¡å·çš„æ‰€æœ‰é€»è¾‘ã€‚
    """
    def __init__(self, channel):
        super().__init__()
        self.epsilon = 1e-6
        
        # å®šä¹‰Sobelæ»¤æ³¢å™¨ (ä¸å¯å­¦ä¹ )
        sobel_x = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]], dtype=torch.float32).reshape(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]], dtype=torch.float32).reshape(1, 1, 3, 3)
        
        self.conv_x = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        self.conv_y = nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel, bias=False)
        
        self.conv_x.weight.data = sobel_x.repeat(channel, 1, 1, 1)
        self.conv_y.weight.data = sobel_y.repeat(channel, 1, 1, 1)
        
        self.conv_x.weight.requires_grad = False
        self.conv_y.weight.requires_grad = False

        self.norm = nn.BatchNorm2d(channel)
        self.act = nn.GELU()

    def forward(self, x):
        # æ··åˆç²¾åº¦ç¨³å®šæ€§å¤„ç†
        input_dtype = x.dtype
        x_f32 = x.to(torch.float32)
        
        # è®¡ç®—æ¢¯åº¦
        edges_x = self.conv_x(x_f32)
        edges_y = self.conv_y(x_f32)
        
        # è®¡ç®—æ¢¯åº¦å¹…å€¼
        gradient_magnitude = torch.sqrt(edges_x.pow(2) + edges_y.pow(2) + self.epsilon)
    
        # ç”Ÿæˆé—¨æ§ä¿¡å·å¹¶è¿”å›
        gate_signal = self.act(self.norm(gradient_magnitude))
        
        return gate_signal.to(input_dtype)

class Context_Exploration_Block(nn.Module):
    """
    é›†æˆäº† C2f ç»“æ„ã€GELU æ¿€æ´»å‡½æ•°å’Œ ECA æ³¨æ„åŠ›æœºåˆ¶çš„ä¸Šä¸‹æ–‡æ¢ç´¢æ¨¡å— (æœ€ç»ˆæ­£ç¡®ç‰ˆæœ¬)ã€‚
    """
    def __init__(self, c1, c2, e=0.5):
        super(Context_Exploration_Block, self).__init__()
        assert c1 == c2, "C1 and C2 should be equal for Context_Exploration_Block"
        
        self.c_ = int(c2 * e)
        self.cv1 = Conv(c1, 2 * self.c_, k=1, s=1)
        self.channels_single = self.c_ // 4
        
        def get_safe_gn_groups(num_channels, prefer=32):
            num_channels = int(num_channels)
            if num_channels == 0: return 1
            for divisor in [prefer, 16, 8, 4, 2, 1]:
                if num_channels % divisor == 0: return divisor
            return 1

        gn_groups_single = get_safe_gn_groups(self.channels_single)
        gn_groups_c = get_safe_gn_groups(self.c_)

        act = nn.GELU

        # ä¸º y[0] (ç›´è¿è·¯å¾„) å®šä¹‰ä¸€ä¸ª 1x1 å·ç§¯å¤„ç†å—
        self.shortcut_conv = nn.Sequential(
            nn.Conv2d(self.c_, self.c_, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_c, self.c_),
            act()
        )

        # å¤šåˆ†æ”¯è·¯å¾„çš„å®Œæ•´å®šä¹‰
        self.p1_channel_reduction = nn.Sequential(
            nn.Conv2d(self.c_, self.channels_single, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p2_channel_reduction = nn.Sequential(
            nn.Conv2d(self.c_, self.channels_single, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p3_channel_reduction = nn.Sequential(
            nn.Conv2d(self.c_, self.channels_single, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p4_channel_reduction = nn.Sequential(
            nn.Conv2d(self.c_, self.channels_single, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())

        self.p1 = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p1_dc = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1, dilation=1, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())

        self.p2 = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 1, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p2_dc = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 2, dilation=2, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())

        self.p3 = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 5, 1, 2, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p3_dc = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 4, dilation=4, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())

        self.p4 = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 7, 1, 3, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())
        self.p4_dc = nn.Sequential(
            nn.Conv2d(self.channels_single, self.channels_single, 3, 1, 8, dilation=8, bias=False),
            nn.GroupNorm(gn_groups_single, self.channels_single), act())

        fusion_in_channels = 4 * self.channels_single
        self.fusion = nn.Sequential(
            nn.Conv2d(fusion_in_channels, self.c_, 1, 1, 0, bias=False),
            nn.GroupNorm(gn_groups_c, self.c_), act(),
            nn.Conv2d(self.c_, self.c_, 3, 1, 1, bias=False),
            nn.GroupNorm(gn_groups_c, self.c_), act()
        )
        
        self.cv2 = Conv(2 * self.c_, c2, k=1, s=1)
        self.eca = ECA(c2)

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))

        # å¤„ç†è·¯å¾„
        y_process = y[1]
        p1_input = self.p1_channel_reduction(y_process)
        p1 = self.p1(p1_input)
        p1_dc = self.p1_dc(p1)

        p2_input = self.p2_channel_reduction(y_process) + p1_dc
        p2 = self.p2(p2_input)
        p2_dc = self.p2_dc(p2)

        p3_input = self.p3_channel_reduction(y_process) + p2_dc
        p3 = self.p3(p3_input)
        p3_dc = self.p3_dc(p3)

        p4_input = self.p4_channel_reduction(y_process) + p3_dc
        p4 = self.p4(p4_input)
        p4_dc = self.p4_dc(p4) 
        
        ce_out = self.fusion(torch.cat((p1_dc, p2_dc, p3_dc, p4_dc), 1))

        # ç›´è¿è·¯å¾„
        y0_processed = self.shortcut_conv(y[0])
        
        out = self.cv2(torch.cat((y0_processed, ce_out), 1))
        
        return self.eca(out)

class ConcatShuffleConv(nn.Module):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å—ï¼Œå®ƒå°†ä¸¤ä¸ªè¾“å…¥è¿›è¡Œæ‹¼æ¥(Concat)ï¼Œç„¶åè¿›è¡Œé€šé“æ··æ´—(Channel Shuffle)ï¼Œ
    æœ€åé€šè¿‡ä¸€ä¸ª1x1å·ç§¯è¿›è¡Œé™ç»´ã€‚
    """
    def __init__(self, c_list, c_out):
        """
        Args:
            c_list (list of int): ä¸¤ä¸ªè¾“å…¥æºçš„é€šé“æ•°åˆ—è¡¨ï¼Œä¾‹å¦‚ [128, 256]ã€‚
                                 è¿™ä¸ªå‚æ•°å°†ç”±è§£æå™¨è‡ªåŠ¨å¡«å……ã€‚
            c_out (int):          æœ€ç»ˆè¾“å‡ºçš„é€šé“æ•°ã€‚
        """
        super().__init__()
        # æ¨¡å—å†…éƒ¨ä¸éœ€è¦å­˜å‚¨ c_listï¼Œå› ä¸ºå®ƒåªåœ¨åˆå§‹åŒ–æ—¶ä½¿ç”¨
        
        # 1. è®¡ç®—æ‹¼æ¥åçš„æ€»è¾“å…¥é€šé“æ•°
        c_in = sum(c_list)
        
        # 2. å®šä¹‰1x1çš„é™ç»´å·ç§¯å±‚
        #    å®ƒçš„è¾“å…¥é€šé“æ•°æ˜¯ c_inï¼Œè¾“å‡ºé€šé“æ•°æ˜¯ c_out
        self.conv = Conv(c_in, c_out, k=1, s=1)

    def forward(self, x_list):
        """
        Args:
            x_list (list of Tensor): åŒ…å«ä¸¤ä¸ªè¾“å…¥ç‰¹å¾å›¾çš„åˆ—è¡¨ã€‚
        
        Returns:
            Tensor: å¤„ç†åçš„è¾“å‡ºç‰¹å¾å›¾ã€‚
        """
        # 1. æ‹¼æ¥
        # x_list æ˜¯ä¸€ä¸ªåŒ…å«äº†ä¸¤ä¸ªå¼ é‡çš„åˆ—è¡¨
        x_concatenated = torch.cat(x_list, dim=1)
        
        # 2. é€šé“æ··æ´—
        # æˆ‘ä»¬æœ‰ä¸¤ä¸ªè¾“å…¥æºï¼Œæ‰€ä»¥æ··æ´—çš„ç»„æ•°æ˜¯ 2
        x_shuffled = channel_shuffle(x_concatenated, groups=2)
        
        # 3. 1x1å·ç§¯é™ç»´
        return self.conv(x_shuffled)

class AAttn(nn.Module):
    """
    Area-attention module with the requirement of flash attention.

    Attributes:
        dim (int): Number of hidden channels;
        num_heads (int): Number of heads into which the attention mechanism is divided;
        area (int, optional): Number of areas the feature map is divided. Defaults to 1.

    Methods:
        forward: Performs a forward process of input tensor and outputs a tensor after the execution of the area attention mechanism.

    Examples:
        >>> import torch
        >>> from ultralytics.nn.modules import AAttn
        >>> model = AAttn(dim=64, num_heads=2, area=4)
        >>> x = torch.randn(2, 64, 128, 128)
        >>> output = model(x)
        >>> print(output.shape)
    
    Notes: 
        recommend that dim//num_heads be a multiple of 32 or 64.

    """

    def __init__(self, dim, num_heads, area=1):
        """Initializes the area-attention module, a simple yet efficient attention module for YOLO."""
        super().__init__()
        self.area = area

        self.num_heads = num_heads
        self.head_dim = head_dim = dim // num_heads
        all_head_dim = head_dim * self.num_heads

        self.qkv = Conv(dim, all_head_dim * 3, 1, act=False)
        self.proj = Conv(all_head_dim, dim, 1, act=False)
        self.pe = Conv(all_head_dim, dim, 7, 1, 3, g=dim, act=False)


    def forward(self, x):
        """Processes the input tensor 'x' through the area-attention"""
        B, C, H, W = x.shape
        N = H * W

        qkv = self.qkv(x).flatten(2).transpose(1, 2)
        if self.area > 1:
            qkv = qkv.reshape(B * self.area, N // self.area, C * 3)
            B, N, _ = qkv.shape
        q, k, v = qkv.view(B, N, self.num_heads, self.head_dim * 3).split(
            [self.head_dim, self.head_dim, self.head_dim], dim=3
        )

        if x.is_cuda and USE_FLASH_ATTN:
            x = flash_attn_func(
                q.contiguous().half(),
                k.contiguous().half(),
                v.contiguous().half()
            ).to(q.dtype)
        elif x.is_cuda and not USE_FLASH_ATTN:
            x = sdpa(
                q.permute(0, 2, 1, 3).contiguous(), 
                k.permute(0, 2, 1, 3).contiguous(), 
                v.permute(0, 2, 1, 3).contiguous(), 
                attn_mask=None, 
                dropout_p=0.0, 
                is_causal=False
            )
            x = x.permute(0, 2, 1, 3)
        else:
            q = q.permute(0, 2, 3, 1)
            k = k.permute(0, 2, 3, 1)
            v = v.permute(0, 2, 3, 1)
            attn = (q.transpose(-2, -1) @ k) * (self.head_dim ** -0.5)
            max_attn = attn.max(dim=-1, keepdim=True).values 
            exp_attn = torch.exp(attn - max_attn)
            attn = exp_attn / exp_attn.sum(dim=-1, keepdim=True)
            x = (v @ attn.transpose(-2, -1))
            x = x.permute(0, 3, 1, 2)
            v = v.permute(0, 3, 1, 2)

        if self.area > 1:
            x = x.reshape(B // self.area, N * self.area, C)
            v = v.reshape(B // self.area, N * self.area, C)
            B, N, _ = x.shape

        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)
        v = v.reshape(B, H, W, C).permute(0, 3, 1, 2)
        
        x = x + self.pe(v)
        x = self.proj(x)
        return x
    

class ABlock(nn.Module):
    """
    ABlock class implementing a Area-Attention block with effective feature extraction.

    This class encapsulates the functionality for applying multi-head attention with feature map are dividing into areas
    and feed-forward neural network layers.

    Attributes:
        dim (int): Number of hidden channels;
        num_heads (int): Number of heads into which the attention mechanism is divided;
        mlp_ratio (float, optional): MLP expansion ratio (or MLP hidden dimension ratio). Defaults to 1.2;
        area (int, optional): Number of areas the feature map is divided.  Defaults to 1.

    Methods:
        forward: Performs a forward pass through the ABlock, applying area-attention and feed-forward layers.

    Examples:
        Create a ABlock and perform a forward pass
        >>> model = ABlock(dim=64, num_heads=2, mlp_ratio=1.2, area=4)
        >>> x = torch.randn(2, 64, 128, 128)
        >>> output = model(x)
        >>> print(output.shape)
    
    Notes: 
        recommend that dim//num_heads be a multiple of 32 or 64.
    """

    def __init__(self, dim, num_heads, mlp_ratio=1.2, area=1):
        """Initializes the ABlock with area-attention and feed-forward layers for faster feature extraction."""
        super().__init__()

        self.attn = AAttn(dim, num_heads=num_heads, area=area)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(Conv(dim, mlp_hidden_dim, 1), Conv(mlp_hidden_dim, dim, 1, act=False))

        self.apply(self._init_weights)

    def _init_weights(self, m):
        """Initialize weights using a truncated normal distribution."""
        if isinstance(m, nn.Conv2d):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        """Executes a forward pass through ABlock, applying area-attention and feed-forward layers to the input tensor."""
        x = x + self.attn(x)
        x = x + self.mlp(x)
        return x

class A2C2f(nn.Module):  
    """
    A2C2f module with residual enhanced feature extraction using ABlock blocks with area-attention. Also known as R-ELAN

    This class extends the C2f module by incorporating ABlock blocks for fast attention mechanisms and feature extraction.

    Attributes:
        c1 (int): Number of input channels;
        c2 (int): Number of output channels;
        n (int, optional): Number of 2xABlock modules to stack. Defaults to 1;
        a2 (bool, optional): Whether use area-attention. Defaults to True;
        area (int, optional): Number of areas the feature map is divided. Defaults to 1;
        residual (bool, optional): Whether use the residual (with layer scale). Defaults to False;
        mlp_ratio (float, optional): MLP expansion ratio (or MLP hidden dimension ratio). Defaults to 1.2;
        e (float, optional): Expansion ratio for R-ELAN modules. Defaults to 0.5;
        g (int, optional): Number of groups for grouped convolution. Defaults to 1;
        shortcut (bool, optional): Whether to use shortcut connection. Defaults to True;

    Methods:
        forward: Performs a forward pass through the A2C2f module.

    Examples:
        >>> import torch
        >>> from ultralytics.nn.modules import A2C2f
        >>> model = A2C2f(c1=64, c2=64, n=2, a2=True, area=4, residual=True, e=0.5)
        >>> x = torch.randn(2, 64, 128, 128)
        >>> output = model(x)
        >>> print(output.shape)
    """

    def __init__(self, c1, c2, n=1, a2=True, area=1, residual=False, mlp_ratio=2.0, e=0.5, g=1, shortcut=True):
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        assert c_ % 32 == 0, "Dimension of ABlock be a multiple of 32."

        # num_heads = c_ // 64 if c_ // 64 >= 2 else c_ // 32
        num_heads = c_ // 32

        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv((1 + n) * c_, c2, 1)  # optional act=FReLU(c2)

        init_values = 0.01  # or smaller
        self.gamma = nn.Parameter(init_values * torch.ones((c2)), requires_grad=True) if a2 and residual else None

        self.m = nn.ModuleList(
            nn.Sequential(*(ABlock(c_, num_heads, mlp_ratio, area) for _ in range(2))) if a2 else C3k(c_, c_, 2, shortcut, g) for _ in range(n)
        )

    def forward(self, x):
        """Forward pass through R-ELAN layer."""
        y = [self.cv1(x)]
        y.extend(m(y[-1]) for m in self.m)
        
        # 1. å…ˆè®¡ç®—å‡ºæ¨¡å—çš„ä¸»å¹²è¾“å‡º
        output = self.cv2(torch.cat(y, 1))

        # 2. åªæœ‰åœ¨ä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³æ—¶ï¼Œæ‰æ‰§è¡Œæ®‹å·®è¿æ¥
        #    - æ¡ä»¶ä¸€ï¼šç”¨æˆ·æƒ³è¦æ®‹å·®è¿æ¥ (self.gamma è¢«åˆ›å»º)
        #    - æ¡ä»¶äºŒï¼šè¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°å¿…é¡»ç›¸ç­‰ (å®‰å…¨æ£€æŸ¥)
        if self.gamma is not None and x.shape[1] == output.shape[1]:
            return x + self.gamma.view(1, -1, 1, 1) * output
            
        # 3. å¦‚æœæ¡ä»¶ä¸æ»¡è¶³ï¼Œå°±ç›´æ¥è¿”å›ä¸»å¹²è¾“å‡º
        return output
