import torch
import torch.nn as nn
import cv2
import numpy as np
import sys
import os
# --- å¼€å§‹æ·»åŠ çš„ä»£ç  ---
# å°†é¡¹ç›®çš„æ ¹ç›®å½•æ·»åŠ åˆ°Pythonçš„æ¨¡å—æœç´¢è·¯å¾„ä¸­
# æˆ‘ä»¬éœ€è¦ä»å½“å‰æ–‡ä»¶ '/mnt/zhouzj/mycode/my_ultralytics/YOLO/gradcam.py'
# å‘ä¸Šå›æº¯ä¸‰çº§ï¼Œæ‰¾åˆ° '/mnt/zhouzj/mycode/'
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
# æ£€æŸ¥æ˜¯å¦å·²åœ¨è·¯å¾„ä¸­ï¼Œé¿å…é‡å¤æ·»åŠ 
if project_root not in sys.path:
    print(f"å°†é¡¹ç›®æ ¹ç›®å½• '{project_root}' æ·»åŠ åˆ° sys.path")
    sys.path.insert(0, project_root)

from torchvision.transforms import functional as F
import traceback
import os  # <--- æ–°å¢ï¼šç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„å’Œåˆ›å»ºæ–‡ä»¶å¤¹

# ç¡®ä¿ grad-cam åº“å·²å®‰è£…: pip install grad-ca
# æ­£ç¡®çš„å¯¼å…¥è¯­å¥
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image

# except ImportError:
#     print("é”™è¯¯ï¼š'grad-cam' åº“æœªæ‰¾åˆ°ã€‚è¯·è¿è¡Œ 'pip install grad-cam' è¿›è¡Œå®‰è£…ã€‚")
#     exit()

# =================================================================================
#
#  ã€ã€ã€ æ ¸å¿ƒé…ç½®åŒºåŸŸï¼šè¯·åœ¨æ­¤å¤„ä¿®æ”¹ä¸ºæ‚¨è‡ªå·±çš„è®¾ç½® ã€‘ã€‘ã€‘
#
# =================================================================================

# 1. å¯¼å…¥æ‚¨çš„æ¨¡å‹å®šä¹‰ç±»
#    ä¸‹é¢è¿™è¡Œéœ€è¦æ‚¨æ ¹æ®æ‚¨çš„é¡¹ç›®ç»“æ„è¿›è¡Œä¿®æ”¹
from ultralytics.models.yolo.model import DetectionModel as Model

# 2. æŒ‡å®šæ‚¨çš„æ¨¡å‹é…ç½®æ–‡ä»¶ (CFG) å’Œæƒé‡æ–‡ä»¶è·¯å¾„
CFG_PATH = '/mnt/zhouzj/mycode/my_ultralytics/cfg/models/vits/CSPeg_0.5.yaml'
WEIGHTS_PATH = '/mnt/zhouzj/mycode/runner/vit/CSPeg_0.5/weights/best.pt'

# 3. æŒ‡å®šè¦åˆ†æçš„å›¾åƒè·¯å¾„
IMAGE_PATH = '/mnt/zhouzj/mycode/underwater datasets/DUO/VOCdevkit_DUO/VOC2007/images/train/57.jpg'

# 4. æŒ‡å®šç›®æ ‡ç±»åˆ«ç´¢å¼•
TARGET_CATEGORY_INDEX = 0

# 5. æŒ‡å®šæ¨¡å‹è¾“å…¥å°ºå¯¸
MODEL_INPUT_SIZE = 800

# 6. ã€ã€æ–°å¢ã€‘ã€‘æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
#    æ‚¨å¯ä»¥è®¾ç½®ä¸ºä»»ä½•æ‚¨å¸Œæœ›çš„è·¯å¾„ã€‚å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œç¨‹åºä¼šè‡ªåŠ¨åˆ›å»ºã€‚
OUTPUT_DIR = '/mnt/zhouzj/mycode/my_ultralytics/YOLO/heatputput'  # <-- ï¼ï¼ï¼ æ‚¨å¯ä»¥ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶å¤¹åæˆ–è·¯å¾„ ï¼ï¼ï¼

# =================================================================================
#  æ ¸å¿ƒé€»è¾‘ï¼š(é€šå¸¸æ— éœ€ä¿®æ”¹ä»¥ä¸‹ä»£ç )
# =================================================================================

# ... (preprocess_image, postprocess_heatmap, DetectionModelWrapper å‡½æ•°ä¿æŒä¸å˜) ...
def preprocess_image(img: np.ndarray, target_size: int) -> tuple:
    h, w, _ = img.shape
    ratio = target_size / max(h, w)
    new_w, new_h = int(w * ratio), int(h * ratio)
    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
    padded_img = np.full((target_size, target_size, 3), 114, dtype=np.uint8)
    pad_w = (target_size - new_w) // 2
    pad_h = (target_size - new_h) // 2
    padded_img[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized_img
    rgb_padded_img = cv2.cvtColor(padded_img, cv2.COLOR_BGR2RGB)
    rgb_padded_float = np.float32(rgb_padded_img) / 255.0
    input_tensor = F.to_tensor(rgb_padded_float).unsqueeze(0)
    return input_tensor, (h, w), (new_h, new_w), (pad_h, pad_w)

def postprocess_heatmap(grayscale_cam, original_shape, resized_shape, padding, input_size):
    original_h, original_w = original_shape
    new_h, new_w = resized_shape
    pad_h, pad_w = padding
    cam_resized_to_input = cv2.resize(grayscale_cam, (input_size, input_size), interpolation=cv2.INTER_LINEAR)
    cam_cropped = cam_resized_to_input[pad_h:pad_h + new_h, pad_w:pad_w + new_w]
    cam_final = cv2.resize(cam_cropped, (original_w, original_h), interpolation=cv2.INTER_LINEAR)
    cam_final = np.maximum(cam_final, 0)
    cam_final = cam_final - np.min(cam_final)
    cam_final = cam_final / (np.max(cam_final) + 1e-8)
    return cam_final

class DetectionModelWrapper(torch.nn.Module):
    def __init__(self, model, category_index):
        super(DetectionModelWrapper, self).__init__()
        self.model = model
        self.category_index = category_index
    def forward(self, x):
        outputs = self.model(x)
        # ultralytics v8/v10æ¨¡å‹è¾“å‡ºåœ¨ training=False æ—¶æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ [bs, 84, 8400] æ ¼å¼
        # æˆ‘ä»¬éœ€è¦ä»è¿™ä¸ªè¾“å‡ºä¸­æå–ç‰¹å®šç±»åˆ«çš„ç½®ä¿¡åº¦
        # ç»´åº¦ 4 æ˜¯ objectness score, 5 åŠä¹‹åæ˜¯ class scores
        # æˆ‘ä»¬å…³å¿ƒçš„æ˜¯ class score
        target_class_scores = outputs[0][:, 4 + self.category_index, :] # æ³¨æ„ï¼šYOLOv8/10 æ˜¯ 4 + class_index
        
        if target_class_scores.numel() == 0:
            return torch.tensor(0.0, device=x.device, requires_grad=True)
        return torch.max(target_class_scores)


def run_grad_cam():
    # --- 1. åŠ è½½æ¨¡å‹ ---
    print(f"æ­£åœ¨ä»CFG '{CFG_PATH}' åŠ è½½æ¨¡å‹ç»“æ„...")
    # æ³¨æ„ï¼šè¿™é‡Œçš„ Model è°ƒç”¨æ–¹å¼æ˜¯åŸºäº ultralytics åº“çš„æ ‡å‡†
    model = Model(cfg=CFG_PATH, ch=3, nc=4) 
    
    print(f"æ­£åœ¨ä» '{WEIGHTS_PATH}' åŠ è½½æ¨¡å‹æƒé‡...")
    checkpoint = torch.load(WEIGHTS_PATH, map_location='cpu')
    # ultralytics çš„ 'best.pt' é€šå¸¸åŒ…å«ä¸€ä¸ª 'model' é”®
    state_dict = checkpoint.get('model', checkpoint).float().state_dict()
    model.load_state_dict(state_dict, strict=True)
    
    model.eval()
    if torch.cuda.is_available():
        model.cuda()
    print("æ¨¡å‹åŠ è½½æˆåŠŸå¹¶å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚")

    # --- 2. ç¡®å®šç›®æ ‡å±‚ ---
    target_layer = None
    try:
        target_layer = model.model[17].cv2
        print(f"æˆåŠŸå®šä½ç›®æ ‡å±‚: model.model[17].cv2 (ç±»å‹: {type(target_layer).__name__})")
    except (AttributeError, IndexError) as e:
        print("\n" + "="*80)
        print("ã€ã€ã€ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è‡ªåŠ¨å®šä½ç›®æ ‡å±‚ 'model.model[17].cv2' ã€‘ã€‘ã€‘")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        print("è¯·æ£€æŸ¥æ‚¨çš„æ¨¡å‹ç»“æ„ï¼Œå¹¶æ‰‹åŠ¨ä¿®æ”¹ 'target_layer = ...' è¿™ä¸€è¡Œã€‚")
        print("="*80 + "\n")
        print("å®Œæ•´çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š\n")
        print(model)
        return

    # --- 3. åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ ---
    original_image = cv2.imread(IMAGE_PATH)
    if original_image is None:
        print(f"é”™è¯¯ï¼šæ— æ³•è¯»å–å›¾åƒ {IMAGE_PATH}")
        return
    
    print(f"åŸå§‹å›¾åƒå°ºå¯¸: {original_image.shape[:2]}, æ­£åœ¨é¢„å¤„ç†ä¸º {MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE}...")
    input_tensor, orig_shape, resized_shape, padding = preprocess_image(original_image, MODEL_INPUT_SIZE)
    if torch.cuda.is_available():
        input_tensor = input_tensor.cuda()

    # --- 4. è®¾ç½®Grad-CAM ---
    wrapped_model = DetectionModelWrapper(model, TARGET_CATEGORY_INDEX)
    cam = GradCAM(model=wrapped_model, target_layer=target_layer, use_cuda=torch.cuda.is_available())

    # --- 5. ç”Ÿæˆå’Œåå¤„ç†çƒ­å›¾ ---
    print(f"æ­£åœ¨ä¸ºç±»åˆ«ç´¢å¼• {TARGET_CATEGORY_INDEX} ç”ŸæˆGrad-CAMçƒ­å›¾...")
    grayscale_cam = cam(input_tensor=input_tensor, target_category=None)
    grayscale_cam = grayscale_cam[0, :]
    
    print("æ­£åœ¨å°†çƒ­å›¾å¯¹é½å›åŸå§‹å›¾åƒå°ºå¯¸...")
    final_heatmap = postprocess_heatmap(grayscale_cam, orig_shape, resized_shape, padding, MODEL_INPUT_SIZE)

    # --- 6. å¯è§†åŒ–å¹¶ä¿å­˜ --- <--- ä¿®æ”¹éƒ¨åˆ†
    
    # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    print(f"æ£€æŸ¥å¹¶åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: '{OUTPUT_DIR}'")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # æ ¹æ®è¾“å…¥å›¾åƒåå’Œç±»åˆ«ç”ŸæˆåŠ¨æ€æ–‡ä»¶å
    base_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]
    output_filename = f"{base_name}_class_{TARGET_CATEGORY_INDEX}_heatmap.jpg"
    
    # æ„å»ºå®Œæ•´çš„è¾“å‡ºè·¯å¾„
    full_output_path = os.path.join(OUTPUT_DIR, output_filename)
    
    # å åŠ å¹¶ä¿å­˜å›¾åƒ
    original_rgb_float = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB) / 255.0
    visualization = show_cam_on_image(original_rgb_float, final_heatmap, use_rgb=True)
    visualization_bgr = cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR)
    cv2.imwrite(full_output_path, visualization_bgr)
    
    print("\n" + "="*50)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼ğŸ‰")
    print(f"å¯è§†åŒ–ç»“æœå·²æˆåŠŸä¿å­˜è‡³: '{full_output_path}'") # <--- ä¿®æ”¹
    print("="*50)

if __name__ == '__main__':
    try:
        run_grad_cam()
    except Exception as e:
        print(f"\nè„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å…¨å±€é”™è¯¯: {e}")
        traceback.print_exc()
        print("\nè¯·æ£€æŸ¥é¡¶éƒ¨çš„ã€æ ¸å¿ƒé…ç½®åŒºåŸŸã€‘æ˜¯å¦å·²å…¨éƒ¨æ­£ç¡®å¡«å†™ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹å¯¼å…¥è¯­å¥ã€‚")

